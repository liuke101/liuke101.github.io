<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Pytorch精粹 | 游戏江湖</title><meta name="author" content="灵玉"><meta name="copyright" content="灵玉"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="一、基础1 数据操作01 张量1import torch   深度学习存储和操作数据的主要接口是张量（n维数组）。它提供了各种功能，包括基本数学运算、广播、索引、切片、内存节省和转换其他 Python 对象。  张量（tensor） 表示一个由数值组成的数组，这个数组可能有多个维度。   具有一个轴的张量对应数学上的向量（vector）；  具有两个轴的张量对应数学上的矩阵（matrix）； 具有">
<meta property="og:type" content="article">
<meta property="og:title" content="Pytorch精粹">
<meta property="og:url" content="http://liuke101.github.io/post/54732.html">
<meta property="og:site_name" content="游戏江湖">
<meta property="og:description" content="一、基础1 数据操作01 张量1import torch   深度学习存储和操作数据的主要接口是张量（n维数组）。它提供了各种功能，包括基本数学运算、广播、索引、切片、内存节省和转换其他 Python 对象。  张量（tensor） 表示一个由数值组成的数组，这个数组可能有多个维度。   具有一个轴的张量对应数学上的向量（vector）；  具有两个轴的张量对应数学上的矩阵（matrix）； 具有">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdmirror.com/gh/liuke101/PicGo-For-Hexo/image/202408010047802.png">
<meta property="article:published_time" content="2022-10-19T16:00:00.000Z">
<meta property="article:modified_time" content="2024-08-08T17:07:00.328Z">
<meta property="article:author" content="灵玉">
<meta property="article:tag" content="Pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdmirror.com/gh/liuke101/PicGo-For-Hexo/image/202408010047802.png"><link rel="shortcut icon" href="https://cdn.jsdmirror.com/gh/liuke101/PicGo-For-Hexo/image/202404232138939.png"><link rel="canonical" href="http://liuke101.github.io/post/54732.html"><link rel="preconnect" href="//cdn.jsdmirror.com"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="6wt4eQbcmYIioyaSsUswEjImObRq1GoUZP_ZRoEoOXc"/><meta name="msvalidate.01" content="9315C074B7CC1C8996DDD5A9533EF368"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdmirror.com/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdmirror.com/npm/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdmirror.com/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?1d08ce745bb272195666f030857b5f8b";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: {"appId":"QTHLKKVLAS","apiKey":"7f01e97a350ca196bda1df9cd9168b30","indexName":"hexo","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容：${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: 灵玉","link":"链接: ","source":"来源: 游戏江湖","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体中文","cht_to_chs":"你已切换为简体中文","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#804bfb","bgDark":"#804bfb","position":"bottom-left"},
  infinitegrid: {
    js: 'https://cdn.jsdmirror.com/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Pytorch精粹',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-08-09 01:07:00'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.2.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://cdn.jsdmirror.com/gh/liuke101/PicGo-For-Hexo/image/I168.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">79</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">22</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header" style="background-image: url('https://cdn.jsdmirror.com/gh/liuke101/PicGo-For-Hexo/image/202408010047802.png')"><nav id="nav"><span id="blog-info"><a href="/" title="游戏江湖"><span class="site-name">游戏江湖</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Pytorch精粹</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-10-19T16:00:00.000Z" title="发表于 2022-10-20 00:00:00">2022-10-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-08-08T17:07:00.328Z" title="更新于 2024-08-09 01:07:00">2024-08-09</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%BC%96%E7%A8%8B/">编程</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">31k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>111分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Pytorch精粹"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/post/54732.html#post-comment"><span class="waline-comment-count" data-path="/post/54732.html"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="一、基础"><a href="#一、基础" class="headerlink" title="一、基础"></a>一、基础</h1><h2 id="1-数据操作"><a href="#1-数据操作" class="headerlink" title="1 数据操作"></a>1 数据操作</h2><h3 id="01-张量"><a href="#01-张量" class="headerlink" title="01 张量"></a>01 张量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>

<ul>
<li><p>深度学习存储和操作数据的主要接口是张量（n维数组）。它提供了各种功能，包括基本数学运算、广播、索引、切片、内存节省和转换其他 Python 对象。</p>
</li>
<li><p><strong>张量（tensor）</strong> 表示一个由数值组成的数组，这个数组可能有多个维度。 </p>
<ul>
<li>具有一个轴的张量对应数学上的<strong>向量（vector）</strong>； </li>
<li>具有两个轴的张量对应数学上的<strong>矩阵（matrix）</strong>；</li>
<li>具有两个轴以上的张量没有特殊的数学名称。</li>
<li>张量中的每个值都称为张量的 <strong>元素（element）</strong></li>
</ul>
</li>
<li><p>使用 <code>arange</code> 创建一个<strong>行向量</strong> <code>x</code>。这个行向量包含以0开始的前12个整数，范围为 $[0,12)$，它们默认创建为整数。也可指定创建类型为浮点数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">12</span>)</span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor([ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>])</span><br></pre></td></tr></table></figure>
</li>
<li><p>张量（沿每个轴的长度）的形状：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x.shape</span><br><span class="line"><span class="comment">###</span></span><br><span class="line">torch.Size([<span class="number">12</span>])</span><br></pre></td></tr></table></figure>
</li>
<li><p>张量的大小（size）：即张量中元素的总数，即形状的所有元素乘积</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X = x.reshape(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor([[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">       [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>],</span><br><span class="line">        [ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]])</span><br></pre></td></tr></table></figure></li>
</ul>
<p>不需要通过手动指定每个维度来改变形状。也就是说，如果我们的目标形状是（高度,宽度），那么在知道宽度后，高度会被自动计算得出，不必我们自己做除法。在上面的例子中，为了获得一个3行的矩阵，我们手动指定了它有3行和4列。幸运的是，我们可以<strong>通过 <code>-1</code> 来调用此自动计算出维度的功能</strong>。即我们可以用 <code>x.reshape(-1,4)</code> 或 <code>x.reshape(3,-1)</code> 来取代 <code>x.reshape(3,4)</code>。 </p>
<ul>
<li>创建一个形状为（2,3,4）的张量，指定初始值<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">torch.zeros((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor([[[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]]])</span><br></pre></td></tr></table></figure></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">torch.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor([[[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">         [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">         [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">         [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">         [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]]])</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>通过从某个特定的概率分布中随机采样来得到张量中每个元素的值。</strong> 例如，当我们构造数组来作为神经网络中的参数时，我们通常会随机初始化参数的值。以下代码创建一个形状为（3,4）的张量。<strong>其中的每个元素都从均值为0、标准差为1的标准高斯分布（正态分布）中随机采样。</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.randn(<span class="number">3</span>, <span class="number">4</span>) <span class="comment">#torch.randn函数用于创建一个具有随机值的张量</span></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor([[-<span class="number">0.0135</span>,  <span class="number">0.0665</span>,  <span class="number">0.0912</span>,  <span class="number">0.3212</span>],</span><br><span class="line">        [ <span class="number">1.4653</span>,  <span class="number">0.1843</span>, -<span class="number">1.6995</span>, -<span class="number">0.3036</span>],</span><br><span class="line">        [ <span class="number">1.7646</span>,  <span class="number">1.0450</span>,  <span class="number">0.2457</span>, -<span class="number">0.7732</span>]])</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>通过提供包含数值的 Python 列表（或嵌套列表），来为所需张量中的每个元素赋予确定值</strong>。在这里，最外层的列表对应于轴0，内层的列表对应于轴1。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([[<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]])</span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor([[<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]])</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="02-运算符"><a href="#02-运算符" class="headerlink" title="02 运算符"></a>02 运算符</h3><p>对于任意具有<mark style="background: #FF5582A6;">相同形状</mark>的张量，常见的标准算术运算符（<code>+</code>、<code>-</code>、<code>*</code>、<code>/</code> 和 <code>**</code>）都可以被升级为<strong>按元素运算</strong>。</p>
<p>在下面的例子中，我们使用逗号来表示一个具有5个元素的元组，其中每个元素都是按元素操作的结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1.0</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">8</span>])</span><br><span class="line">y = torch.tensor([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">x + y</span><br><span class="line">x - y</span><br><span class="line">x * y</span><br><span class="line">x / y</span><br><span class="line">x ** y  <span class="comment"># **运算符是求幂运算</span></span><br><span class="line">torch.exp(x) <span class="comment"># 按元素求幂</span></span><br></pre></td></tr></table></figure>

<p>可以把<strong>多个张量连结</strong>（concatenate）在一起，把它们端对端地叠起来形成一个更大的张量。我们只需要提供张量列表，并给出沿哪个轴连结。<br>下面的例子分别演示了当我们沿行（轴-0，形状的第一个元素） 和按列（轴-1，形状的第二个元素）连结两个矩阵时，会发生什么情况。我们可以看到，第一个输出张量的轴-0长度（6）是两个输入张量轴-0长度的总和（3+3）；第二个输出张量的轴-1长度（8）是两个输入张量轴-1长度的总和（4+4）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">X = torch.arange(<span class="number">12</span>, dtype=torch.float32).reshape((<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">Y = torch.tensor([[<span class="number">2.0</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]])</span><br><span class="line">torch.cat((X, Y), dim=<span class="number">0</span>), torch.cat((X, Y), dim=<span class="number">1</span>)</span><br><span class="line"><span class="comment">###</span></span><br><span class="line">(tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">         [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>],</span><br><span class="line">         [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>],</span><br><span class="line">         [ <span class="number">2.</span>,  <span class="number">1.</span>,  <span class="number">4.</span>,  <span class="number">3.</span>],</span><br><span class="line">         [ <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">4.</span>],</span><br><span class="line">         [ <span class="number">4.</span>,  <span class="number">3.</span>,  <span class="number">2.</span>,  <span class="number">1.</span>]]),</span><br><span class="line"> tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">2.</span>,  <span class="number">1.</span>,  <span class="number">4.</span>,  <span class="number">3.</span>],</span><br><span class="line">         [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">4.</span>],</span><br><span class="line">         [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>,  <span class="number">4.</span>,  <span class="number">3.</span>,  <span class="number">2.</span>,  <span class="number">1.</span>]]))</span><br></pre></td></tr></table></figure>

<p><strong>逻辑运算符</strong>也是按元素运算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X == Y</span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor([[<span class="literal">False</span>,  <span class="literal">True</span>, <span class="literal">False</span>,  <span class="literal">True</span>],</span><br><span class="line">        [<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>],</span><br><span class="line">        [<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>]])</span><br></pre></td></tr></table></figure>

<p>对张量中的所有元素进行<strong>求和</strong>，会产生一个单元素张量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X.<span class="built_in">sum</span>()</span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor(<span class="number">66.</span>)</span><br></pre></td></tr></table></figure>

<h3 id="03-广播"><a href="#03-广播" class="headerlink" title="03 广播"></a>03 广播</h3><p>在上面的部分是在相同形状的两个张量上执行按元素操作。在某些情况下，<strong>即使<mark style="background: #FF5582A6;">形状不同</mark>，我们仍然可以通过调用广播机制（broadcasting mechanism）来执行按元素操作</strong>。</p>
<p><strong>这种机制的工作方式如下：</strong></p>
<ol>
<li>通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；</li>
<li>对生成的数组执行按元素操作。</li>
</ol>
<p><strong>在大多数情况下，我们将沿着数组中长度为1的轴进行广播，如下例子：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">3</span>).reshape((<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">b = torch.arange(<span class="number">2</span>).reshape((<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line"><span class="comment">###</span></span><br><span class="line">(tensor([[<span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>],</span><br><span class="line">         [<span class="number">2</span>]]),</span><br><span class="line"></span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">1</span>]]))</span><br></pre></td></tr></table></figure>

<p>由于 <code>a</code> 和 <code>b</code> 分别是 $3×1$ 和 $1×2$ 矩阵，如果让它们相加，它们的形状不匹配。我们将两个矩阵<strong>广播</strong>为一个更大的 $3×2$ 矩阵，如下所示：矩阵 <code>a</code> 将复制列，矩阵 <code>b</code> 将复制行，然后再按元素相加。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a + b</span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">3</span>]])</span><br></pre></td></tr></table></figure>

<h3 id="04-索引和切片"><a href="#04-索引和切片" class="headerlink" title="04 索引和切片"></a>04 索引和切片</h3><p>就像在任何其他 Python 数组中一样，<strong>张量中的元素可以通过索引访问</strong>。与任何 Python 数组一样：第一个元素的索引是0，最后一个元素索引是-1；可以指定范围以包含第一个元素和最后一个之前的元素。<br>[[Python基础#切片语法]]</p>
<p>如下所示，我们可以用 <code>[-1]</code> 选择最后一个元素，可以用 <code>[1:3]</code> 选择第二个和第三个元素：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X[-<span class="number">1</span>], X[<span class="number">1</span>:<span class="number">3</span>]</span><br></pre></td></tr></table></figure>

<p>除读取外，我们还可以通过指定索引来将元素写入矩阵。</p>
<p>如果我们想为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值。例如，<code>[0:2, :]</code> 访问第1行和第2行，其中“:”代表沿轴1（列）的所有元素。虽然我们讨论的是矩阵的索引，但这也适用于向量和超过2个维度的张量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X[<span class="number">0</span>:<span class="number">2</span>, :] = <span class="number">12</span></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">X=tensor([[<span class="number">12.</span>, <span class="number">12.</span>, <span class="number">12.</span>, <span class="number">12.</span>],</span><br><span class="line">        [<span class="number">12.</span>, <span class="number">12.</span>, <span class="number">12.</span>, <span class="number">12.</span>],</span><br><span class="line">        [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>]])</span><br></pre></td></tr></table></figure>

<h3 id="05-节省内存"><a href="#05-节省内存" class="headerlink" title="05 节省内存"></a>05 节省内存</h3><p>运行一些操作可能会导致为新结果分配内存。例如，如果我们用 <code>Y = X + Y</code>，我们将取消引用 <code>Y</code> 指向的张量，而是指向新分配的内存处的张量。</p>
<p>这可能是不可取的，原因有两个：</p>
<ol>
<li>首先，我们不想总是不必要地分配内存。在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。通常情况下，我们希望原地执行这些更新；</li>
<li>如果我们不原地更新，其他引用仍然会指向旧的内存位置，这样我们的某些代码可能会无意中引用旧的参数。</li>
</ol>
<p>幸运的是，<strong>执行原地操作</strong>非常简单。我们可以**使用切片表示法将操作的结果分配给先前分配的数组，例如 <code>Y[:] = &lt;expression&gt;</code>**。为了说明这一点，我们首先创建一个新的矩阵 <code>Z</code>，其形状与另一个 <code>Y</code> 相同，使用 <code>zeros_like</code> 来分配一个全0的块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Z = torch.zeros_like(Y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;id(Z):&#x27;</span>, <span class="built_in">id</span>(Z)) <span class="comment">#Python的id()函数返回内存中引用对象的确切地址。</span></span><br><span class="line">Z[:] = X + Y</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;id(Z):&#x27;</span>, <span class="built_in">id</span>(Z))</span><br><span class="line"></span><br><span class="line"><span class="comment">#id(Z): 140327634811696</span></span><br><span class="line"><span class="comment">#id(Z): 140327634811696</span></span><br></pre></td></tr></table></figure>

<p>如果在后续计算中没有重复使用<code>X</code>， 我们也可以使用<code>X[:] = X + Y</code>或<code>X += Y</code>来减少操作的内存开销。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">before = <span class="built_in">id</span>(X)</span><br><span class="line">X += Y</span><br><span class="line"><span class="built_in">id</span>(X) == before</span><br><span class="line"><span class="comment">#True</span></span><br></pre></td></tr></table></figure>

<h3 id="06-转换为其他-Python-对象"><a href="#06-转换为其他-Python-对象" class="headerlink" title="06 转换为其他 Python 对象"></a>06 转换为其他 Python 对象</h3><p>将Pytorch定义的张量转换为 NumPy 张量（<code>ndarray</code>）很容易，反之也同样容易。 torch 张量和 numpy 数组将共享它们的底层内存，就地操作更改一个张量也会同时更改另一个张量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">A = X.numpy()</span><br><span class="line">B = torch.tensor(A)</span><br><span class="line"><span class="built_in">type</span>(A), <span class="built_in">type</span>(B)</span><br><span class="line"></span><br><span class="line"><span class="comment"># (numpy.ndarray, torch.Tensor) </span></span><br></pre></td></tr></table></figure>

<p>要将大小为1的张量转换为 Python 标量，我们可以调用 <code>item</code> 函数或 Python 的内置函数进行类型转换。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">3.5</span>])</span><br><span class="line">a, a.item(), <span class="built_in">float</span>(a), <span class="built_in">int</span>(a)</span><br><span class="line"><span class="comment">#(tensor([3.5000]), 3.5, 3.5, 3)</span></span><br></pre></td></tr></table></figure>

<h2 id="2-数据预处理"><a href="#2-数据预处理" class="headerlink" title="2 数据预处理"></a>2 数据预处理</h2><p>在 Python 中常用的数据分析工具中，我们通常使用 <code>pandas</code> 软件包进行数据预处理。像庞大的 Python 生态系统中的许多其他扩展包一样，<code>pandas</code> 可以与张量兼容</p>
<h3 id="01-读取数据集"><a href="#01-读取数据集" class="headerlink" title="01 读取数据集"></a>01 读取数据集</h3><h4 id="读取CSV"><a href="#读取CSV" class="headerlink" title="读取CSV"></a>读取CSV</h4><p>创建一个数据集合存入CSV</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">os.makedirs(os.path.join(<span class="string">&#x27;..&#x27;</span>, <span class="string">&#x27;data&#x27;</span>), exist_ok=<span class="literal">True</span>)</span><br><span class="line">data_file = os.path.join(<span class="string">&#x27;..&#x27;</span>, <span class="string">&#x27;data&#x27;</span>, <span class="string">&#x27;house_tiny.csv&#x27;</span>)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(data_file, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&#x27;NumRooms,Alley,Price\n&#x27;</span>)  <span class="comment"># 列名</span></span><br><span class="line">    f.write(<span class="string">&#x27;NA,Pave,127500\n&#x27;</span>)  <span class="comment"># 每行表示一个数据样本</span></span><br><span class="line">    f.write(<span class="string">&#x27;2,NA,106000\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;4,NA,178100\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;NA,NA,140000\n&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://cdn.jsdmirror.com/gh/liuke101/PicGo-For-Hexo/image/202408010046332.png" alt="Pasted image 20231111173516|350"><br>调用 <code>read_csv()</code> 函数即可读取改 CSV 文件</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(data_file)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出,最左边一排是自动生成的序号，不是数据</span></span><br><span class="line">   NumRooms Alley   Price</span><br><span class="line"><span class="number">0</span>       NaN  Pave  <span class="number">127500</span></span><br><span class="line"><span class="number">1</span>       <span class="number">2.0</span>   NaN  <span class="number">106000</span></span><br><span class="line"><span class="number">2</span>       <span class="number">4.0</span>   NaN  <span class="number">178100</span></span><br><span class="line"><span class="number">3</span>       NaN   NaN  <span class="number">140000</span></span><br></pre></td></tr></table></figure>

<h4 id="Dataset-和-DataLoader"><a href="#Dataset-和-DataLoader" class="headerlink" title="Dataset 和 DataLoader"></a>Dataset 和 DataLoader</h4><p><a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html">数据集和数据加载器 - PyTorch 教程 2.2.1+cu121 文档 — Datasets &amp; DataLoaders — PyTorch Tutorials 2.2.1+cu121 documentation</a></p>
<p>获取数据及其 label</p>
<ul>
<li>提供两数据加载函数: <code>torch.utils.data.DataLoader</code> 和 <code>torch.utils.data.Dataset</code></li>
<li>提供了许多预加载的数据集（例如 FashionMNIST)，这些数据集是 torch. utils. data. Dataset 的子类，并对于特定数据实现了特定的功能。</li>
<li>实现数据集代码与模型训练代码分离，以获得更好的可读性和模块化</li>
<li>制作自己的数据集必须要实现三个函数:<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">_init_，_len_，_getitem__</span><br></pre></td></tr></table></figure>
init 函数在实例化 Dataset 对象时运行一次<br>len 返回数据集中样本的数量<br>getitem 函数的作用是: 从给定索引 idx, 从数据集中加载并返回一个样本并将其转换为张量。</li>
</ul>
<h3 id="02-处理缺失值"><a href="#02-处理缺失值" class="headerlink" title="02 处理缺失值"></a>02 处理缺失值</h3><p>注意，“<code>NaN</code>”项代表缺失值。为了处理缺失的数据，典型的方法包括<strong>插值法</strong>和<strong>删除法</strong>，其中插值法用一个替代值弥补缺失值，而删除法则直接忽略缺失值。<strong>在这里，我们将考虑插值法。</strong></p>
<p>通过位置索引 <code>iloc</code>，我们将 <code>data</code>（上节中的 CSV 数据，5 行 3 列） 分成 <code>inputs</code> 和 <code>outputs</code>，其中前者为 <code>data</code> 的前两列，而后者为 <code>data</code> 的最后一列。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">inputs, outputs = data.iloc[:, <span class="number">0</span>:<span class="number">2</span>], data.iloc[:, <span class="number">2</span>]</span><br><span class="line"><span class="comment">#此时input为</span></span><br><span class="line">   NumRooms Alley</span><br><span class="line"><span class="number">0</span>       NaN  Pave</span><br><span class="line"><span class="number">1</span>       <span class="number">2.0</span>   NaN</span><br><span class="line"><span class="number">2</span>       <span class="number">4.0</span>   NaN</span><br><span class="line"><span class="number">3</span>       NaN   NaN</span><br><span class="line"></span><br><span class="line"><span class="comment">#output为</span></span><br><span class="line"><span class="number">0</span>    <span class="number">127500.0</span></span><br><span class="line"><span class="number">1</span>    <span class="number">106000.0</span></span><br><span class="line"><span class="number">2</span>    <span class="number">178100.0</span></span><br><span class="line"><span class="number">3</span>    <span class="number">140000.0</span></span><br><span class="line"><span class="number">4</span>         NaN</span><br><span class="line">Name: Price, dtype: float64</span><br></pre></td></tr></table></figure>

<p>对于 <code>inputs</code> 中缺少的数值，我们用同一列的均值替换“NaN”项。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">inputs = inputs.fillna(inputs.mean())</span><br><span class="line"><span class="built_in">print</span>(inputs)</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line">   NumRooms Alley</span><br><span class="line"><span class="number">0</span>       <span class="number">3.0</span>  Pave</span><br><span class="line"><span class="number">1</span>       <span class="number">2.0</span>   NaN</span><br><span class="line"><span class="number">2</span>       <span class="number">4.0</span>   NaN</span><br><span class="line"><span class="number">3</span>       <span class="number">3.0</span>   NaN</span><br></pre></td></tr></table></figure>

<p>对于 <code>inputs</code> 中的类别值或离散值，我们将“NaN”视为一个类别。由于“Alley”列只接受两种类型的类别值“Pave”和“NaN”， <code>pandas</code> 可以自动将此列转换为两列“Alley_Pave”和“Alley_nan”。“Alley”列为“Pave”的行会将“Alley_Pave”的值设置为1，“Alley_nan”的值设置为0。缺少“Alley”列的行会将“Alley_Pave”和“Alley_nan”分别设置为0和1。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">inputs = pd.get_dummies(inputs, dummy_na=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(inputs)</span><br><span class="line"><span class="comment">###</span></span><br><span class="line">   NumRooms  Alley_Pave  Alley_nan</span><br><span class="line"><span class="number">0</span>       <span class="number">3.0</span>           <span class="number">1</span>          <span class="number">0</span></span><br><span class="line"><span class="number">1</span>       <span class="number">2.0</span>           <span class="number">0</span>          <span class="number">1</span></span><br><span class="line"><span class="number">2</span>       <span class="number">4.0</span>           <span class="number">0</span>          <span class="number">1</span></span><br><span class="line"><span class="number">3</span>       <span class="number">3.0</span>           <span class="number">0</span>          <span class="number">1</span></span><br></pre></td></tr></table></figure>

<h3 id="03-转换为张量格式"><a href="#03-转换为张量格式" class="headerlink" title="03 转换为张量格式"></a>03 转换为张量格式</h3><p>现在 <code>inputs</code> 和 <code>outputs</code> 中的所有条目都是数值类型，它们可以转换为张量格式, 使用 <code>to_numpy</code>。当数据采用张量格式后，可以通过张量函数来进一步操作。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">X = torch.tensor(inputs.to_numpy(dtype=<span class="built_in">float</span>))</span><br><span class="line">y = torch.tensor(outputs.to_numpy(dtype=<span class="built_in">float</span>))</span><br><span class="line">X, y</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">(tensor([[<span class="number">3.</span>, <span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">2.</span>, <span class="number">0.</span>, <span class="number">1.</span>],</span><br><span class="line">         [<span class="number">4.</span>, <span class="number">0.</span>, <span class="number">1.</span>],</span><br><span class="line">         [<span class="number">3.</span>, <span class="number">0.</span>, <span class="number">1.</span>]], dtype=torch.float64),</span><br><span class="line"> tensor([<span class="number">127500.</span>, <span class="number">106000.</span>, <span class="number">178100.</span>, <span class="number">140000.</span>], dtype=torch.float64))</span><br></pre></td></tr></table></figure>

<h2 id="3-线性代数"><a href="#3-线性代数" class="headerlink" title="3 线性代数"></a>3 线性代数</h2><blockquote>
<p>[!info] 约定</p>
<ul>
<li>列向量是向量的默认方向。在数学中，向量可以写为：$\mathbf{x}&#x3D;\begin{bmatrix}x_1\x_2\\vdots\x_n\end{bmatrix}$</li>
<li>尽管单个向量的默认方向是列向量，但在表示表格数据集的矩阵中，将每个数据样本作为矩阵中的行向量更为常见。这种约定将支持常见的深度学习实践。例如，沿着张量的最外轴，我们可以访问或遍历小批量的数据样本。</li>
</ul>
</blockquote>
<h3 id="01-标量"><a href="#01-标量" class="headerlink" title="01 标量"></a>01 标量</h3><p>标量由只有一个元素的张量表示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor(<span class="number">3.0</span>)</span><br><span class="line">y = torch.tensor(<span class="number">2.0</span>)</span><br><span class="line"></span><br><span class="line">x + y, x * y, x / y, x**y</span><br><span class="line"></span><br><span class="line"><span class="comment"># (tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))</span></span><br></pre></td></tr></table></figure>
<h3 id="02-向量"><a href="#02-向量" class="headerlink" title="02 向量"></a>02 向量</h3><p>向量可以被视为标量值组成的列表。人们通过一维张量表示向量。一般来说，张量可以具有任意长度，取决于机器的内存限制。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">4</span>) </span><br><span class="line"><span class="comment"># tensor([0, 1, 2, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#使用下标引用向量的任意元素</span></span><br><span class="line">x[<span class="number">3</span>] </span><br><span class="line"><span class="comment"># tensor(3)</span></span><br></pre></td></tr></table></figure>
<h4 id="点积"><a href="#点积" class="headerlink" title="点积"></a>点积</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = tensor([<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>])</span><br><span class="line">y = tensor([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>])</span><br><span class="line"></span><br><span class="line">torch.dot(x, y) = tensor(<span class="number">6.</span>))</span><br></pre></td></tr></table></figure>

<p>注意，我们也可以通过执行按元素乘法，然后进行求和来表示两个向量的点积：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">sum</span>(x * y)</span><br></pre></td></tr></table></figure>

<p><img src="https://cdn.jsdmirror.com/gh/liuke101/PicGo-For-Hexo/image/202408010046333.png" alt="Pasted image 20231111205452"></p>
<h3 id="03-长度、维度和形状"><a href="#03-长度、维度和形状" class="headerlink" title="03 长度、维度和形状"></a>03 长度、维度和形状</h3><blockquote>
<p>[!NOTE] 维度<br>维度（dimension）这个词在不同上下文时往往会有不同的含义，这经常会使人感到困惑。为了清楚起见，我们在此明确一下： </p>
<ul>
<li>向量或轴的维度被用来表示向量或轴的长度，即向量或轴的元素数量。</li>
<li>张量的维度用来表示张量具有的轴数。在这个意义上，张量的某个轴的维数就是这个轴的长度。</li>
</ul>
</blockquote>
<ul>
<li><strong>向量的长度</strong>通常称为向量的<strong>维度（dimension）</strong>。<ul>
<li>与普通的 Python 数组一样，我们可以通过调用 Python 的内置 <code>len()</code> 函数来访问张量的长度。</li>
</ul>
</li>
<li><strong>形状（shape）是一个元素组</strong>，列出了<strong>张量沿每个轴的长度（维数）</strong>。对于只有一个轴的张量，形状只有一个元素。<ul>
<li>当用张量表示一个向量（只有一个轴）时，我们也可以通过 <code>.shape</code> 属性访问向量的长度。</li>
</ul>
</li>
</ul>
<h3 id="04-矩阵"><a href="#04-矩阵" class="headerlink" title="04 矩阵"></a>04 矩阵</h3><p>当调用函数来实例化张量时， 我们可以通过指定两个分量 $m$ 和 $n$ 来创建一个形状为 $m×n$ 的矩阵。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">A = np.arange(<span class="number">20</span>).reshape(<span class="number">5</span>, <span class="number">4</span>)</span><br><span class="line"><span class="comment">###</span></span><br><span class="line">array([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">       [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>],</span><br><span class="line">       [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>],</span><br><span class="line">       [<span class="number">12.</span>, <span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>],</span><br><span class="line">       [<span class="number">16.</span>, <span class="number">17.</span>, <span class="number">18.</span>, <span class="number">19.</span>]])</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>矩阵的转置</strong>：<code>A.T</code></li>
</ul>
<h4 id="矩阵-向量积"><a href="#矩阵-向量积" class="headerlink" title="矩阵-向量积"></a>矩阵-向量积</h4><p>在代码中使用张量表示矩阵-向量积，我们使用 <code>mv</code> 函数。当我们为矩阵 <code>A</code> 和向量 <code>x</code> 调用 <code>torch.mv(A, x)</code> 时，会执行矩阵-向量积。注意，<code>A</code> 的列维数（沿轴 1 的长度）必须与 <code>x</code> 的维数（其长度）相同。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.mv(A, x)</span><br></pre></td></tr></table></figure>

<h4 id="矩阵-矩阵积"><a href="#矩阵-矩阵积" class="headerlink" title="矩阵-矩阵积"></a>矩阵-矩阵积</h4><h5 id="torch-mul"><a href="#torch-mul" class="headerlink" title="torch.mul ()"></a>torch.mul ()</h5><ul>
<li><code>torch.mul(a, b)</code> 是矩阵 a 和 b <code>对应位相乘</code></li>
<li><code>torch.mul(a, b)中a和b的维度相等</code>，但是，对应维度上的数字可以不同，可以用利用广播机制扩展到相同的形状，再进行点乘操作</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 比如a的维度是(1, 2)，b的维度是(1, 2)，返回的仍是(1, 2)的矩阵</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.rand(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = torch.rand(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.mul(a, b)  <span class="comment"># 返回 1*2 的tensor</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 乘列向量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.ones(<span class="number">3</span>,<span class="number">4</span>) </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">       [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">       [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = torch.Tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]).reshape((<span class="number">3</span>,<span class="number">1</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b</span><br><span class="line">tensor([[<span class="number">1.</span>],</span><br><span class="line">       [<span class="number">2.</span>],</span><br><span class="line">       [<span class="number">3.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.mul(a, b)</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">       [<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>],</span><br><span class="line">       [<span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>]])</span><br></pre></td></tr></table></figure>

<h5 id="torch-mm"><a href="#torch-mm" class="headerlink" title="torch.mm ()"></a>torch.mm ()</h5><ul>
<li><code>torch.mm(a, b)</code> 是矩阵 a 和 b 矩阵相乘，比如 a 的维度是 (3, 4)，b 的维度是 (4, 2)，返回的就是 (3, 2) 的矩阵 </li>
<li>torch.mm (a, b)<strong>针对二维矩阵</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.ones(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = torch.ones(<span class="number">4</span>,<span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.mm(a, b)</span><br><span class="line">tensor([[<span class="number">4.</span>, <span class="number">4.</span>],</span><br><span class="line">        [<span class="number">4.</span>, <span class="number">4.</span>],</span><br><span class="line">        [<span class="number">4.</span>, <span class="number">4.</span>]])</span><br></pre></td></tr></table></figure>

<p><code>mm()是mutmul()的简称？</code></p>
<h5 id="torch-matmul"><a href="#torch-matmul" class="headerlink" title="torch.matmul ()"></a>torch.matmul ()</h5><ul>
<li><code>torch.matmul(a, b)</code> 也是一种类似于矩阵相乘操作的 tensor 联乘操作，一般是高维矩阵 a 和 b 相乘，但是它可以利用 python 中的广播机制，处理一些维度不同的 tensor 结构进行相乘操作。</li>
</ul>
<h6 id="3-1-输入都是二维"><a href="#3-1-输入都是二维" class="headerlink" title="3.1 输入都是二维"></a>3.1 输入都是二维</h6><ul>
<li><p>当输入都是二维时，就是普通的矩阵乘法，和<code>tensor.mm()</code> 函数用法相同。  </p>
<p><img src="https://cdn.jsdmirror.com/gh/liuke101/PicGo-For-Hexo/image/202408010046334.png" alt="eeb7173e2f99224584f25504227c0685_MD5"></p>
</li>
</ul>
<h6 id="3-2-输入都是三维"><a href="#3-2-输入都是三维" class="headerlink" title="3.2 输入都是三维"></a>3.2 输入都是三维</h6><ul>
<li><p>下面看一个两个都是 3 维的例子：  </p>
<p><img src="https://cdn.jsdmirror.com/gh/liuke101/PicGo-For-Hexo/image/202408010046335.png" alt="4ce698041329917ba1df09b3f9ae9d5f_MD5"></p>
<p>将 b 的第 0 维 1broadcast 成 2 提出来，后两维做矩阵乘法即可。</p>
</li>
</ul>
<h6 id="3-3-输入的维度不同"><a href="#3-3-输入的维度不同" class="headerlink" title="3.3 输入的维度不同"></a>3.3 输入的维度不同</h6><ul>
<li><p>当输入有多维时，把多出的一维作为 batch 提出来，其他部分做矩阵乘法。  </p>
<p><img src="https://cdn.jsdmirror.com/gh/liuke101/PicGo-For-Hexo/image/202408010046336.png" alt="e78d4960fb31324a695d44fc9d98f086_MD5"></p>
</li>
<li><p>再看一个复杂一点的，是官网的例子：  </p>
<p><img src="https://cdn.jsdmirror.com/gh/liuke101/PicGo-For-Hexo/image/202408010046337.png" alt="c444f333ec274e2816b2f839e2404f43_MD5"></p>
<p>首先把 a 的第 0 维 2 作为 batch 提出来，则 a 和 b 都可看作三维。再把 a 的 1broadcat 成 5，提取公因式 5。（这样说虽然不严谨，但是便于理解。）然后 a 剩下 (3,4)，b 剩下 (4,2)，做矩阵乘法得到 (3,2)。</p>
</li>
</ul>
<h3 id="05-张量"><a href="#05-张量" class="headerlink" title="05 张量"></a>05 张量</h3><p>张量（本小节中的“张量”指代数对象）是描述具有任意数量轴的 $n$ 维数组的通用方法。例如，向量是一阶张量，矩阵是二阶张量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">X = torch.arange(<span class="number">24</span>).reshape(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor([[[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">         [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>],</span><br><span class="line">         [ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>],</span><br><span class="line">         [<span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>],</span><br><span class="line">         [<span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>]]])</span><br></pre></td></tr></table></figure>

<h4 id="张量算法"><a href="#张量算法" class="headerlink" title="张量算法"></a>张量算法</h4><p>给定具有相同形状的任意两个张量，任何按元素二元运算的结果都将是相同形状的张量。例如，将两个相同形状的矩阵相加，会在这两个矩阵上执行元素加法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">A = torch.arange(<span class="number">20</span>, dtype=torch.float32).reshape(<span class="number">5</span>, <span class="number">4</span>)</span><br><span class="line">B = A.clone()  <span class="comment"># 通过分配新内存，将A的一个副本分配给B</span></span><br><span class="line"><span class="comment">### </span></span><br><span class="line">A </span><br><span class="line">(tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">         [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>],</span><br><span class="line">         [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>],</span><br><span class="line">         [<span class="number">12.</span>, <span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>],</span><br><span class="line">         [<span class="number">16.</span>, <span class="number">17.</span>, <span class="number">18.</span>, <span class="number">19.</span>]]),</span><br><span class="line"><span class="comment">###  </span></span><br><span class="line">A + B </span><br><span class="line">tensor([[ <span class="number">0.</span>,  <span class="number">2.</span>,  <span class="number">4.</span>,  <span class="number">6.</span>],</span><br><span class="line">         [ <span class="number">8.</span>, <span class="number">10.</span>, <span class="number">12.</span>, <span class="number">14.</span>],</span><br><span class="line">         [<span class="number">16.</span>, <span class="number">18.</span>, <span class="number">20.</span>, <span class="number">22.</span>],</span><br><span class="line">         [<span class="number">24.</span>, <span class="number">26.</span>, <span class="number">28.</span>, <span class="number">30.</span>],</span><br><span class="line">         [<span class="number">32.</span>, <span class="number">34.</span>, <span class="number">36.</span>, <span class="number">38.</span>]]))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>两个矩阵的按元素乘法称为阿达马积（Hadamard product）：<br>$$\mathbf{A}\odot\mathbf{B}&#x3D;\begin{bmatrix}a_{11}b_{11}&amp;a_{12}b_{12}&amp;\ldots&amp;a_{1n}b_{1n}\a_{21}b_{21}&amp;a_{22}b_{22}&amp;\ldots&amp;a_{2n}b_{2n}\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\a_{m1}b_{m1}&amp;a_{m2}b_{m2}&amp;\ldots&amp;a_{mn}b_{mn}\end{bmatrix}$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">A * B = tensor([[  <span class="number">0.</span>,   <span class="number">1.</span>,   <span class="number">4.</span>,   <span class="number">9.</span>],</span><br><span class="line">        [ <span class="number">16.</span>,  <span class="number">25.</span>,  <span class="number">36.</span>,  <span class="number">49.</span>],</span><br><span class="line">        [ <span class="number">64.</span>,  <span class="number">81.</span>, <span class="number">100.</span>, <span class="number">121.</span>],</span><br><span class="line">        [<span class="number">144.</span>, <span class="number">169.</span>, <span class="number">196.</span>, <span class="number">225.</span>],</span><br><span class="line">        [<span class="number">256.</span>, <span class="number">289.</span>, <span class="number">324.</span>, <span class="number">361.</span>]])</span><br></pre></td></tr></table></figure>

<p>将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">a = <span class="number">2</span></span><br><span class="line">X = torch.arange(<span class="number">24</span>).reshape(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="comment">###</span></span><br><span class="line">a + X</span><br><span class="line">(tensor([[[ <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>],</span><br><span class="line">          [ <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>],</span><br><span class="line">          [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>]],</span><br><span class="line"></span><br><span class="line">         [[<span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>],</span><br><span class="line">          [<span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>, <span class="number">21</span>],</span><br><span class="line">          [<span class="number">22</span>, <span class="number">23</span>, <span class="number">24</span>, <span class="number">25</span>]]]),</span><br><span class="line"></span><br><span class="line"><span class="comment">### </span></span><br><span class="line">(a * X).shape</span><br><span class="line"> torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]))</span><br></pre></td></tr></table></figure>

<h4 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h4><h5 id="求和"><a href="#求和" class="headerlink" title="求和"></a>求和</h5><p>计算张量元素的总和, 张量可以为任意形状<br><strong>默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">4</span>, dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">x</span><br><span class="line">(tensor([<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>])</span><br><span class="line"><span class="comment">###</span></span><br><span class="line">x.<span class="built_in">sum</span>()</span><br><span class="line">tensor(<span class="number">6.</span>))</span><br></pre></td></tr></table></figure>

<p><strong>我们还可以指定张量沿哪一个轴来通过求和降低维度</strong>。以矩阵为例，为了通过求和所有行的元素来降维（轴0，即 x 轴），可以在调用函数时指定 <code>axis=0</code>。<strong>由于输入矩阵沿0轴降维以生成输出向量，因此行的维数在输出形状中消失。</strong>（在水平方向将矩阵压扁。）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">A  = (tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">         [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>],</span><br><span class="line">         [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>],</span><br><span class="line">         [<span class="number">12.</span>, <span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>],</span><br><span class="line">         [<span class="number">16.</span>, <span class="number">17.</span>, <span class="number">18.</span>, <span class="number">19.</span>]]),</span><br><span class="line">         </span><br><span class="line"><span class="comment">###</span></span><br><span class="line">A_sum_axis0 = A.<span class="built_in">sum</span>(axis=<span class="number">0</span>) <span class="comment">#沿0轴（x轴）求和</span></span><br><span class="line">(tensor([<span class="number">40.</span>, <span class="number">45.</span>, <span class="number">50.</span>, <span class="number">55.</span>])</span><br><span class="line"></span><br><span class="line"> <span class="comment">###</span></span><br><span class="line">A_sum_axis0, A_sum_axis0.shape</span><br><span class="line">torch.Size([<span class="number">4</span>]))</span><br></pre></td></tr></table></figure>

<p>同理，指定 <code>axis=1</code> 将通过汇总所有列的元素降维（轴1，即 y 轴）。因此，列的的维数在输出形状中消失。（在竖直方向将矩阵压扁）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">###</span></span><br><span class="line">A_sum_axis1 = A.<span class="built_in">sum</span>(axis=<span class="number">1</span>)</span><br><span class="line">(tensor([ <span class="number">6.</span>,</span><br><span class="line">          <span class="number">22.</span>,</span><br><span class="line">          <span class="number">38.</span>,</span><br><span class="line">          <span class="number">54.</span>,</span><br><span class="line">          <span class="number">70.</span>])</span><br><span class="line"> </span><br><span class="line"><span class="comment">###</span></span><br><span class="line">A_sum_axis1, A_sum_axis1.shape</span><br><span class="line">torch.Size([<span class="number">5</span>]))</span><br></pre></td></tr></table></figure>

<p>沿着行和列对矩阵求和，等价于对矩阵的所有元素进行求和</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A.<span class="built_in">sum</span>(axis=[<span class="number">0</span>, <span class="number">1</span>])  <span class="comment"># 结果和A.sum()相同</span></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor(<span class="number">190.</span>)</span><br></pre></td></tr></table></figure>

<p>可以指定保持在原始张量的轴数 <code>keepdim=True</code>，而不折叠求和的维度:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X = torch.tensor([[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>],</span><br><span class="line">                  [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>]]) <span class="comment">#形状为（2, 3），二维</span></span><br><span class="line">X.<span class="built_in">sum</span>(<span class="number">0</span>) <span class="comment"># tensor([5., 7., 9.])  形状为(3,)，降维到1维</span></span><br><span class="line">X.<span class="built_in">sum</span>(<span class="number">0</span>, keepdim=<span class="literal">True</span>)  <span class="comment"># tensor([[5., 7., 9.]])  形状为（1, 3） ，仍是二维</span></span><br><span class="line"></span><br><span class="line">X.<span class="built_in">sum</span>(<span class="number">1</span>) <span class="comment">#tensor([ 6., 15.])  形状为(2,)，降维到1维</span></span><br><span class="line">X.<span class="built_in">sum</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># tensor([[ 6.],</span></span><br><span class="line"><span class="comment">#        [15.]])</span></span><br><span class="line"><span class="comment">#  形状为（2, 1），仍是二维</span></span><br></pre></td></tr></table></figure>
<h5 id="求平均"><a href="#求平均" class="headerlink" title="求平均"></a>求平均</h5><ul>
<li>调用 <code>mean()</code> 函数来计算任意形状张量的平均值。</li>
<li>通过将总和除以元素总数来计算平均值。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A.mean()</span><br><span class="line"><span class="comment">#等价</span></span><br><span class="line">A.<span class="built_in">sum</span>() / A.numel()</span><br></pre></td></tr></table></figure></li>
</ul>
<p>计算平均值的函数也可以沿指定轴降低张量的维度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A.mean(axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">A.<span class="built_in">sum</span>(axis=<span class="number">0</span>) / A.shape[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<h4 id="非降维求和"><a href="#非降维求和" class="headerlink" title="非降维求和"></a>非降维求和</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">A  = (tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">         [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>],</span><br><span class="line">         [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>],</span><br><span class="line">         [<span class="number">12.</span>, <span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>],</span><br><span class="line">         [<span class="number">16.</span>, <span class="number">17.</span>, <span class="number">18.</span>, <span class="number">19.</span>]]),</span><br></pre></td></tr></table></figure>

<p>有时在调用函数来计算总和或均值时保持轴数不变会很有用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sum_A = A.<span class="built_in">sum</span>(axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor([[ <span class="number">6.</span>],</span><br><span class="line">        [<span class="number">22.</span>],</span><br><span class="line">        [<span class="number">38.</span>],</span><br><span class="line">        [<span class="number">54.</span>],</span><br><span class="line">        [<span class="number">70.</span>]])</span><br></pre></td></tr></table></figure>

<p>例如，由于 <code>sum_A</code> 在对每行进行求和后仍保持两个轴，我们可以通过广播将 <code>A</code> 除以 <code>sum_A</code>。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">A / sum_A</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor([[<span class="number">0.0000</span>, <span class="number">0.1667</span>, <span class="number">0.3333</span>, <span class="number">0.5000</span>],</span><br><span class="line">        [<span class="number">0.1818</span>, <span class="number">0.2273</span>, <span class="number">0.2727</span>, <span class="number">0.3182</span>],</span><br><span class="line">        [<span class="number">0.2105</span>, <span class="number">0.2368</span>, <span class="number">0.2632</span>, <span class="number">0.2895</span>],</span><br><span class="line">        [<span class="number">0.2222</span>, <span class="number">0.2407</span>, <span class="number">0.2593</span>, <span class="number">0.2778</span>],</span><br><span class="line">        [<span class="number">0.2286</span>, <span class="number">0.2429</span>, <span class="number">0.2571</span>, <span class="number">0.2714</span>]])</span><br></pre></td></tr></table></figure>

<p>如果我们想沿某个轴计算 <code>A</code> 元素的累积总和，比如 <code>axis=0</code>（按行计算），可以调用 <code>cumsum</code> 函数。此函数不会沿任何轴降低输入张量的维度。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">A.cumsum(axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">        [ <span class="number">4.</span>,  <span class="number">6.</span>,  <span class="number">8.</span>, <span class="number">10.</span>], <span class="comment">#原来的第一行+第二行</span></span><br><span class="line">        [<span class="number">12.</span>, <span class="number">15.</span>, <span class="number">18.</span>, <span class="number">21.</span>], <span class="comment">#第第一行+第二行+第三行</span></span><br><span class="line">        [<span class="number">24.</span>, <span class="number">28.</span>, <span class="number">32.</span>, <span class="number">36.</span>], <span class="comment">#以此类推...</span></span><br><span class="line">        [<span class="number">40.</span>, <span class="number">45.</span>, <span class="number">50.</span>, <span class="number">55.</span>]])</span><br></pre></td></tr></table></figure>

<h3 id="06-范数"><a href="#06-范数" class="headerlink" title="06 范数"></a>06 范数</h3><p><strong>线性代数中最有用的一些运算符是范数（norm）。非正式地说，向量的范数是表示一个向量有多大。</strong> 这里考虑的大小（size）概念不涉及维度，而是分量的大小。</p>
<p><strong>在线性代数中，向量范数是将向量映射到标量的函数 $f$ （即求大小）。</strong></p>
<p><strong>给定任意向量 $x$，向量范数要满足一些属性：</strong></p>
<ul>
<li>性质一：如果我们按常数因子 $\alpha$ 缩放向量的所有元素，其范数也会按相同常数因子的绝对值缩放：$$<br>f(\alpha\mathbf{x})&#x3D;|\alpha|f(\mathbf{x})<br>$$</li>
<li>性质二：是熟悉的三角不等式：$$<br>f(\mathbf{x}+\mathbf{y})\leq f(\mathbf{x})+f(\mathbf{y})<br>$$</li>
<li>性质三：范数必须是非负的，这是有道理的。因为在大多数情况下，任何东西的最小的_大小_是0。<br>$$<br>f(\mathbf{x})\geq0<br>$$ </li>
<li>性质四：要求范数最小为 0，当且仅当向量全由 0 组成。$$<br>\forall i,[\mathbf{x}]_i&#x3D;0\Leftrightarrow f(\mathbf{x})&#x3D;0.<br>$$<br>范数听起来很像距离的度量。欧几里得距离和毕达哥拉斯定理中的非负性概念和三角不等式可能会给出一些启发。<strong>事实上，欧几里得距离是一个 $L_2$ 范数： 假设 $n$ 维向量 $x$ 中的元素是 $x_1,\ldots,x_n$，其 $L_2$ 范数是向量元素平方和的平方根：</strong><br>$$|\mathbf{x}|<em>2&#x3D;\sqrt{\sum</em>{i&#x3D;1}^nx_i^2},$$</li>
</ul>
<p>其中，在 $L_2$ 范数中常常省略下标 $2$，也就是说 $‖x‖$ 等同于 $‖x‖_2$。在代码中，我们可以按如下方式计算向量的 $L_2$ 范数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">u = torch.tensor([<span class="number">3.0</span>, -<span class="number">4.0</span>])</span><br><span class="line">torch.norm(u)</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor(<span class="number">5.</span>)</span><br></pre></td></tr></table></figure>

<p>深度学习中更经常地使用 $L_2$ 范数的平方，也会经常遇到 <strong>$L_1$ 范数，它表示为向量元素的绝对值之和：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">abs</span>(u).<span class="built_in">sum</span>()</span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor(<span class="number">7.</span>)</span><br></pre></td></tr></table></figure>

<p>这些范数都是 $L_p$ 范数的特例：$\displaystyle|\mathbf{x}|<em>p&#x3D;\left(\sum</em>{i&#x3D;1}^n|x_i|^p\right)^{1&#x2F;p}$</p>
<p>类似于向量的 $L_2$ 范数，<strong>矩阵 $\mathbf{X}\in\mathbb{R}^{m\times n}$ 的 Frobenius 范数（Frobenius norm）是矩阵元素平方和的平方根：</strong><br>$$<br>|\mathbf{X}|<em>F&#x3D;\sqrt{\sum</em>{i&#x3D;1}^m\sum_{j&#x3D;1}^nx_{ij}^2}<br>$$<br>Frobenius 范数满足向量范数的所有性质，它就像是矩阵形向量的 $L_2$ 范数。调用以下函数将计算矩阵的 Frobenius 范数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.norm(torch.ones((<span class="number">4</span>, <span class="number">9</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor(<span class="number">6.</span>)</span><br></pre></td></tr></table></figure>

<h4 id="范数和目标"><a href="#范数和目标" class="headerlink" title="范数和目标"></a>范数和目标</h4><p>在深度学习中，我们经常试图解决优化问题： 最大化分配给观测数据的概率; 最小化预测和真实观测之间的距离。用向量表示物品（如单词、产品或新闻文章），以便最小化相似项目之间的距离，最大化不同项目之间的距离。 <strong>目标，或许是深度学习算法最重要的组成部分（除了数据），通常被表达为范数。</strong></p>
<h2 id="4-微积分"><a href="#4-微积分" class="headerlink" title="4 微积分"></a>4 微积分</h2><p>在深度学习中，我们“训练”模型，不断更新它们，使它们在看到越来越多的数据时变得越来越好。通常情况下，变得更好意味着最小化一个损失函数（loss function），即一个衡量“模型有多糟糕”这个问题的分数。最终，我们真正关心的是生成一个模型，它能够在从未见过的数据上表现良好。但“训练”模型只能将模型与我们实际能看到的数据相拟合。</p>
<p><strong>因此，我们可以将拟合模型的任务分解为两个关键问题：</strong></p>
<ul>
<li>_优化_（optimization）：用模型拟合观测数据的过程；</li>
<li>_泛化_（generalization）：数学原理和实践者的智慧，能够指导我们生成出有效性超出用于训练的数据集本身的模型。</li>
</ul>
<h3 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h3><p>梯度是一个向量，其分量是多变量函数相对于其所有变量的偏导数。</p>
<p>我们可以连结一个多元函数<strong>对其所有变量的偏导数</strong>，以得到该函数的梯度（gradient）<strong>向量</strong>。具体而言，设函数 $f:\mathbb{R}^n\to\mathbb{R}$ 的输入是一个 $n$ 维向量 $\mathbf{x}&#x3D;[x_1, x_2,\ldots, x_n]^\top$，并且输出是一个标量。函数 $f(x)$ 相对于 $x$ 的梯度是一个包含 $n$ 个偏导数的向量:<br>$$<br>\nabla_{\mathbf{x}}f(\mathbf{x})&#x3D;\left[\frac{\partial f(\mathbf{x})}{\partial x_1},\frac{\partial f(\mathbf{x})}{\partial x_2},\ldots,\frac{\partial f(\mathbf{x})}{\partial x_n}\right]^\top,<br>$$<br>其中 $\nabla_{\mathbf{x}}f(\mathbf{x})$ 通常在没有歧义时被 $\nabla f(\mathbf{x})$ 取代</p>
<p><img src="https://cdn.jsdmirror.com/gh/liuke101/PicGo-For-Hexo/image/202408010046338.png" alt="Pasted image 20231111213813"></p>
<h3 id="链式法则"><a href="#链式法则" class="headerlink" title="链式法则"></a>链式法则</h3><p>然而，上面方法可能很难找到梯度。这是因为在深度学习中，多元函数通常是复合（composite）的，所以难以应用上述任何规则来微分这些函数。幸运的是，<strong>链式法则可以被用来微分复合函数。</strong></p>
<p>让我们先考虑单变量函数。假设函数 $:y&#x3D;f(u)$ 和 $u&#x3D;g(x)$ 都是可微的，根据链式法则：<br>$$<br>\frac{dy}{dx}&#x3D;\frac{dy}{du}\frac{du}{dx}.<br>$$<br>现在考虑一个更一般的场景，即函数具有任意数量的变量的情况。假设可微分函数 $y$ 有变量 $u_1,u_2,…,u_3$，其中每个可微分函数 $u_i$ 都有变量 $x_1,x_2,…,x_3$。注意，$y$ 是 $x_1,x_2,…,x_3$ 的函数。对于任意 $i&#x3D;1,2,\ldots,n$，链式法则给出：<br>$$<br>\begin{aligned}\frac{\partial y}{\partial x_i}&amp;&#x3D;\frac{\partial y}{\partial u_1}\frac{\partial u_1}{\partial x_i}+\frac{\partial y}{\partial u_2}\frac{\partial u_2}{\partial x_i}+\cdots+\frac{\partial y}{\partial u_m}\frac{\partial u_m}{\partial x_i}\end{aligned}<br>$$</p>
<h3 id="自动微分"><a href="#自动微分" class="headerlink" title="自动微分"></a>自动微分</h3><p>Pytorch通过自动计算导数，即自动微分（automatic differentiation）来加快求导。实际中，根据设计好的模型，系统会构建一个计算图（computational graph），来跟踪计算是哪些数据通过哪些操作组合起来产生输出。自动微分使系统能够随后<strong>反向传播梯度</strong>。这里，_反向传播_（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。</p>
<h4 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h4><p>例子：$y&#x3D;2\mathbf{x}^\top\mathbf{x}$ 关于列向量 $x$ 求导</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = tensor([<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>])</span><br></pre></td></tr></table></figure>

<p><strong>在我们计算 $y$ 关于 $x$ 的梯度之前，需要一个地方来存储梯度</strong>。重要的是，我们不会在每次对一个参数求导时都分配新的内存。因为我们经常会成千上万次地更新相同的参数，每次都分配新的内存可能很快就会将内存耗尽。</p>
<blockquote>
<p>注意，一个标量函数关于向量 $x$ 的梯度是向量，并且与 $x$ 具有相同的形状。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将张量设置为需要梯度计算，这意味着在进行反向传播时，该张量的梯度将被计算和更新。</span></span><br><span class="line">x.requires_grad_(<span class="literal">True</span>)  <span class="comment"># 等价于x=torch.arange(4.0,requires_grad=True)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">x.grad = <span class="literal">None</span>  <span class="comment"># 默认值是None</span></span><br></pre></td></tr></table></figure>

<p>现在计算 $y$。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">y = <span class="number">2</span> * torch.dot(x, x)</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor(<span class="number">28.</span>, grad_fn=&lt;MulBackward0&gt;)</span><br></pre></td></tr></table></figure>

<p><code>x</code> 是一个长度为4的向量，计算 <code>x</code> 和 <code>x</code> 的点积（等于 $\mathbf{x}^\top\mathbf{x}$），得到了我们赋值给 <code>y</code> 的标量输出。接下来，<strong>通过调用反向传播函数来自动计算 <code>y</code> 关于 <code>x</code> 每个分量的梯度，并打印这些梯度。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">y.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">x.grad = tensor([ <span class="number">0.</span>,  <span class="number">4.</span>,  <span class="number">8.</span>, <span class="number">12.</span>])</span><br></pre></td></tr></table></figure>

<p>函数 $y$ 关于 $x$ 的梯度应为 $4x$，快速验证这个梯度计算是否正确</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x.grad == <span class="number">4</span> * x</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>])</span><br></pre></td></tr></table></figure>

<p>如果要继续计算 <code>x</code> 的另一个函数，要记得清空梯度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值</span></span><br><span class="line">x.grad.zero_()</span><br></pre></td></tr></table></figure>

<h4 id="非标量变量的反向传播"><a href="#非标量变量的反向传播" class="headerlink" title="非标量变量的反向传播"></a>非标量变量的反向传播</h4><p>当 <code>y</code> 不是标量时，向量 <code>y</code> 关于向量 <code>x</code> 的导数的最自然解释是一个矩阵。对于高阶和高维的 <code>y</code> 和 <code>x</code>，求导的结果可以是一个高阶张量。</p>
<p>然而，虽然这些更奇特的对象确实出现在高级机器学习中（包括深度学习中）， <strong>但当调用向量的反向计算时，我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数。这里，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。</span></span><br><span class="line"><span class="comment"># 本例只想求偏导数的和，所以传递一个1的梯度是合适的</span></span><br><span class="line">x.grad.zero_()</span><br><span class="line">y = x * x</span><br><span class="line"><span class="comment"># 等价于y.backward(torch.ones(len(x)))</span></span><br><span class="line">y.<span class="built_in">sum</span>().backward()</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">x.grad = tensor([<span class="number">0.</span>, <span class="number">2.</span>, <span class="number">4.</span>, <span class="number">6.</span>])</span><br></pre></td></tr></table></figure>

<h4 id="分离计算"><a href="#分离计算" class="headerlink" title="分离计算"></a>分离计算</h4><p>有时，我们希望将某些计算移动到记录的计算图之外。例如，假设 <code>y</code> 是作为 <code>x</code> 的函数计算的，而 <code>z</code> 则是作为 <code>y</code> 和 <code>x</code> 的函数计算的。想象一下，我们想计算 <code>z</code> 关于 <code>x</code> 的梯度，但由于某种原因，希望将 <code>y</code> 视为一个常数，并且只考虑到 <code>x</code> 在 <code>y</code> 被计算后发挥的作用。</p>
<p>这里可以分离 <code>y</code> 来返回一个新变量 <code>u</code>，该变量与 <code>y</code> 具有相同的值，但丢弃计算图中如何计算 <code>y</code> 的任何信息。换句话说，梯度不会向后流经 <code>u</code> 到 <code>x</code>。因此，下面的反向传播函数计算 <code>z=u*x</code> 关于 <code>x</code> 的偏导数，同时将 <code>u</code> 作为常数处理，而不是 <code>z=x*x*x</code> 关于 <code>x</code> 的偏导数。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line">y = x * x</span><br><span class="line">u = y.detach() <span class="comment"># 分离y，将上一行的乘积赋给新变量u</span></span><br><span class="line">z = u * x <span class="comment"># 此时u不再是x*x,而只是一个乘积</span></span><br><span class="line"></span><br><span class="line">z.<span class="built_in">sum</span>().backward() </span><br><span class="line">x.grad == u <span class="comment"># z=u*x 对x求导等于u。如果不分离y，这里应该是3x^2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>])</span><br></pre></td></tr></table></figure>

<p>由于记录了 <code>y</code> 的计算结果，我们可以随后在 <code>y</code> 上调用反向传播，得到 <code>y=x*x</code> 关于的 <code>x</code> 的导数，即 <code>2*x</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line"></span><br><span class="line">y.<span class="built_in">sum</span>().backward()</span><br><span class="line">x.grad == <span class="number">2</span> * x</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>])</span><br></pre></td></tr></table></figure>

<h4 id="Python-控制流的梯度计算"><a href="#Python-控制流的梯度计算" class="headerlink" title="Python 控制流的梯度计算"></a>Python 控制流的梯度计算</h4><p>使用自动微分的一个好处是： 即使构建函数的计算图需要通过 Python 控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度。</p>
<p>在下面的代码中，<code>while</code> 循环的迭代次数和 <code>if</code> 语句的结果都取决于输入 <code>a</code> 的值。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">a</span>):</span><br><span class="line">    b = a * <span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span> b.norm() &lt; <span class="number">1000</span>:</span><br><span class="line">        b = b * <span class="number">2</span></span><br><span class="line">    <span class="keyword">if</span> b.<span class="built_in">sum</span>() &gt; <span class="number">0</span>:</span><br><span class="line">        c = b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        c = <span class="number">100</span> * b</span><br><span class="line">    <span class="keyword">return</span> c</span><br></pre></td></tr></table></figure>

<p>让我们计算梯度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(size=(), requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#size=()表示创建一个标量（0维张量）</span></span><br><span class="line">d = f(a)</span><br><span class="line">d.backward()</span><br></pre></td></tr></table></figure>

<p>我们现在可以分析上面定义的 <code>f</code> 函数。请注意，它在其输入 <code>a</code> 中是分段线性的。换言之，对于任何 <code>a</code>，存在某个常量标量 <code>k</code>，使得 <code>f(a)=k*a</code>，其中 <code>k</code> 的值取决于输入 <code>a</code>，因此可以用 <code>d/a</code> 验证梯度是否正确。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a.grad == d / a</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h3 id="torch-no-grad"><a href="#torch-no-grad" class="headerlink" title="torch.no_grad()"></a>torch.no_grad()</h3><p><code>torch.no_grad()</code> 是一个上下文管理器，用于指定在其范围内的代码块中不进行梯度计算。<br>在某些情况下，我们可能只是希望使用模型进行推理或评估，而不需要计算梯度。这时，可以使用 <code>torch.no_grad()</code> 来关闭梯度计算，从而减少内存消耗并提高代码的执行效率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型和输入</span></span><br><span class="line">model = torch.nn.Linear(<span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">inputs = torch.randn(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用torch.no_grad()上下文管理器进行推理</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    outputs = model(inputs)</span><br><span class="line">    <span class="built_in">print</span>(outputs)</span><br></pre></td></tr></table></figure>

<p>在上述示例中，<code>torch.no_grad()</code> 上下文管理器包裹的代码块中，模型的输出 <code>outputs</code> 会被计算出来，但不会计算梯度，因此不会对模型的参数进行更新。</p>
<h2 id="5-概率"><a href="#5-概率" class="headerlink" title="5 概率"></a>5 概率</h2><p>简单地说，机器学习就是做出预测。</p>
<ul>
<li>我们可以从概率分布中采样。</li>
<li>我们可以使用联合分布、条件分布、Bayes定理、边缘化和独立性假设来分析多个随机变量。</li>
<li>期望和方差为概率分布的关键特征的概括提供了实用的度量形式。</li>
</ul>
<h2 id="6-查阅文档"><a href="#6-查阅文档" class="headerlink" title="6 查阅文档"></a>6 查阅文档</h2><ul>
<li><strong>查找模块中的所有函数和类</strong><br>为了知道模块中可以调用哪些函数和类，可以调用 <code>dir</code> 函数。例如，我们可以查询随机数生成模块中的所有属性：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">dir</span>(torch.distributions))</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">[<span class="string">&#x27;AbsTransform&#x27;</span>, <span class="string">&#x27;AffineTransform&#x27;</span>, <span class="string">&#x27;Bernoulli&#x27;</span>, <span class="string">&#x27;Beta&#x27;</span>, <span class="string">&#x27;Binomial&#x27;</span>, <span class="string">&#x27;CatTransform&#x27;</span>, <span class="string">&#x27;Categorical&#x27;</span>, <span class="string">&#x27;Cauchy&#x27;</span>, <span class="string">&#x27;Chi2&#x27;</span>......</span><br></pre></td></tr></table></figure>

<p>通常可以忽略以“<code>__</code>”（双下划线）开始和结束的函数，它们是Python中的特殊对象， 或以单个“<code>_</code>”（单下划线）开始的函数，它们通常是内部函数。 根据剩余的函数名或属性名，我们可能会猜测这个模块提供了各种生成随机数的方法， 包括从均匀分布（<code>uniform</code>）、正态分布（<code>normal</code>）和多项分布（<code>multinomial</code>）中采样。</p>
<ul>
<li><strong>查找特定函数和类的用法</strong><br>有关如何使用给定函数或类的更具体说明，可以调用 <code>help</code> 函数。例如，我们来查看张量 <code>ones</code> 函数的用法。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">help</span>(torch.ones)</span><br></pre></td></tr></table></figure></li>
</ul>
<p>在 Jupyter 记事本中，我们可以使用 <code>?</code> 指令在另一个浏览器窗口中显示文档。例如，<code>list?</code> 指令将创建与 <code>help(list)</code> 指令几乎相同的内容，并在新的浏览器窗口中显示它。此外，如果我们使用两个问号，如 <code>list??</code>，将显示实现该函数的 Python 代码。</p>
<h1 id="二、线性神经网络"><a href="#二、线性神经网络" class="headerlink" title="二、线性神经网络"></a>二、线性神经网络</h1><h2 id="1-线性回归"><a href="#1-线性回归" class="headerlink" title="1 线性回归"></a>1 线性回归</h2><ul>
<li>线性回归是对 n 维输入的加权，外加偏差</li>
<li>使用均方损失来衡量预测值和真实值的差异</li>
<li>线性回归有解析（只适合于简单模型，以后学的都没有解析解）</li>
<li>线性回归可以看做是单层神经网络</li>
</ul>
<h3 id="01-基本元素"><a href="#01-基本元素" class="headerlink" title="01 基本元素"></a>01 基本元素</h3><p><strong>回归（regression）是能为一个或多个自变量与因变量之间关系建模的一类方法</strong>。在自然科学和社会科学领域，回归经常用来表示输入和输出之间的关系。</p>
<p>在机器学习领域中的大多数任务通常都与预测（prediction）有关。 <strong>当我们想预测一个数值时，就会涉及到回归问题</strong>。 常见的例子包括：预测价格（房屋、股票等）、预测住院时间（针对住院病人等）、 预测需求（零售销量等）。 但不是所有的预测都是回归问题。</p>
<hr>
<p>为了解释线性回归，我们举一个实际的例子： 我们希望根据房屋的<strong>面积</strong>和<strong>房龄</strong>来估算<strong>价格</strong>。为了开发一个能预测房价的模型，我们需要收集一个真实的数据集。这个数据集包括了房屋的销售价格、面积和房龄。 </p>
<p><strong>对应的机器学习术语：</strong></p>
<ul>
<li>该数据集称为<strong>训练数据集</strong>（training data set） 或训练集（training set）。</li>
<li>每行数据（比如一次房屋交易相对应的数据）称为<strong>样本</strong>（sample），也可以称为数据点（data point）或数据样本（data instance）。</li>
<li>试图预测的目标（比如预测房屋价格）称为<strong>标签</strong>（label）或目标（target）。</li>
<li>预测所依据的自变量（面积和房龄）称为<strong>特征</strong>（feature）或协变量（covariate）。</li>
</ul>
<p><strong>超参数</strong>：可以调整但不在训练过程中更新的参数称为超参数（hyperparameter），如下面提到的学习率、num_epochs。<br><strong>调参</strong>（hyperparameter tuning）是选择超参数的过程。</p>
<h3 id="02-矢量化加速"><a href="#02-矢量化加速" class="headerlink" title="02 矢量化加速"></a>02 矢量化加速</h3><p>在训练我们的模型时，我们经常希望能够同时处理整个小批量的样本。为了实现这一点，需要我们对计算进行矢量化，从而利用线性代数库，而不是在 Python 中编写开销高昂的 for 循环。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">n = <span class="number">10000</span></span><br><span class="line">a = torch.ones([n])</span><br><span class="line">b = torch.ones([n])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用for循环，按元素相加，耗时&#x27;0.16749 sec&#x27;</span></span><br><span class="line">c = torch.zeros(n)</span><br><span class="line">timer = Timer()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    c[i] = a[i] + b[i]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用重载的+运算符，计算按元素的和。耗时&#x27;0.00042 sec&#x27;</span></span><br><span class="line">d = a + b</span><br></pre></td></tr></table></figure>

<p>第二种方法比第一种方法快得多。矢量化代码通常会带来数量级的加速。</p>
<h2 id="2-线性回归的实现"><a href="#2-线性回归的实现" class="headerlink" title="2 线性回归的实现"></a>2 线性回归的实现</h2><ul>
<li>我们可以使用 PyTorch 的高级 API 更简洁地实现模型。</li>
<li>在PyTorch中，<code>data</code>模块提供了数据处理工具，<code>nn</code>模块定义了大量的神经网络层和常见损失函数。</li>
<li>我们可以通过 <code>_</code> 结尾的方法将参数替换，从而初始化参数。</li>
</ul>
<h4 id="（1）生成数据集"><a href="#（1）生成数据集" class="headerlink" title="（1）生成数据集"></a>（1）生成数据集</h4><p>为了简单起见，我们将根据带有噪声的线性模型构造一个人造数据集。我们的任务是使用这个有限样本的数据集来恢复这个模型的参数。我们将使用低维数据，这样可以很容易地将其可视化。<br>在下面的代码中，我们生成一个包含1000个样本的数据集，每个样本包含从标准正态分布中采样的2个特征。我们的合成数据集是一个 1000X2 的矩阵。<br>我们使用线性模型参数：权重 $\mathbf{w}&#x3D;[2,-3.4]^\top$、偏置 $b&#x3D;4.2$ 和噪声项 $\epsilon$ 生成<strong>数据集 X</strong>及其<strong>标签 y:</strong><br>$$<br>\mathbf{y}&#x3D;\mathbf{X}\mathbf{w}+b+\epsilon.<br>$$<br>噪声项 $\epsilon$ 可以视为模型预测和标签时的<strong>潜在观测误差</strong>。在这里我们认为标准假设成立，即 $\epsilon$ 服从均值为0的正态分布。为了简化问题，我们将标准差设为0.01。下面的代码生成合成数据集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成合成数据集和标签</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">synthetic_data</span>(<span class="params">w, b, num_examples</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成y=Xw+b+噪声&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 生成数据集X：服从正态分布的随机特征矩阵X，形状为(num_examples, len(w))</span></span><br><span class="line">    X = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (num_examples, <span class="built_in">len</span>(w)))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 生成标签y</span></span><br><span class="line">    y = torch.matmul(X, w) + b</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 引入噪声，生成服从正态分布的随机噪声矩阵，形状与y相同，并将其加到y上</span></span><br><span class="line">    y += torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, y.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回数据集X和标签y（真实值）</span></span><br><span class="line">    <span class="comment"># -1用于自动计算维度，相当于（n，1），即转换为列向量</span></span><br><span class="line">    <span class="keyword">return</span> X, y.reshape((-<span class="number">1</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">true_w = torch.tensor([<span class="number">2</span>, -<span class="number">3.4</span>]) <span class="comment"># 定义真实的权重</span></span><br><span class="line">true_b = <span class="number">4.2</span> <span class="comment"># 定义真实的偏置</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成合成数据集和标签</span></span><br><span class="line"><span class="comment"># features中的每一行都包含一个二维数据样本</span></span><br><span class="line"><span class="comment"># labels中的每一行都包含一维标签值（一个标量），作为真实值</span></span><br><span class="line">features, labels = synthetic_data(true_w, true_b, <span class="number">1000</span>)</span><br></pre></td></tr></table></figure>
<h4 id="（2）读取数据集-data-iter"><a href="#（2）读取数据集-data-iter" class="headerlink" title="（2）读取数据集 data_iter"></a>（2）读取数据集 <code>data_iter</code></h4><p>我们可以调用框架中现有的 API 来读取数据。我们将 <code>features</code> 和 <code>labels</code> 作为 API 的参数传递，并通过数据迭代器指定 <code>batch_size</code>。此外，布尔值 <code>is_train</code> 表示是否希望数据迭代器对象在每个迭代周期内打乱数据。 </p>
<p>当我们运行迭代时，我们会连续地获得不同的小批量，直至遍历完整个数据集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_array</span>(<span class="params">data_arrays, batch_size, is_train=<span class="literal">True</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;构造一个PyTorch数据迭代器&quot;&quot;&quot;</span></span><br><span class="line">    dataset = data.c(*data_arrays)</span><br><span class="line">    <span class="keyword">return</span> data.DataLoader(dataset, batch_size, shuffle=is_train)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">data_iter = load_array((features, labels), batch_size)</span><br></pre></td></tr></table></figure>

<p>为了验证是否正常工作，让我们读取并打印第一个小批量样本。我们使用 <code>iter</code> 构造 Python 迭代器，并使用 <code>next</code> 从迭代器中获取第一项。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">next</span>(<span class="built_in">iter</span>(data_iter))</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">[tensor([[-<span class="number">1.3116</span>, -<span class="number">0.3062</span>],</span><br><span class="line">         [-<span class="number">1.5653</span>,  <span class="number">0.4830</span>],</span><br><span class="line">         [-<span class="number">0.8893</span>, -<span class="number">0.9466</span>],</span><br><span class="line">         [-<span class="number">1.2417</span>,  <span class="number">1.6891</span>],</span><br><span class="line">         [-<span class="number">0.7148</span>,  <span class="number">0.1376</span>],</span><br><span class="line">         [-<span class="number">0.2162</span>, -<span class="number">0.6122</span>],</span><br><span class="line">         [ <span class="number">2.4048</span>, -<span class="number">0.3211</span>],</span><br><span class="line">         [-<span class="number">0.1516</span>,  <span class="number">0.4997</span>],</span><br><span class="line">         [ <span class="number">1.5298</span>, -<span class="number">0.2291</span>],</span><br><span class="line">         [ <span class="number">1.3895</span>,  <span class="number">1.2602</span>]]),</span><br><span class="line"> tensor([[ <span class="number">2.6073</span>],</span><br><span class="line">         [-<span class="number">0.5787</span>],</span><br><span class="line">         [ <span class="number">5.6339</span>],</span><br><span class="line">         [-<span class="number">4.0211</span>],</span><br><span class="line">         [ <span class="number">2.3117</span>],</span><br><span class="line">         [ <span class="number">5.8492</span>],</span><br><span class="line">         [<span class="number">10.0926</span>],</span><br><span class="line">         [ <span class="number">2.1932</span>],</span><br><span class="line">         [ <span class="number">8.0441</span>],</span><br><span class="line">         [ <span class="number">2.6943</span>]])]</span><br></pre></td></tr></table></figure>
<h4 id="（3）定义模型-net"><a href="#（3）定义模型-net" class="headerlink" title="（3）定义模型 net()"></a>（3）定义模型 <code>net()</code></h4><p>定义模型，将模型的输入和参数同模型的输出关联起来</p>
<p><strong>对于标准深度学习模型，我们可以使用框架的预定义好的层。这使我们只需关注使用哪些层来构造模型，而不必关注层的实现细节。</strong><br>我们首先定义一个模型变量 <code>net</code>，它是一个 <code>Sequential（顺序）</code> 类的实例。 <code>Sequential</code> 类将多个层串联在一起。当给定输入数据时，<code>Sequential</code> 实例将数据传入到第一层，然后将第一层的输出作为第二层的输入，以此类推。 </p>
<blockquote>
<p>在下面的例子中，我们的模型只包含一个层（全连接层），因此实际上不需要 <code>Sequential</code>。但是由于以后几乎所有的模型都是多层的，在这里使用 <code>Sequential</code> 会让你熟悉“标准的流水线”。<br><img src="https://cdn.jsdmirror.com/gh/liuke101/PicGo-For-Hexo/image/202408010046339.svg" alt="singleneuron"></p>
</blockquote>
<p><strong><code>nn.Linear</code> 定义一个全连接层</strong>：在 PyTorch 中，全连接层在 <code>Linear</code> 类中定义。</p>
<ul>
<li>第一个参数：指定输入特征形状，即2，</li>
<li>第二个参数：指定输出特征形状，输出特征形状为单个标量，因此为1。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># nn是神经网络的缩写</span></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">2</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<h4 id="（4）初始化模型参数"><a href="#（4）初始化模型参数" class="headerlink" title="（4）初始化模型参数"></a>（4）初始化模型参数</h4><p>在使用 <code>net</code> 之前，我们需要初始化模型参数。如在线性回归模型中的权重和偏置。 <strong>Pytorch通常有预定义的方法来初始化参数</strong>。</p>
<p>正如我们在构造 <code>nn.Linear</code> 时指定输入和输出尺寸一样，现在我们能直接访问参数以设定它们的初始值。</p>
<ul>
<li>我们通过 <code>net[0]</code> 选择网络中的第一个图层</li>
<li>然后使用 <code>weight.data</code> 和 <code>bias.data</code> 方法访问参数。</li>
<li>我们还可以使用替换方法 <code>normal_</code> 和 <code>fill_</code> 来重写参数值。</li>
</ul>
<p>在这里，我们指定每个权重参数应该从均值为 0、标准差为 0.01 的正态分布中随机采样，偏置参数将初始化为零。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">net[<span class="number">0</span>].weight.data.normal_(<span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">net[<span class="number">0</span>].bias.data.fill_(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor([<span class="number">0.</span>])</span><br></pre></td></tr></table></figure>

<p>在初始化参数之后，我们的任务是更新这些参数，直到这些参数足够拟合我们的数据。每次更新都需要计算损失函数关于模型参数的梯度。有了这个梯度，我们就可以向减小损失的方向更新每个参数。</p>
<blockquote>
<p>因为手动计算梯度很枯燥而且容易出错，所以没有人会手动计算梯度。我们使用自动微分来计算梯度。</p>
</blockquote>
<h4 id="（5）定义损失函数-loss"><a href="#（5）定义损失函数-loss" class="headerlink" title="（5）定义损失函数 loss()"></a>（5）定义损失函数 <code>loss()</code></h4><p>因为需要计算损失函数的梯度，所以我们应该先定义损失函数</p>
<p>这里损失函数采用<strong>均方误差（MSE，mean square error）</strong><br>计算<strong>均方误差</strong>使用的是 <code>MSELoss</code> 类，也称为平方 $L_2$ 范数。默认情况下，它返回所有样本损失的平均值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.MSELoss()</span><br></pre></td></tr></table></figure>

<h4 id="（6）定义优化算法-SGD"><a href="#（6）定义优化算法-SGD" class="headerlink" title="（6）定义优化算法 SGD"></a>（6）定义优化算法 <code>SGD</code></h4><p>小批量：mini-batch<br>随机梯度下降：SGD，Stochastic Gradient Descent</p>
<p>线性回归有解析解。<strong>尽管线性回归有解析解，但本书中的其他模型却没有</strong>。所以我们选择使用更通用的小批量随机梯度下降。</p>
<p>在每一步中，使用从数据集中随机抽取的一个小批量，然后根据参数计算损失的梯度。 接下来，朝着减少损失的方向更新我们的参数。</p>
<p><strong>小批量随机梯度下降算法</strong>是一种优化神经网络的标准工具， PyTorch 在 <code>optim</code> 模块中实现了该算法的许多变种。当我们实例化一个 <code>SGD</code> 实例时，我们要指定优化的参数 （可通过 <code>net.parameters()</code> 从我们的模型中获得）以及优化算法所需的超参数字典。小批量随机梯度下降只需要设置学习速率 <code>lr</code> 值（每一步更新的大小由学习速率 <code>lr</code> 决定），这里设置为0.03。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.03</span>)</span><br></pre></td></tr></table></figure>

<h4 id="（7）训练"><a href="#（7）训练" class="headerlink" title="（7）训练"></a>（7）训练</h4><p>通过 Pytorch 的高级 API 来实现我们的模型只需要相对较少的代码。我们不必单独分配参数、不必定义我们的损失函数，也不必手动实现小批量随机梯度下降。当我们需要更复杂的模型时，高级 API 的优势将大大增加。当我们有了所有的基本组件，训练过程代码与我们从零开始实现时所做的非常相似。 </p>
<ul>
<li>在每个迭代周期里，我们将完整遍历一次数据集（<code>train_data</code>），不停地从中获取一个小批量的输入和相应的标签。</li>
<li><strong>对于每一个小批量，我们会进行以下步骤:</strong><ul>
<li>通过调用 <code>net(X)</code> 生成预测值并计算损失 <code>l</code>（前向传播）。</li>
<li>通过进行反向传播来计算梯度</li>
<li>通过调用优化器来更新模型参数。</li>
</ul>
</li>
</ul>
<p>为了更好的衡量训练效果，我们计算每个迭代周期后的损失，并打印它来监控训练过程。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">3</span> <span class="comment">#迭代次数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 迭代指定次数的epoch，每次都要完整遍历一次数据集</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="comment"># 遍历数据集的每个小批量样本，直到完整遍历一次数据集</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter: <span class="comment">#特征矩阵X和标签y（真实值）</span></span><br><span class="line">        <span class="comment"># 使用网络模型net(X)生成预测值，和真实值y带入损失函数计算损失</span></span><br><span class="line">        l = loss(net(X), y)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 梯度清零，防止梯度累积</span></span><br><span class="line">        trainer.zero_grad()</span><br><span class="line">        <span class="comment"># 反向传播计算梯度</span></span><br><span class="line">        l.backward()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 使用优化器更新模型参数</span></span><br><span class="line">        trainer.step()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算每个迭代周期整个训练集上的损失函数值，用来监控训练过程</span></span><br><span class="line">    l = loss(net(features), labels) </span><br><span class="line">    <span class="comment"># 打印当前epoch的序号和损失函数值</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;l:f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">epoch <span class="number">1</span>, loss <span class="number">0.000248</span></span><br><span class="line">epoch <span class="number">2</span>, loss <span class="number">0.000103</span></span><br><span class="line">epoch <span class="number">3</span>, loss <span class="number">0.000103</span></span><br></pre></td></tr></table></figure>

<p>下面我们比较生成数据集的真实参数和通过有限数据训练获得的模型参数。要访问参数，我们首先从 <code>net</code> 访问所需的层，然后读取该层的权重和偏置。我们估计得到的参数与生成数据的真实参数非常接近。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">w = net[<span class="number">0</span>].weight.data</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;w的估计误差：&#x27;</span>, true_w - w.reshape(true_w.shape))</span><br><span class="line">b = net[<span class="number">0</span>].bias.data</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b的估计误差：&#x27;</span>, true_b - b)</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">w的估计误差： tensor([-<span class="number">0.0010</span>, -<span class="number">0.0003</span>])</span><br><span class="line">b的估计误差： tensor([-<span class="number">0.0003</span>])</span><br></pre></td></tr></table></figure>
<h2 id="3-图像分类数据集-Fashion-MNIST"><a href="#3-图像分类数据集-Fashion-MNIST" class="headerlink" title="3 图像分类数据集 Fashion-MNIST"></a>3 图像分类数据集 Fashion-MNIST</h2><p>MNIST 数据集 (<a target="_blank" rel="noopener" href="http://zh.d2l.ai/chapter_references/zreferences.html#id90" title="LeCun, Y., Bottou, L., Bengio, Y., Haffner, P., &amp; others. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278–2324.">LeCun <em>et al.</em>, 1998</a>) 是图像分类中广泛使用的数据集之一，但作为基准数据集过于简单。我们将使用类似但更复杂的 Fashion-MNIST 数据集 (<a target="_blank" rel="noopener" href="http://zh.d2l.ai/chapter_references/zreferences.html#id189" title="Xiao, H., Rasul, K., &amp; Vollgraf, R. (2017). Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv: 1708.07747.">Xiao <em>et al.</em>, 2017</a>)。<br>Fashion-MNIST 是一个服装分类数据集，由10个类别的图像组成。我们将在后续章节中使用此数据集来评估各种分类算法。</p>
<h4 id="读取数据集"><a href="#读取数据集" class="headerlink" title="读取数据集"></a>读取数据集</h4><p>我们可以通过框架中的内置函数将 Fashion-MNIST 数据集下载并读取到内存中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">d2l.use_svg_display()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过ToTensor实例将图像数据从PIL类型变换成32位浮点数格式，</span></span><br><span class="line"><span class="comment"># 并除以255使得所有像素的数值均在0～1之间</span></span><br><span class="line">trans = transforms.ToTensor()</span><br><span class="line">mnist_train = torchvision.datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&quot;../data&quot;</span>, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">mnist_test = torchvision.datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&quot;../data&quot;</span>, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#train=True：训练数据集</span></span><br><span class="line"><span class="comment">#transform=trans：转换成张量</span></span><br><span class="line"><span class="comment">#download=True 下载</span></span><br></pre></td></tr></table></figure>

<p>Fashion-MNIST 由10个类别的图像组成，每个类别由训练数据集（train dataset）中的6000张图像和测试数据集（test dataset）中的1000张图像组成。<br>因此，训练集和测试集分别包含60000和10000张图像。测试数据集不会用于训练，只用于评估模型性能。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">len</span>(mnist_train), <span class="built_in">len</span>(mnist_test)</span><br><span class="line"><span class="comment">###</span></span><br><span class="line">(<span class="number">60000</span>, <span class="number">10000</span>)</span><br></pre></td></tr></table></figure>

<p>每个输入图像的高度和宽度均为28像素。数据集由灰度图像组成，其通道数为1。**为了简洁起见，本书将高度 $h$ 像素、宽度 $w$ 像素图像的形状记为 $h×w$ 或 $(h,w)$ **</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mnist_train[<span class="number">0</span>][<span class="number">0</span>].shape</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br></pre></td></tr></table></figure>

<p>Fashion-MNIST 中包含的10个类别，分别为 t-shirt（T 恤）、trouser（裤子）、pullover（套衫）、dress（连衣裙）、coat（外套）、sandal（凉鞋）、shirt（衬衫）、sneaker（运动鞋）、bag（包）和 ankle boot（短靴）。<strong>以下函数用于在数字标签索引及其文本名称之间进行转换</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_fashion_mnist_labels</span>(<span class="params">labels</span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回Fashion-MNIST数据集的文本标签&quot;&quot;&quot;</span></span><br><span class="line">    text_labels = [<span class="string">&#x27;t-shirt&#x27;</span>, <span class="string">&#x27;trouser&#x27;</span>, <span class="string">&#x27;pullover&#x27;</span>, <span class="string">&#x27;dress&#x27;</span>, <span class="string">&#x27;coat&#x27;</span>,</span><br><span class="line">                   <span class="string">&#x27;sandal&#x27;</span>, <span class="string">&#x27;shirt&#x27;</span>, <span class="string">&#x27;sneaker&#x27;</span>, <span class="string">&#x27;bag&#x27;</span>, <span class="string">&#x27;ankle boot&#x27;</span>]</span><br><span class="line">    <span class="keyword">return</span> [text_labels[<span class="built_in">int</span>(i)] <span class="keyword">for</span> i <span class="keyword">in</span> labels]</span><br></pre></td></tr></table></figure>

<p>我们现在可以创建一个函数来可视化这些样本。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">show_images</span>(<span class="params">imgs, num_rows, num_cols, titles=<span class="literal">None</span>, scale=<span class="number">1.5</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;绘制图像列表&quot;&quot;&quot;</span></span><br><span class="line">    figsize = (num_cols * scale, num_rows * scale)</span><br><span class="line">    _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)</span><br><span class="line">    axes = axes.flatten()</span><br><span class="line">    <span class="keyword">for</span> i, (ax, img) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(axes, imgs)):</span><br><span class="line">        <span class="keyword">if</span> torch.is_tensor(img):</span><br><span class="line">            <span class="comment"># 图片张量</span></span><br><span class="line">            ax.imshow(img.numpy())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># PIL图片</span></span><br><span class="line">            ax.imshow(img)</span><br><span class="line">        ax.axes.get_xaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">        ax.axes.get_yaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">if</span> titles:</span><br><span class="line">            ax.set_title(titles[i])</span><br><span class="line">    <span class="keyword">return</span> axes</span><br></pre></td></tr></table></figure>

<p>以下是训练数据集中前几个样本的图像及其相应的标签。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X, y = <span class="built_in">next</span>(<span class="built_in">iter</span>(data.DataLoader(mnist_train, batch_size=<span class="number">18</span>)))</span><br><span class="line">show_images(X.reshape(<span class="number">18</span>, <span class="number">28</span>, <span class="number">28</span>), <span class="number">2</span>, <span class="number">9</span>, titles=get_fashion_mnist_labels(y));</span><br></pre></td></tr></table></figure>
<p><img src="https://cdn.jsdmirror.com/gh/liuke101/PicGo-For-Hexo/image/202408010046340.svg" alt="output_image-classification-dataset_e45669_83_0"></p>
<h4 id="读取小批量"><a href="#读取小批量" class="headerlink" title="读取小批量"></a>读取小批量</h4><p>为了使我们在读取训练集和测试集时更容易，我们使用内置的数据迭代器，数据迭代器是获得更高性能的关键组件。依靠实现良好的数据迭代器，利用高性能计算来避免减慢训练过程。</p>
<p><code>DataLoader</code> 每次都会读取一小批量数据，大小为 <code>batch_size</code>。通过内置数据迭代器，我们可以随机打乱了所有样本，从而无偏见地读取小批量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_dataloader_workers</span>():  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用4个进程来读取数据&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">4</span></span><br><span class="line"></span><br><span class="line">train_iter = data.DataLoader(mnist_train, batch_size, shuffle=<span class="literal">True</span>,num_workers=get_dataloader_workers())</span><br></pre></td></tr></table></figure>

<p>我们看一下读取训练数据所需的时间。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">timer = d2l.Timer()</span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="keyword">continue</span></span><br><span class="line"><span class="string">f&#x27;<span class="subst">&#123;timer.stop():<span class="number">.2</span>f&#125;</span> sec&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line"><span class="string">&#x27;3.37 sec&#x27;</span></span><br></pre></td></tr></table></figure>

<h4 id="整合所有组件"><a href="#整合所有组件" class="headerlink" title="整合所有组件"></a>整合所有组件</h4><p>现在我们定义 <code>load_data_fashion_mnist</code> 函数，<strong>用于获取和读取 Fashion-MNIST 数据集。这个函数返回训练集和验证集的数据迭代器（迭代器用于按小批量读取一遍完整数据 <code>for X, y in train_iter</code>）</strong>。此外，这个函数还接受一个可选参数 <code>resize</code>，用来将图像大小调整为另一种形状。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_fashion_mnist</span>(<span class="params">batch_size, resize=<span class="literal">None</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载Fashion-MNIST数据集，然后将其加载到内存中&quot;&quot;&quot;</span></span><br><span class="line">    trans = [transforms.ToTensor()]</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.insert(<span class="number">0</span>, transforms.Resize(resize))</span><br><span class="line">    trans = transforms.Compose(trans)</span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&quot;../data&quot;</span>, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&quot;../data&quot;</span>, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> (data.DataLoader(mnist_train, batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()),</span><br><span class="line">            data.DataLoader(mnist_test, batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()))</span><br></pre></td></tr></table></figure>

<p>下面，我们通过指定 <code>resize</code> 参数来测试 <code>load_data_fashion_mnist</code> 函数的图像大小调整功能。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">train_iter, test_iter = load_data_fashion_mnist(<span class="number">32</span>, resize=<span class="number">64</span>)</span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="built_in">print</span>(X.shape, X.dtype, y.shape, y.dtype)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">###</span></span><br><span class="line">torch.Size([<span class="number">32</span>, <span class="number">1</span>, <span class="number">64</span>, <span class="number">64</span>]) torch.float32 torch.Size([<span class="number">32</span>]) torch.int64</span><br></pre></td></tr></table></figure>

<p>我们现在已经准备好使用Fashion-MNIST数据集，便于下面的章节调用来评估各种分类算法。</p>
<h2 id="4-Softmax-回归的实现"><a href="#4-Softmax-回归的实现" class="headerlink" title="4 Softmax 回归的实现"></a>4 Softmax 回归的实现</h2><h4 id="从回归到分类"><a href="#从回归到分类" class="headerlink" title="从回归到分类"></a>从回归到分类</h4><p><img src="https://cdn.jsdmirror.com/gh/liuke101/PicGo-For-Hexo/image/202408010046341.png" alt="Pasted image 20231112205941"></p>
<ul>
<li>回归估计的是一个连续值</li>
<li>分类预测一个离散类别</li>
</ul>
<p>通常，机器学习实践者用分类这个词来描述两个有微妙差别的问题： </p>
<ol>
<li>我们只对样本的“<strong>硬性”类别</strong>感兴趣，即<strong>属于哪个类别</strong>；</li>
<li>我们希望得到“<strong>软性”类别</strong>，即得到<strong>属于每个类别的概率</strong>。<br>这两者的界限往往很模糊。其中的一个原因是：<strong>即使我们只关心硬类别，我们仍然使用软类别的模型。</strong></li>
</ol>
<p><strong>Softmax 函数具有以下特征：</strong></p>
<ul>
<li>softmax 函数的输出是 0.0 到 1.0 之间的实数。  </li>
<li><strong>softmax 函数的输出总和为 1 是 softmax 函数的一个重要性质，正因为有了这个性质，可以把 softmax 函数的输出解释为 “概率”。</strong>  </li>
<li>数组 a 中元素的大小关系和输出 y 中元素的大小关系一致，<strong>即使用了 softmax 函数，各个元素之间的大小关系也不会改变。</strong></li>
<li>尽管 softmax 是一个非线性函数，但 softmax 回归的输出仍然由输入特征的仿射变换决定。因此，softmax 回归是一个线性模型（linear model）。</li>
</ul>
<h4 id="（1）数据集"><a href="#（1）数据集" class="headerlink" title="（1）数据集"></a>（1）数据集</h4><p>使用 Fashion-MNIST 数据集，并保持批量大小为256。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure>

<h4 id="（2）定义模型-net-，初始化模型参数"><a href="#（2）定义模型-net-，初始化模型参数" class="headerlink" title="（2）定义模型 net()，初始化模型参数"></a>（2）定义模型 <code>net()</code>，初始化模型参数</h4><p>和之前线性回归的例子一样，这里的每个样本都将用固定长度的向量表示。原始数据集中的每个样本都是28×28的图像。<strong>本节将展平每个图像，把它们看作长度为784的向量</strong>。在后面的章节中，我们将讨论能够利用图像空间结构的特征，但现在我们<strong>暂时只把每个像素位置看作一个特征。</strong><br><strong>在 softmax 回归中，我们的输出与类别一样多</strong>。因为我们的数据集有<strong>10个类别，所以网络输出维度为10</strong>。因此，权重将构成一个 $784×10$ 的矩阵，偏置将构成一个 $1×10$ 的行向量。与线性回归一样，我们将使用正态分布初始化我们的权重 <code>W</code>，偏置初始化为0。</p>
<p><strong>Softmax 回归的输出层是一个全连接层</strong>。因此，为了实现我们的模型，我们<strong>只需在 <code>Sequential</code> 中添加一个带有10个输出的全连接层</strong>。同样，在这里 <code>Sequential</code> 并不是必要的，但它是实现深度模型的基础。我们仍然以均值0和标准差0.01随机初始化权重。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># PyTorch不会隐式地调整输入的形状。因此，我们在线性层前定义了展平层（flatten），来调整网络输入的形状</span></span><br><span class="line">net = nn.Sequential(nn.Flatten(), nn.Linear(<span class="number">784</span>, <span class="number">10</span>))</span><br><span class="line"><span class="comment">#nn.Flatten()用于将输入的多维张量展平为一维张量，这里用于将输入的图像展平为一维向量。</span></span><br><span class="line"><span class="comment">#nn.Linear(784, 10)定义了一个线性层，输入维度为784，输出维度为10，这里用于将展平后的图像向量映射为10个类别的得分。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#如果是线性层，则对其权重进行正态分布初始化。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#将init_weights函数应用到net的每一层，从而实现对权重的初始化。</span></span><br><span class="line">net.apply(init_weights); </span><br></pre></td></tr></table></figure>

<h4 id="（3）-定义损失函数-loss"><a href="#（3）-定义损失函数-loss" class="headerlink" title="（3） 定义损失函数 loss()"></a>（3） 定义损失函数 <code>loss()</code></h4><p>Softmax 函数：<br>$$<br>\hat{y}_j&#x3D;\frac{\exp(o_j)}{\sum_k\exp(o_k)}<br>$$<br>$exp (x)$ 是表示 $e^x$ 的指数函数。上表示假设输出层共有 $k$ 个神经元，计算第 $j$ 个神经元的输出 $y_k$。 $softmax$ 函数的分子是输入信号 $o_j$ 的指数函数，分母是所有输入信号的指数函数的和。  </p>
<p>原函数分子分母会发生上溢，这种情况下无法得到明确定义的交叉熵值。解决方法：改成下面的函数<br>$$<br>\hat{y}_j&#x3D;\frac{\exp(o_j-\max(o_k))}{\sum_k\exp(o_k-\max(o_k))}<br>$$</p>
<p>改成这样后在反向传播时会返回 NAN, 因此尽管我们要计算指数函数，但我们最终在计算交叉熵损失时会取它们的对数。通过将 softmax 和交叉熵结合在一起，可以避免反向传播过程中可能会困扰我们的数值稳定性问题。</p>
<p>$$<br>\begin{aligned}<br>\log\left(\hat{y}_j\right)&amp; &#x3D;\log\left(\frac{\exp(o_j-\max(o_k))}{\sum_k\exp(o_k-\max(o_k))}\right)  \<br>&amp;&#x3D;\log\left(\exp(o_j-\max(o_k))\right)-\log\left(\sum_k\exp(o_k-\max(o_k))\right) \<br>&amp;&#x3D;o_j-\max(o_k)-\log\left(\sum_k\exp(o_k-\max(o_k))\right).<br>\end{aligned}<br>$$<br>我们也希望保留传统的 softmax 函数，以备我们需要评估通过模型输出的概率。<strong>但是，我们没有将 softmax 概率传递到损失函数中，而是在交叉熵损失函数中传递未规范化的预测，并同时计算 softmax 及其对数，这是一种类似 <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/LogSumExp">“LogSumExp技巧”</a>的聪明方式。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>![[《鱼书》#交叉熵]]</p>
<h4 id="（4）优化算法-SGD"><a href="#（4）优化算法-SGD" class="headerlink" title="（4）优化算法 SGD"></a>（4）优化算法 SGD</h4><p>在这里，我们<strong>使用学习率为0.1的小批量随机梯度下降作为优化算法</strong>。这与我们在线性回归例子中的相同，这说明了优化器的普适性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<h4 id="（5）-分类精度"><a href="#（5）-分类精度" class="headerlink" title="（5） 分类精度"></a>（5） 分类精度</h4><p>给定预测概率分布 <code>y_hat</code>，当我们必须输出硬性预测（即属于哪个类别）时，我们<strong>通常选择预测概率最高的类。</strong></p>
<p><strong>当预测与标签分类<code>y</code>一致时，即是正确的。 分类精度即正确预测数量与总预测数量之比。</strong> 虽然直接优化精度可能很困难（因为精度的计算不可导）， 但<strong>精度通常是我们最关心的性能衡量标准，我们在训练分类器时几乎总会关注它。</strong></p>
<p>为了计算精度，我们执行以下操作。首先，如果 <code>y_hat</code>（预测值）是矩阵，那么<strong>假定第二个维度存储每个类的预测分数</strong>。我们使用 <code>argmax</code> 获得每行中最大元素的索引来获得预测类别。然后我们将预测类别与真实 <code>y</code> 元素进行比较。由于等式运算符“<code>==</code>”对数据类型很敏感，因此我们将 <code>y_hat</code> 的数据类型转换为与 <code>y</code> 的数据类型一致。结果是一个包含0（错）和1（对）的张量。最后，我们求和会<strong>得到正确预测的数量。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">y_hat, y</span>): </span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算预测正确的数量&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(y_hat.shape) &gt; <span class="number">1</span> <span class="keyword">and</span> y_hat.shape[<span class="number">1</span>] &gt; <span class="number">1</span>:</span><br><span class="line">        y_hat = y_hat.argmax(axis=<span class="number">1</span>)</span><br><span class="line">    cmp = y_hat.<span class="built_in">type</span>(y.dtype) == y</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(cmp.<span class="built_in">type</span>(y.dtype).<span class="built_in">sum</span>())</span><br></pre></td></tr></table></figure>

<p><strong>同样，对于任意数据迭代器 <code>data_iter</code> 可访问的数据集，我们可以评估在任意模型 <code>net</code> 的精度。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy</span>(<span class="params">net, data_iter</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算在指定数据集上模型的精度&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">        net.<span class="built_in">eval</span>()  <span class="comment"># 将模型设置为评估模式</span></span><br><span class="line">    metric = Accumulator(<span class="number">2</span>)  <span class="comment"># 正确预测数、预测总数</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            metric.add(accuracy(net(X), y), y.numel()) <span class="comment"># 正确预测数、预测总数添加到metric</span></span><br><span class="line">            <span class="comment">#numel()方法返回张量中元素的总数</span></span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>] <span class="comment">## 正确预测数/预测总数 = 精度</span></span><br></pre></td></tr></table></figure>

<p><strong>这里定义一个实用程序类 <code>Accumulator</code>，用于对多个变量进行累加。</strong> 在上面的 <code>evaluate_accuracy</code> 函数中，我们在 <code>Accumulator</code> 实例中创建了2个变量，分别用于存储正确预测的数量和预测的总数量。当我们遍历数据集时，两者都将随着时间的推移而累加。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Accumulator</span>:  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;在n个变量上累加&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n</span>):</span><br><span class="line">        self.data = [<span class="number">0.0</span>] * n</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, *args</span>):</span><br><span class="line">        self.data = [a + <span class="built_in">float</span>(b) <span class="keyword">for</span> a, b <span class="keyword">in</span> <span class="built_in">zip</span>(self.data, args)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset</span>(<span class="params">self</span>):</span><br><span class="line">        self.data = [<span class="number">0.0</span>] * <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data[idx]</span><br></pre></td></tr></table></figure>
<h4 id="（6）-训练"><a href="#（6）-训练" class="headerlink" title="（6） 训练"></a>（6） 训练</h4><p>在我们看过线性回归实现， softmax 回归的训练过程代码应该看起来非常眼熟。<br>在这里，我们重构训练过程的实现以使其可重复使用。首先，我们定义一个函数来训练一个迭代周期。<strong>请注意，<code>updater</code> 是更新模型参数的常用函数，它接受批量大小作为参数。它可以是 <code>d2l.sgd</code> 函数，也可以是框架的内置优化函数。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch_ch3</span>(<span class="params">net, train_iter, loss, updater</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型一个迭代周期（定义见第3章）&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 将模型设置为训练模式</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">        net.train()</span><br><span class="line">    <span class="comment"># 训练损失总和、训练准确度总和、样本数</span></span><br><span class="line">    metric = Accumulator(<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">        <span class="comment"># 计算梯度并更新参数</span></span><br><span class="line">        y_hat = net(X)</span><br><span class="line">        l = loss(y_hat, y)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(updater, torch.optim.Optimizer):</span><br><span class="line">            <span class="comment"># 使用PyTorch内置的优化器和损失函数</span></span><br><span class="line">            updater.zero_grad()</span><br><span class="line">            l.mean().backward()</span><br><span class="line">            updater.step()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 使用定制的优化器和损失函数</span></span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            updater(X.shape[<span class="number">0</span>])</span><br><span class="line">        metric.add(<span class="built_in">float</span>(l.<span class="built_in">sum</span>()), accuracy(y_hat, y), y.numel())</span><br><span class="line">    <span class="comment"># 返回训练损失和训练精度</span></span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">2</span>], metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br></pre></td></tr></table></figure>

<p>在展示训练函数的实现之前，我们定义一个在动画中绘制数据的实用程序类<code>Animator</code>， 它能够简化本书其余部分的代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Animator</span>:  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;在动画中绘制数据&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, xlabel=<span class="literal">None</span>, ylabel=<span class="literal">None</span>, legend=<span class="literal">None</span>, xlim=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 ylim=<span class="literal">None</span>, xscale=<span class="string">&#x27;linear&#x27;</span>, yscale=<span class="string">&#x27;linear&#x27;</span>,</span></span><br><span class="line"><span class="params">                 fmts=(<span class="params"><span class="string">&#x27;-&#x27;</span>, <span class="string">&#x27;m--&#x27;</span>, <span class="string">&#x27;g-.&#x27;</span>, <span class="string">&#x27;r:&#x27;</span></span>), nrows=<span class="number">1</span>, ncols=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 figsize=(<span class="params"><span class="number">3.5</span>, <span class="number">2.5</span></span>)</span>):</span><br><span class="line">        <span class="comment"># 增量地绘制多条线</span></span><br><span class="line">        <span class="keyword">if</span> legend <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            legend = []</span><br><span class="line">        d2l.use_svg_display()</span><br><span class="line">        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)</span><br><span class="line">        <span class="keyword">if</span> nrows * ncols == <span class="number">1</span>:</span><br><span class="line">            self.axes = [self.axes, ]</span><br><span class="line">        <span class="comment"># 使用lambda函数捕获参数</span></span><br><span class="line">        self.config_axes = <span class="keyword">lambda</span>: d2l.set_axes(</span><br><span class="line">            self.axes[<span class="number">0</span>], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)</span><br><span class="line">        self.X, self.Y, self.fmts = <span class="literal">None</span>, <span class="literal">None</span>, fmts</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        <span class="comment"># 向图表中添加多个数据点</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(y, <span class="string">&quot;__len__&quot;</span>):</span><br><span class="line">            y = [y]</span><br><span class="line">        n = <span class="built_in">len</span>(y)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(x, <span class="string">&quot;__len__&quot;</span>):</span><br><span class="line">            x = [x] * n</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.X:</span><br><span class="line">            self.X = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.Y:</span><br><span class="line">            self.Y = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">        <span class="keyword">for</span> i, (a, b) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(x, y)):</span><br><span class="line">            <span class="keyword">if</span> a <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> b <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                self.X[i].append(a)</span><br><span class="line">                self.Y[i].append(b)</span><br><span class="line">        self.axes[<span class="number">0</span>].cla()</span><br><span class="line">        <span class="keyword">for</span> x, y, fmt <span class="keyword">in</span> <span class="built_in">zip</span>(self.X, self.Y, self.fmts):</span><br><span class="line">            self.axes[<span class="number">0</span>].plot(x, y, fmt)</span><br><span class="line">        self.config_axes()</span><br><span class="line">        display.display(self.fig)</span><br><span class="line">        display.clear_output(wait=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>接下来我们实现一个<strong>训练函数</strong>，它会在 <code>train_iter</code> 访问到的训练数据集上训练一个模型 <code>net</code>。该训练函数将会运行多个迭代周期（由 <code>num_epochs</code> 指定）。在每个迭代周期结束时，利用 <code>test_iter</code> 访问到的测试数据集对模型进行评估。我们将利用 <code>Animator</code> 类来可视化训练进度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch3</span>(<span class="params">net, train_iter, test_iter, loss, num_epochs, updater</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型（定义见第3章）&quot;&quot;&quot;</span></span><br><span class="line">    animator = Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs], ylim=[<span class="number">0.3</span>, <span class="number">0.9</span>], legend=[<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        train_metrics = train_epoch_ch3(net, train_iter, loss, updater) <span class="comment">#训练损失和训练精度</span></span><br><span class="line">        test_acc = evaluate_accuracy(net, test_iter)  <span class="comment">#计算在指定数据集上模型的精度</span></span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, train_metrics + (test_acc,))</span><br><span class="line">    train_loss, train_acc = train_metrics</span><br><span class="line">    <span class="keyword">assert</span> train_loss &lt; <span class="number">0.5</span>, train_loss</span><br><span class="line">    <span class="keyword">assert</span> train_acc &lt;= <span class="number">1</span> <span class="keyword">and</span> train_acc &gt; <span class="number">0.7</span>, train_acc</span><br><span class="line">    <span class="keyword">assert</span> test_acc &lt;= <span class="number">1</span> <span class="keyword">and</span> test_acc &gt; <span class="number">0.7</span>, test_acc</span><br></pre></td></tr></table></figure>

<p>接下来我们调用训练函数来训练模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure>

<p><img src="https://cdn.jsdmirror.com/gh/liuke101/PicGo-For-Hexo/image/202408010046342.svg" alt="output_softmax-regression-concise_75d138_66_0"></p>
<h1 id="三、多层感知机"><a href="#三、多层感知机" class="headerlink" title="三、多层感知机"></a>三、多层感知机</h1><h2 id="1-激活函数"><a href="#1-激活函数" class="headerlink" title="1 激活函数"></a>1 激活函数</h2><ul>
<li>多层感知机在输出层和输入层之间增加一个或多个全连接隐藏层，并通过激活函数转换隐藏层的输出。</li>
<li>常用的激活函数包括ReLU函数、sigmoid函数和tanh函数。</li>
</ul>
<p><strong>激活函数</strong>（activation function）通过计算加权和并加上偏置来确定神经元是否应该被激活，它们将输入信号转换为输出的可微运算。大多数激活函数都是非线性的。<strong>激活函数的输出称作活性值</strong>：<br>由于激活函数是深度学习的基础，下面简要介绍一些常见的激活函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>
<h3 id="RELU-函数"><a href="#RELU-函数" class="headerlink" title="RELU 函数"></a>RELU 函数</h3><p>最受欢迎的激活函数是<strong>修正线性单元（Rectified linear unit，ReLU）</strong>，因为它实现简单，同时在各种预测任务中表现良好。<br>ReLU 提供了一种非常简单的非线性变换。给定元素 $x$，ReLU 函数被定义为该元素与 0 的最大值：<br>$$<br>ReLU(x)&#x3D;\max(x,0)<br>$$<br>通俗地说，ReLU 函数通过将相应的活性值设为0，仅保留正元素并丢弃所有负元素。如图所示，激活函数是分段线性的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(-<span class="number">8.0</span>, <span class="number">8.0</span>, <span class="number">0.1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = torch.relu(x) <span class="comment">#ReLU函数</span></span><br><span class="line">y.backward(torch.ones_like(x), retain_graph=<span class="literal">True</span>) <span class="comment">#对ReLU求导</span></span><br></pre></td></tr></table></figure>

<p><img src="https://cdn.jsdmirror.com/gh/liuke101/PicGo-For-Hexo/image/202408010046343.svg" alt="output_mlp_76f463_21_0|327"><img src="https://cdn.jsdmirror.com/gh/liuke101/PicGo-For-Hexo/image/202408010046344.svg" alt="output_mlp_76f463_36_0|340"><br>当输入为负时，ReLU 函数的导数为0，而当输入为正时，ReLU 函数的导数为1。注意，当输入值精确等于0时，ReLU 函数不可导。在此时，我们默认使用左侧的导数，即当输入为0时导数为0。<strong>我们可以忽略这种情况，因为输入可能永远都不会是0。</strong></p>
<p><strong>使用 ReLU 的原因是，它求导表现得特别好</strong>：要么让参数消失，要么让参数通过。这使得优化表现得更好，并且 ReLU 减轻了困扰以往神经网络的<strong>梯度消失</strong>问题。</p>
<p><strong>注意，ReLU 函数有许多变体</strong>，包括参数化 ReLU（Parameterized ReLU，pReLU） 函数 (<a target="_blank" rel="noopener" href="http://zh.d2l.ai/chapter_references/zreferences.html#id59" title="He, K., Zhang, X., Ren, S., &amp; Sun, J. (2015). Delving deep into rectifiers: surpassing human-level performance on imagenet classification. Proceedings of the IEEE international conference on computer vision (pp. 1026–1034).">He <em>et al.</em>, 2015</a>)。该变体为 ReLU 添加了一个线性项，因此即使参数是负的，某些信息仍然可以通过：<br>$$pReLU (x)&#x3D;\max (x, 0)+\alpha \min(0,x)$$</p>
<h3 id="Sigmoid-函数"><a href="#Sigmoid-函数" class="headerlink" title="Sigmoid 函数"></a>Sigmoid 函数</h3><p>对于一个定义域在 $\mathbb{R}$ 中的输入， sigmoid 函数将输入变换为区间 $(0, 1)$ 上的输出。因此，sigmoid 通常称为<strong>挤压函数</strong>（squashing function）： 它将范围 $(-inf, inf)$ 中的任意输入压缩到区间 $(0, 1)$ 中的某个值：<br>$$<br>\operatorname{sigmoid}(x) &#x3D; \frac{1}{1 + \exp(-x)}<br>$$<br>当人们逐渐关注到到基于梯度的学习时， sigmoid 函数是一个自然的选择，因为它是一个平滑的、可微的阈值单元近似。 </p>
<ul>
<li>当我们想要将输出视作二元分类问题的概率时， sigmoid 仍然被广泛用作输出单元上的激活函数 （sigmoid 可以视为 softmax 的特例）。</li>
<li>sigmoid 在隐藏层中已经较少使用，它在大部分时候被更简单、更容易训练的 ReLU 所取代。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">y = torch.sigmoid(x) <span class="comment">#sigmoid函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 清除以前的梯度</span></span><br><span class="line">x.grad.data.zero_()</span><br><span class="line">y.backward(torch.ones_like(x),retain_graph=<span class="literal">True</span>) <span class="comment">#导数</span></span><br></pre></td></tr></table></figure>
<p><img src="https://cdn.jsdmirror.com/gh/liuke101/PicGo-For-Hexo/image/202408010046345.svg" alt="output_mlp_76f463_51_0"> </p>
<blockquote>
<p>注意，当输入接近 0 时，sigmoid 函数接近线性变换。</p>
</blockquote>
<p><strong>sigmoid 函数的导数</strong>为下面的公式：<br>$$<br>\frac{d}{dx} \operatorname{sigmoid}(x) &#x3D; \frac{\exp(-x)}{(1 + \exp(-x))^2} &#x3D; \operatorname{sigmoid}(x)\left(1-\operatorname{sigmoid}(x)\right).<br>$$<br>sigmoid 函数的导数图像如下所示。注意，当输入为0时，sigmoid 函数的导数达到最大值0.25；而输入在任一方向上越远离0点时，导数越接近0。</p>
<p><img src="https://cdn.jsdmirror.com/gh/liuke101/PicGo-For-Hexo/image/202408010046346.svg" alt="output_mlp_76f463_66_0">当sigmoid函数的输入很大或是很小时，它的梯度都会消失**。 此外，当反向传播通过许多层时，除非我们在刚刚好的地方， 这些地方sigmoid函数的输入接近于零，否则整个乘积的梯度可能会消失。 当我们的网络有很多层时，除非我们很小心，否则在某一层可能会切断梯度。 事实上，这个问题曾经困扰着深度网络的训练。 <strong>因此，更稳定的ReLU系列函数已经成为从业者的默认选择</strong>（虽然在神经科学的角度看起来不太合理）。 ^rb6csu</p>
<h3 id="tanh-函数"><a href="#tanh-函数" class="headerlink" title="tanh 函数"></a>tanh 函数</h3><p>与 sigmoid 函数类似， <strong>tanh(双曲正切)函数</strong>也能将其输入压缩转换到区间(-1, 1)上。 tanh 函数的公式如下：<br>$$<br>\operatorname{tanh}(x) &#x3D; \frac{1 - \exp(-2x)}{1 + \exp(-2x)}<br>$$</p>
<p><img src="https://cdn.jsdmirror.com/gh/liuke101/PicGo-For-Hexo/image/202408010046347.svg" alt="output_mlp_76f463_81_0"><br>注意，当输入在0附近时，tanh 函数接近线性变换。函数的形状类似于 sigmoid 函数，不同的是 tanh 函数关于坐标系原点中心对称。</p>
<p>tanh 函数的导数是：<br>$$<br>\frac{d}{dx} \operatorname{tanh}(x) &#x3D; 1 - \operatorname{tanh}^2(x)<br>$$<br>tanh函数的导数图像如下所示。 当输入接近0时，tanh函数的导数接近最大值1。 与我们在sigmoid函数图像中看到的类似， 输入在任一方向上越远离0点，导数越接近0。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">y = torch.tanh(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 清除以前的梯度</span></span><br><span class="line">x.grad.data.zero_()</span><br><span class="line">y.backward(torch.ones_like(x),retain_graph=<span class="literal">True</span>) <span class="comment">#倒数</span></span><br></pre></td></tr></table></figure>

<p><img src="https://cdn.jsdmirror.com/gh/liuke101/PicGo-For-Hexo/image/202408010046348.svg" alt="output_mlp_76f463_96_0"></p>
<h2 id="2-多层感知机的实现"><a href="#2-多层感知机的实现" class="headerlink" title="2 多层感知机的实现"></a>2 多层感知机的实现</h2><ul>
<li>对于相同的分类问题，多层感知机的实现与 softmax 回归的实现相同，只是多层感知机的实现里增加了带有激活函数的隐藏层。</li>
</ul>
<p>为了与之前softmax回归（ <a target="_blank" rel="noopener" href="http://zh.d2l.ai/chapter_linear-networks/softmax-regression-scratch.html#sec-softmax-scratch">3.6节</a> ） 获得的结果进行比较， 我们将继续使用Fashion-MNIST图像分类数据集 （ <a target="_blank" rel="noopener" href="http://zh.d2l.ai/chapter_linear-networks/image-classification-dataset.html#sec-fashion-mnist">3.5节</a>）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure>

<h3 id="定义模型-net"><a href="#定义模型-net" class="headerlink" title="定义模型 net()"></a>定义模型 <code>net()</code></h3><p>回想一下，Fashion-MNIST 中的每个图像由 $28×28&#x3D;784$ 个灰度像素值组成。所有图像共分为 $10$ 个类别。忽略像素之间的空间结构，我们可以将每个图像视为具有 $784$ 个输入特征和 $10$ 个类的简单分类数据集。<br><strong>首先，我们将实现一个具有单隐藏层的多层感知机，它包含 $256$ 个隐藏单元</strong>。注意，我们可以将这两个变量都视为<strong>超参数</strong>。<strong>通常，我们选择 $2$ 的若干次幂作为层的宽度。因为内存在硬件中的分配和寻址方式，这么做往往可以在计算上更高效。</strong> </p>
<p>与 softmax 回归的实现相比，唯一的区别是我们添加了2个全连接层（之前我们只添加了1个全连接层）。</p>
<ul>
<li>第一层是隐藏层，它包含256个隐藏单元，并使用了 ReLU 激活函数。</li>
<li>第二层是输出层。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Flatten(),</span><br><span class="line">                    nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">                    nn.ReLU(), <span class="comment">#激活函数</span></span><br><span class="line">                    nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">net.apply(init_weights);</span><br></pre></td></tr></table></figure>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>训练过程的实现与我们实现 softmax 回归时完全相同，可以直接调用 <code>d2l</code> 包的 <code>train_ch3</code> 函数（参见 <a target="_blank" rel="noopener" href="http://zh.d2l.ai/chapter_linear-networks/softmax-regression-scratch.html#sec-softmax-scratch">3.6节</a> ），将迭代周期数设置为10，并将学习率设置为0.1。<br>这种模块化设计使我们能够将与模型架构有关的内容独立出来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">batch_size, lr, num_epochs = <span class="number">256</span>, <span class="number">0.1</span>, <span class="number">10</span></span><br><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>) <span class="comment">#损失函数：交叉熵损失</span></span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=lr) <span class="comment">#优化算法</span></span><br><span class="line"></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure>

<h2 id="3-模型选择、欠拟合和过拟合"><a href="#3-模型选择、欠拟合和过拟合" class="headerlink" title="3 模型选择、欠拟合和过拟合"></a>3 模型选择、欠拟合和过拟合</h2><ul>
<li><p><strong>过拟合（overfitting）</strong>：将模型在训练数据上拟合的比在潜在分布中更接近的现象称为，用于对抗过拟合的技术称为<strong>正则化（regularization）</strong>。 </p>
<ul>
<li>在实验中调整模型架构或超参数时会发现： 如果有足够多的神经元、层数和训练迭代周期，模型最终可以在训练集上达到完美的精度，此时测试集的准确性却下降了。</li>
</ul>
</li>
<li><p><strong>训练误差（training error）</strong>：模型在训练数据集上计算得到的误差</p>
</li>
<li><p><strong>泛化误差（generalization error）</strong>：模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。</p>
<ul>
<li>我们永远不能准确地计算出泛化误差。这是因为无限多的数据样本是一个虚构的对象。<strong>在实际中，我们只能通过将模型应用于一个独立的测试集来估计泛化误差</strong>，该测试集由随机选取的、未曾在训练集中出现的数据样本构成。</li>
</ul>
</li>
<li><p><strong>独立同分布假设</strong>：在我们目前已探讨、并将在之后继续探讨的监督学习情景中，我们假设训练数据和测试数据都是从相同的分布中独立提取的。这通常被称为<strong>独立同分布假设</strong>（i.i.d. assumption），这意味着对数据进行采样的过程没有进行“记忆”。换句话说，抽取的第2个样本和第3个样本的相关性，并不比抽取的第2个样本和第200万个样本的相关性更强。</p>
<ul>
<li>有时候我们即使轻微违背独立同分布假设，模型仍将继续运行得非常好。</li>
<li>有些违背独立同分布假设的行为肯定会带来麻烦</li>
<li>当我们训练模型时，我们试图找到一个能够尽可能拟合训练数据的函数。但是如果它执行地“太好了”，而不能对看不见的数据做到很好泛化，就会导致过拟合。这种情况正是我们想要避免或控制的。深度学习中有许多启发式的技术旨在防止过拟合。</li>
</ul>
</li>
</ul>
<h3 id="模型复杂性"><a href="#模型复杂性" class="headerlink" title="模型复杂性"></a>模型复杂性</h3><p> <br>模型复杂性由什么构成是一个复杂的问题。一个模型是否能很好地泛化取决于很多因素。例如，具有更多参数的模型可能被认为更复杂，参数有更大取值范围的模型可能更为复杂。<strong>通常对于神经网络，我们认为需要更多训练迭代的模型比较复杂，而需要早停（early stopping）的模型（即较少训练迭代周期）就不那么复杂。</strong></p>
<ul>
<li>统计学家认为，能够轻松解释任意事实的模型是复杂的，而表达能力有限但仍能很好地解释数据的模型可能更有现实用途。</li>
<li>如果一个理论能拟合数据，且有具体的测试可以用来证明它是错误的，那么它就是好的。</li>
</ul>
<h3 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h3><p>在机器学习中，我们通常在<strong>评估几个候选模型后选择最终的模型</strong>。 这个过程叫做<strong>模型选择</strong>。<br>例如，训练多层感知机模型时，我们可能希望比较具有不同数量的隐藏层、不同数量的隐藏单元以及不同的激活函数组合的模型。 <strong>为了确定候选模型中的最佳模型，我们通常会使用验证集。</strong></p>
<h4 id="验证集"><a href="#验证集" class="headerlink" title="验证集"></a>验证集</h4><p><strong>原则上，在我们确定所有的超参数之前，我们不希望用到测试集。</strong> 如果我们在模型选择过程中使用测试数据，可能会有过拟合测试数据的风险，那就麻烦大了。</p>
<blockquote>
<p>如果我们过拟合了训练数据，还可以在测试数据上的评估来判断过拟合。但是如果我们过拟合了测试数据，我们又该怎么知道呢？</p>
</blockquote>
<p><strong>因此，我们决不能依靠测试数据进行模型选择</strong>。 然而，<strong>我们也不能仅仅依靠训练数据来选择模型，因为我们无法估计训练数据的泛化误差。</strong></p>
<p>在实际应用中，情况变得更加复杂。 虽然理想情况下我们只会使用测试数据一次， 以评估最好的模型或比较一些模型效果，但现实是测试数据很少在使用一次后被丢弃。 <strong>我们很少能有充足的数据来对每一轮实验采用全新测试集。</strong></p>
<p><strong>解决此问题的常见做法是将我们的数据分成三份</strong>，除了训练和测试数据集之外，还<strong>增加一个验证数据集（validation dataset），也叫验证集（validation set）</strong>。但现实是验证数据和测试数据之间的边界模糊得令人担忧。除非另有明确说明，<strong>否则在这本书的实验中，我们实际上是在使用应该被正确地称为训练数据和验证数据的数据集，并没有真正的测试数据集。因此，书中每次实验报告的准确度都是验证集准确度，而不是测试集准确度。</strong></p>
<h4 id="K-折折交叉验证"><a href="#K-折折交叉验证" class="headerlink" title="K 折折交叉验证"></a>K 折折交叉验证</h4><p><strong>当训练数据稀缺时，我们甚至可能无法提供足够的数据来构成一个合适的验证集。这个问题的一个流行的解决方案是采用 K 折交叉验证。</strong><br>这里，原始训练数据被分成 K 个不重叠的子集。然后执行 K 次模型训练和验证，每次在 K−1个子集上进行训练，并在剩余的一个子集（在该轮中没有用于训练的子集）上进行验证。最后，通过对 K 次实验的结果取平均来估计训练和验证误差。</p>
<h3 id="欠拟合-过拟合"><a href="#欠拟合-过拟合" class="headerlink" title="欠拟合&#x2F;过拟合"></a>欠拟合&#x2F;过拟合</h3><p>当我们比较训练和验证误差时，我们要注意两种常见的情况。</p>
<ol>
<li><strong>训练误差和验证误差都很严重，但它们之间仅有一点差距。</strong> <ul>
<li>如果模型不能降低训练误差，这可能意味着模型过于简单（即表达能力不足），无法捕获试图学习的模式。</li>
<li>此外，由于我们的训练和验证误差之间的泛化误差很小，我们有理由相信可以用一个更复杂的模型降低训练误差。这种现象被称为<strong>欠拟合</strong>（underfitting）。</li>
</ul>
</li>
<li><strong>训练误差明显低于验证误差时要小心，这表明严重的过拟合（overfitting）</strong>。注意，过拟合并不总是一件坏事。特别是在深度学习领域，众所周知，最好的预测模型在训练数据上的表现往往比在保留（验证）数据上好得多。<strong>最终，我们通常更关心验证误差，而不是训练误差和验证误差之间的差距。</strong></li>
</ol>
<p><strong>是否过拟合或欠拟合可能取决于模型复杂性和可用训练数据集的大小</strong>。</p>
<ul>
<li><strong>模型复杂性</strong>：高阶多项式函数比低阶多项式函数复杂得多。高阶多项式的参数较多，模型函数的选择范围较广。因此在固定训练数据集的情况下，<strong>高阶多项式函数相对于低阶多项式的训练误差应该始终更低（最坏也是相等）</strong> 。<strong>事实上，当数据样本包含了 x  的不同值时，函数阶数等于数据样本数量的多项式函数可以完美拟合训练集。</strong><br><img src="https://cdn.jsdmirror.com/gh/liuke101/PicGo-For-Hexo/image/202408010046349.svg" alt="capacity-vs-error"> <blockquote>
<p>模型复杂度对欠拟合和过拟合的影响</p>
</blockquote>
</li>
<li><strong>数据集大小</strong>： <strong>训练数据集中的样本越少，我们就越有可能（且更严重地）过拟合</strong>。随着训练数据量的增加，泛化误差通常会减小，一般来说更多的数据不会有什么坏处。对于固定的任务和数据分布，模型复杂性和数据集大小之间通常存在关系。 给出更多的数据，我们可能会尝试拟合一个更复杂的模型。 能够拟合更复杂的模型可能是有益的。 如果没有足够的数据，简单的模型可能更有用。 对于许多任务，深度学习只有在有数千个训练样本时才优于线性模型。</li>
</ul>
<h2 id="4-正则化模型-对抗过拟合"><a href="#4-正则化模型-对抗过拟合" class="headerlink" title="4 正则化模型-对抗过拟合"></a>4 正则化模型-对抗过拟合</h2><p>本节我们将介绍一些<strong>正则化模型</strong>的技术。 <strong>我们总是可以通过去收集更多的训练数据来缓解过拟合。但这可能成本很高，耗时颇多，或者完全超出我们的控制，因而在短期内不可能做到</strong>。 <br>假设我们已经拥有尽可能多的高质量数据，我们便可以将重点放在正则化技术上。</p>
<p>在多项式回归的例子（ <a target="_blank" rel="noopener" href="http://zh.d2l.ai/chapter_multilayer-perceptrons/underfit-overfit.html#sec-model-selection">4.4节</a>）中，我们可以通过调整拟合多项式的阶数来限制模型的容量。实际上，限制特征的数量是缓解过拟合的一种常用技术。然而，简单地丢弃特征对这项工作来说可能过于生硬。<br>仅仅通过简单的限制特征数量（在多项式回归中体现为限制阶数），可能仍然使模型在过简单和过复杂中徘徊，我们需要一个<strong>更细粒度的工具来调整函数的复杂性，使其达到一个合适的平衡位置</strong>。</p>
<h3 id="范数与权重衰减"><a href="#范数与权重衰减" class="headerlink" title="范数与权重衰减"></a>范数与权重衰减</h3><blockquote>
<p>[!NOTE] 约定<br>在这本书中，我们将默认使用简单的启发式方法，即在深层网络的所有层上应用权重衰减。</p>
</blockquote>
<p>在训练参数化机器学习模型时， <strong>_权重衰减_（weight decay）是最广泛使用的正则化的技术</strong>之一，它通常也被称为 $L_2$ 正则化。这项技术通过函数与零的距离来衡量函数的复杂度</p>
<ul>
<li><p>正则化是处理过拟合的常用方法：<strong>在训练集的损失函数中加入惩罚项，以降低学习到的模型的复杂度。</strong></p>
</li>
<li><p>保持模型简单的一个特别的选择是使用 $L_2$ 惩罚的权重衰减。这会导致学习算法更新步骤中的权重衰减。</p>
</li>
<li><p>权重衰减功能在深度学习框架的优化器中提供。<br>由于权重衰减在神经网络优化中很常用，深度学习框架为了便于我们使用权重衰减，将权重衰减集成到优化算法中，以便与任何损失函数结合使用。此外，这种集成还有计算上的好处，允许在不增加任何额外的计算开销的情况下向算法中添加权重衰减。由于更新的权重衰减部分仅依赖于每个参数的当前值，因此优化器必须至少接触每个参数一次。</p>
</li>
</ul>
<p>在下面的代码中，我们在<strong>实例化优化器时直接通过 <code>weight_decay</code> 指定 weight decay 超参数</strong>。<strong>默认情况下，PyTorch 同时衰减权重和偏移。</strong> 这里我们只为权重设置了 <code>weight_decay</code>，所以偏置参数 $b$ 不会衰减。</p>
<figure class="highlight python"><figcaption><span>h:9</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_concise</span>(<span class="params">wd</span>): <span class="comment">#wd为衰减参数</span></span><br><span class="line">    net = nn.Sequential(nn.Linear(num_inputs, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> net.parameters():</span><br><span class="line">        param.data.normal_()</span><br><span class="line">    loss = nn.MSELoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">    num_epochs, lr = <span class="number">100</span>, <span class="number">0.003</span></span><br><span class="line">    <span class="comment"># 偏置参数没有衰减</span></span><br><span class="line">    trainer = torch.optim.SGD([</span><br><span class="line">        &#123;<span class="string">&quot;params&quot;</span>:net[<span class="number">0</span>].weight,<span class="string">&#x27;weight_decay&#x27;</span>: wd&#125;,</span><br><span class="line">        &#123;<span class="string">&quot;params&quot;</span>:net[<span class="number">0</span>].bias&#125;], lr=lr)</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epochs&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>,</span><br><span class="line">                            xlim=[<span class="number">5</span>, num_epochs], legend=[<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;test&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            trainer.zero_grad()</span><br><span class="line">            l = loss(net(X), y)</span><br><span class="line">            l.mean().backward()</span><br><span class="line">            trainer.step()</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>,</span><br><span class="line">                         (d2l.evaluate_loss(net, train_iter, loss),</span><br><span class="line">                          d2l.evaluate_loss(net, test_iter, loss)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;w的L2范数：&#x27;</span>, net[<span class="number">0</span>].weight.norm().item())</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#禁用权重衰减，</span></span><br><span class="line">train_concise(<span class="number">0</span>)</span><br><span class="line"><span class="comment">#w的L2范数： 13.727912902832031</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#开启权重衰减</span></span><br><span class="line">train_concise(<span class="number">3</span></span><br><span class="line"><span class="comment">#的L2范数： 0.3890590965747833</span></span><br></pre></td></tr></table></figure>

<h3 id="暂退法（Dropout）"><a href="#暂退法（Dropout）" class="headerlink" title="暂退法（Dropout）"></a>暂退法（Dropout）</h3><ul>
<li>暂退法在前向传播过程中，计算每一内部层的同时丢弃一些神经元。</li>
<li>暂退法可以避免过拟合，它通常与控制权重向量的维数和大小结合使用的。</li>
<li>暂退法将活性值ℎ替换为具有期望值ℎ的随机变量。</li>
<li>暂退法仅在训练期间使用。</li>
</ul>
<p>暂退法在前向传播过程中，计算每一内部层的同时注入噪声，这已经成为训练神经网络的常用技术。</p>
<p>这种方法之所以被称为暂退法，因为我们<strong>从表面上看是在训练过程中丢弃（drop out）一些神经元</strong>。在整个训练过程的<strong>每一次迭代中，标准暂退法包括在计算下一层之前将当前层中的一些节点置零。</strong></p>
<p>关键的挑战就是如何注入这种噪声。一种想法是以一种无偏向（unbiased）的方式注入噪声。这样在固定住其他层时，每一层的期望值等于没有噪音时的值。</p>
<p>回想一下带有1个隐藏层和5个隐藏单元的多层感知机。当我们将暂退法应用到隐藏层，以 $p$ 的概率将隐藏单元置为零时，结果可以看作一个只包含原始神经元子集的网络。<br>如图，删除了ℎ2和ℎ5，因此输出的计算不再依赖于ℎ2或ℎ5，并且它们各自的梯度在执行反向传播时也会消失。这样，输出层的计算不能过度依赖于ℎ1,…,ℎ5的任何一个元素</p>
<p><img src="https://cdn.jsdmirror.com/gh/liuke101/PicGo-For-Hexo/image/202408010046350.svg" alt="dropout2"></p>
<p><strong>通常，我们在测试时不用暂退法。给定一个训练好的模型和一个新的样本，我们不会丢弃任何节点，因此不需要标准化。</strong> 然而也有一些例外：一些研究人员在测试时使用暂退法，用于估计神经网络预测的“不确定性”： 如果通过许多不同的暂退法遮盖后得到的预测结果都是一致的，那么我们可以说网络发挥更稳定。</p>
<p>对于深度学习框架的高级 API，我们只需在每个全连接层之后添加一个 <code>Dropout</code> 层，将暂退概率作为唯一的参数传递给它的构造函数。在训练时，<code>Dropout</code> 层将根据指定的暂退概率随机丢弃上一层的输出（相当于下一层的输入）。<strong>在测试时，<code>Dropout</code> 层仅传递数据。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Flatten(),</span><br><span class="line">        nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        <span class="comment"># 在第一个全连接层之后添加一个dropout层</span></span><br><span class="line">        nn.Dropout(dropout1),</span><br><span class="line">        nn.Linear(<span class="number">256</span>, <span class="number">256</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        <span class="comment"># 在第二个全连接层之后添加一个dropout层</span></span><br><span class="line">        nn.Dropout(dropout2),</span><br><span class="line">        nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">net.apply(init_weights);</span><br></pre></td></tr></table></figure>

<p>接下来，我们对模型进行训练和测试。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure>

<p><img src="https://cdn.jsdmirror.com/gh/liuke101/PicGo-For-Hexo/image/202408010046351.svg" alt="output_dropout_1110bf_96_0"></p>
<h2 id="5-数值稳定性和模型初始化"><a href="#5-数值稳定性和模型初始化" class="headerlink" title="5 数值稳定性和模型初始化"></a>5 数值稳定性和模型初始化</h2><p><strong>初始化方案的选择在神经网络学习中起着举足轻重的作用，它对保持数值稳定性至关重要。</strong>  此外，这些初始化方案的选择可以与非线性激活函数的选择有趣的结合在一起。我们选择哪个函数以及如何初始化参数可以决定优化算法收敛的速度有多快。糟糕选择可能会导致我们在训练时遇到梯度爆炸或梯度消失。</p>
<h3 id="梯度爆炸-梯度消失"><a href="#梯度爆炸-梯度消失" class="headerlink" title="梯度爆炸&#x2F;梯度消失"></a>梯度爆炸&#x2F;梯度消失</h3><p><strong>梯度爆炸</strong>（gradient exploding）： 参数更新过大，破坏了模型的稳定收敛；<br><strong>梯度消失</strong>（gradient vanishing）： 参数更新过小，在每次更新时几乎不会移动，导致模型无法学习。</p>
<ul>
<li>梯度消失示例：Sigmoid 函数导数图像如下<br>![[Pytorch精粹#^rb6csu]]</li>
</ul>
<h3 id="打破对称性"><a href="#打破对称性" class="headerlink" title="打破对称性"></a>打破对称性</h3><ul>
<li>梯度消失和梯度爆炸是深度网络中常见的问题。在参数初始化时需要非常小心，以确保梯度和参数可以得到很好的控制。</li>
<li>需要用启发式的初始化方法来确保初始梯度既不太大也不太小。</li>
<li>ReLU激活函数缓解了梯度消失问题，这样可以加速收敛。</li>
<li>随机初始化是保证在进行优化前打破对称性的关键。</li>
</ul>
<p><strong>神经网络设计中的另一个问题是其参数化所固有的对称性</strong>。假设我们有一个简单的多层感知机，它有一个隐藏层和两个隐藏单元。在这种情况下，我们可以对第一层的权重 $W^{(1)}$ 进行重排列，并且同样对输出层的权重进行重排列，可以获得相同的函数。第一个隐藏单元与第二个隐藏单元没有什么特别的区别。<strong>换句话说，我们在每一层的隐藏单元之间具有排列对称性。</strong><br>假设输出层将上述两个隐藏单元的多层感知机转换为仅一个输出单元。想象一下，如果我们将隐藏层的所有参数初始化为 $\mathbf{W}^{(1)} &#x3D; c$， $c$ 为常量，会发生什么？ 在这种情况下，在前向传播期间，两个隐藏单元采用相同的输入和参数，产生相同的激活，该激活被送到输出单元。在反向传播期间，根据参数 $\mathbf{W}^{(1)}$ 对输出单元进行微分，得到一个梯度，其元素都取相同的值。因此，在基于梯度的迭代（例如，小批量随机梯度下降）之后， $\mathbf{W}^{(1)}$ 的所有元素仍然采用相同的值。这样的迭代永远不会打破对称性，<strong>我们可能永远也无法实现网络的表达能力。隐藏层的行为就好像只有一个单元</strong>。<strong>请注意，虽然小批量随机梯度下降不会打破这种对称性，但暂退法正则化可以。</strong></p>
<p>虽然小批量随机梯度下降不会打破这种对称性，但暂退法正则化可以。</p>
<h3 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h3><p>解决（或至少减轻）上述问题的一种方法是进行参数初始化，优化期间的注意和适当的正则化也可以进一步提高稳定性。</p>
<h4 id="默认初始化"><a href="#默认初始化" class="headerlink" title="默认初始化"></a>默认初始化</h4><p>在前面的部分中，前文我们多次使用正态分布来初始化权重值。<strong>如果我们不指定初始化方法，框架将使用默认的随机初始化方法，对于中等难度的问题，这种方法通常很有效。</strong></p>
<h4 id="Xavier-初始化"><a href="#Xavier-初始化" class="headerlink" title="Xavier 初始化"></a>Xavier 初始化</h4><p>略</p>
<ul>
<li>Xavier初始化表明，对于每一层，输出的方差不受输入数量的影响，任何梯度的方差不受输出数量的影响。</li>
</ul>
<p>深度学习框架通常实现十几种不同的启发式方法。此外，参数初始化一直是深度学习基础研究的热点领域。其中包括专门用于参数绑定（共享）、超分辨率、序列模型和其他情况的启发式算法。</p>
<h1 id="四、深度学习计算"><a href="#四、深度学习计算" class="headerlink" title="四、深度学习计算"></a>四、深度学习计算</h1><h2 id="1-层和块"><a href="#1-层和块" class="headerlink" title="1 层和块"></a>1 层和块</h2><ul>
<li>一个块可以由许多层组成；一个块可以由许多块组成。</li>
<li>块可以包含代码。</li>
<li>块负责大量的内部处理，包括参数初始化和反向传播。</li>
<li>层和块的顺序连接由 <code>Sequential</code> 块处理。</li>
</ul>
<p>对于多层感知机而言，整个模型及其组成层都是这种架构： 整个模型接受原始输入（特征），生成输出（预测），并包含一些参数（所有组成层的参数集合）。同样，每个单独的层接收输入（由前一层提供），生成输出（到下一层的输入），并且具有一组可调参数，这些参数根据从下一层反向传播的信号进行更新。<br>为了实现这些复杂的网络，我们引入了神经网络<strong>块</strong>的概念。 <strong>块（block）可以描述单个层、由多个层组成的组件或整个模型本身。</strong> 使用块进行抽象的一个好处是可以将一些块组合成更大的组件，这一过程通常是递归的，如 <a target="_blank" rel="noopener" href="http://zh.d2l.ai/chapter_deep-learning-computation/model-construction.html#fig-blocks">图5.1.1</a> 所示。通过定义代码来按需生成任意复杂度的块，我们可以通过简洁的代码实现复杂的神经网络。<br><img src="https://cdn.jsdmirror.com/gh/liuke101/PicGo-For-Hexo/image/202408010046352.svg" alt="blocks"></p>
<blockquote>
<p>多个层被组合成块，形成更大的模型</p>
</blockquote>
<p><strong>从编程的角度来看，块由类（class）表示。它的任何子类都必须定义一个将其输入转换为输出的前向传播函数，并且必须存储任何必需的参数。</strong> 注意，有些块不需要任何参数。 <strong>最后，为了计算梯度，块必须具有反向传播函数。在定义我们自己的块时，由于自动微分提供了一些后端实现，我们只需要考虑前向传播函数和必需的参数。</strong></p>
<p>在构造自定义块之前，我们先回顾一下多层感知机的代码。下面的代码生成一个网络，其中包含一个具有256个单元和 <strong>ReLU 激活函数</strong>的<strong>全连接隐藏层</strong>，然后是一个具有10个隐藏单元且<strong>不带激活函数</strong>的<strong>全连接输出层</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">20</span>, <span class="number">256</span>), nn.ReLU(), nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">X = torch.rand(<span class="number">2</span>, <span class="number">20</span>)</span><br><span class="line">net(X)</span><br></pre></td></tr></table></figure>

<p>在这个例子中，我们通过实例化 <code>nn.Sequential</code> 来构建我们的模型，层的执行顺序是作为参数传递的。 <strong>简而言之，<code>nn.Sequential</code> 定义了一种特殊的 <code>Module</code>，即在 PyTorch 中表示一个块的类，它维护了一个由 <code>Module</code> 组成的有序列表。</strong> 注意，<strong>两个全连接层都是 <code>Linear</code> 类的实例， <code>Linear</code> 类本身就是 <code>Module</code> 的子类。</strong><br>另外，到目前为止，我们一直在通过 <code>net(X)</code> 调用我们的模型来获得模型的输出。这实际上是 <code>net.__call__(X)</code> 的简写。 <strong>这个前向传播函数非常简单： 它将列表中的每个块连接在一起，将每个块的输出作为下一个块的输入。</strong></p>
<h3 id="自定义块"><a href="#自定义块" class="headerlink" title="自定义块"></a>自定义块</h3><p><strong>每个块必须提供的基本功能：</strong></p>
<ol>
<li>将输入数据作为其前向传播函数的参数。</li>
<li>通过前向传播函数来生成输出。请注意，输出的形状可能与输入的形状不同。例如，我们上面模型中的第一个全连接的层接收一个20维的输入，但是返回一个维度为256的输出。</li>
<li>计算其输出关于输入的梯度，可通过其反向传播函数进行访问。通常这是<strong>自动发生</strong>的。</li>
<li>存储和访问前向传播计算所需的参数。</li>
<li>根据需要初始化模型参数。</li>
</ol>
<p>在下面的代码片段中，我们从零开始编写一个块。它包含一个多层感知机，其具有256个隐藏单元的隐藏层和一个10维输出层。<strong>注意，下面的 <code>MLP</code> 类继承了表示块的类 <code>nn.Module</code>。我们的实现只需要提供我们自己的构造函数（Python 中的 <code>__init__</code> 函数）和前向传播函数。</strong><br>注意，除非我们实现一个新的运算符，否则我们不必担心反向传播函数或参数初始化，系统将自动生成这些。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 用模型参数声明层。这里，我们声明两个全连接的层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 调用MLP的父类Module的构造函数来执行必要的初始化。</span></span><br><span class="line">        <span class="comment"># 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.hidden = nn.Linear(<span class="number">20</span>, <span class="number">256</span>)  <span class="comment"># 全连接隐藏层</span></span><br><span class="line">        self.out = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)  <span class="comment"># 全连接输出层</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义模型的前向传播，即如何根据输入X返回所需的模型输出</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。</span></span><br><span class="line">        <span class="comment"># 这里隐藏层使用relu激活函数，输出层不使用激活函数，最后只需要输出最终计算结果</span></span><br><span class="line">        <span class="keyword">return</span> self.out(F.relu(self.hidden(X)))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">net = MLP()</span><br><span class="line">net(X)</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor([[ <span class="number">0.0669</span>,  <span class="number">0.2202</span>, -<span class="number">0.0912</span>, -<span class="number">0.0064</span>,  <span class="number">0.1474</span>, -<span class="number">0.0577</span>, -<span class="number">0.3006</span>,  <span class="number">0.1256</span>, -<span class="number">0.0280</span>,  <span class="number">0.4040</span>],</span><br><span class="line">        [ <span class="number">0.0545</span>,  <span class="number">0.2591</span>, -<span class="number">0.0297</span>,  <span class="number">0.1141</span>,  <span class="number">0.1887</span>,  <span class="number">0.0094</span>, -<span class="number">0.2686</span>,  <span class="number">0.0732</span>, -<span class="number">0.0135</span>,  <span class="number">0.3865</span>]], grad_fn=&lt;AddmmBackward0&gt;)</span><br></pre></td></tr></table></figure>

<p>块的一个主要优点是它的多功能性。我们可以子类化块以创建层（如全连接层的类）、整个模型（如上面的 <code>MLP</code> 类）或具有中等复杂度的各种组件。</p>
<h3 id="顺序快"><a href="#顺序快" class="headerlink" title="顺序快"></a>顺序快</h3><p>现在我们可以更仔细地看看 <code>Sequential</code> 类是如何工作的， <strong><code>Sequential</code> 的设计是为了把其他模块串起来</strong>。<br>为了构建我们自己的简化的 <code>MySequential</code>，我们只需要定义两个关键函数：</p>
<ol>
<li>一种将块逐个追加到列表中的函数；</li>
<li>一种前向传播函数，用于将输入按追加块的顺序传递给块组成的“链条”。</li>
</ol>
<p>下面的 <code>MySequential</code> 类提供了与默认 <code>Sequential</code> 类相同的功能。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MySequential</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 每个模块逐个添加到有序字典 _modules 中</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *args</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">for</span> idx, module <span class="keyword">in</span> <span class="built_in">enumerate</span>(args):</span><br><span class="line">            <span class="comment"># 这里，module是Module子类的一个实例。我们把它保存在&#x27;Module&#x27;类的成员</span></span><br><span class="line">            <span class="comment"># 变量_modules中。_module的类型是OrderedDict</span></span><br><span class="line">            self._modules[<span class="built_in">str</span>(idx)] = module</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前向传播函数被调用时，每个添加的块都按照它们被添加的顺序执行。</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># OrderedDict保证了按照成员添加的顺序遍历它们</span></span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> self._modules.values():</span><br><span class="line">            X = block(X)</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>

<blockquote>
<p>[!question] 为什么使用 <code>_modules</code> 存储 module 而不是自己定义一个 Python 列表？<br><code>_modules</code> 的主要优点是： 在模块的参数初始化过程中，系统知道在 <code>_modules</code> 字典中查找需要初始化参数的子块。</p>
</blockquote>
<p>现在可以使用我们的 <code>MySequential</code> 类重新实现多层感知机。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">net = MySequential(nn.Linear(<span class="number">20</span>, <span class="number">256</span>), nn.ReLU(), nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line">net(X)</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor([[ <span class="number">2.2759e-01</span>, -<span class="number">4.7003e-02</span>,  <span class="number">4.2846e-01</span>, -<span class="number">1.2546e-01</span>,  <span class="number">1.5296e-01</span>, <span class="number">1.8972e-01</span>,  <span class="number">9.7048e-02</span>,  <span class="number">4.5479e-04</span>, -<span class="number">3.7986e-02</span>,  <span class="number">6.4842e-02</span>],</span><br><span class="line">        [ <span class="number">2.7825e-01</span>, -<span class="number">9.7517e-02</span>,  <span class="number">4.8541e-01</span>, -<span class="number">2.4519e-01</span>, -<span class="number">8.4580e-02</span>, <span class="number">2.8538e-01</span>,  <span class="number">3.6861e-02</span>,  <span class="number">2.9411e-02</span>, -<span class="number">1.0612e-01</span>,  <span class="number">1.2620e-01</span>]],</span><br><span class="line">       grad_fn=&lt;AddmmBackward0&gt;)</span><br></pre></td></tr></table></figure>

<h3 id="在前向传播函数中执行代码"><a href="#在前向传播函数中执行代码" class="headerlink" title="在前向传播函数中执行代码"></a>在前向传播函数中执行代码</h3><p><code>Sequential</code> 类使模型构造变得简单，允许我们组合新的架构，而不必定义自己的类。 <strong>然而，并不是所有的架构都是简单的顺序架构。当需要更强的灵活性时，我们需要定义自己的块</strong>。<br>例如，我们可能希望在前向传播函数中执行 Python 的控制流。此外，我们可能希望执行任意的数学运算，而不是简单地依赖预定义的神经网络层。</p>
<p>到目前为止， 我们网络中的所有操作都对网络的激活值及网络的参数起作用。 然而，<strong>有时我们可能希望合并既不是上一层的结果也不是可更新参数的项， 我们称之为常数参数</strong><br>例如，我们需要一个计算函数 $f(\mathbf{x},\mathbf{w}) &#x3D; c \cdot \mathbf{w}^\top \mathbf{x}$ 的层，其中 $x$ 是输入，$w$ 是参数，$c$ 是是某个在优化过程中没有更新的指定常量。因此我们实现了一个 <code>FixedHiddenMLP</code> 类，如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FixedHiddenMLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 不计算梯度的随机权重参数。因此其在训练期间保持不变</span></span><br><span class="line">        self.rand_weight = torch.rand((<span class="number">20</span>, <span class="number">20</span>), requires_grad=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        self.linear = nn.Linear(<span class="number">20</span>, <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        X = self.linear(X)</span><br><span class="line">        <span class="comment"># 使用创建的常量参数以及relu和mm函数</span></span><br><span class="line">        X = F.relu(torch.mm(X, self.rand_weight) + <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 复用全连接层。这相当于两个全连接层共享参数</span></span><br><span class="line">        X = self.linear(X)</span><br><span class="line">        <span class="comment"># 控制流</span></span><br><span class="line">        <span class="comment"># 注意，此操作可能不会常用于在任何实际任务中， 我们只展示如何将任意代码集成到神经网络计算的流程中。</span></span><br><span class="line">        <span class="keyword">while</span> X.<span class="built_in">abs</span>().<span class="built_in">sum</span>() &gt; <span class="number">1</span>:</span><br><span class="line">            X /= <span class="number">2</span></span><br><span class="line">        <span class="keyword">return</span> X.<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">net = FixedHiddenMLP()</span><br><span class="line">net(X)</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor(<span class="number">0.1862</span>, grad_fn=&lt;SumBackward0&gt;)</span><br></pre></td></tr></table></figure>

<h3 id="嵌套块"><a href="#嵌套块" class="headerlink" title="嵌套块"></a>嵌套块</h3><p>我们可以混合搭配各种组合块的方法。在下面的例子中，我们以一些想到的方法嵌套块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 块中有两个层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NestMLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 层1</span></span><br><span class="line">        self.net = nn.Sequential(nn.Linear(<span class="number">20</span>, <span class="number">64</span>), nn.ReLU(),</span><br><span class="line">                                 nn.Linear(<span class="number">64</span>, <span class="number">32</span>), nn.ReLU())</span><br><span class="line">        <span class="comment"># 层2</span></span><br><span class="line">        self.linear = nn.Linear(<span class="number">32</span>, <span class="number">16</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> self.linear(self.net(X))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 顺序快组合所有块</span></span><br><span class="line">chimera = nn.Sequential(NestMLP(), nn.Linear(<span class="number">16</span>, <span class="number">20</span>), FixedHiddenMLP())</span><br><span class="line">chimera(X)</span><br></pre></td></tr></table></figure>

<h2 id="2-参数管理"><a href="#2-参数管理" class="headerlink" title="2 参数管理"></a>2 参数管理</h2><p>在选择了架构并设置了超参数后，我们就进入了训练阶段。此时，我们的目标是找到使损失函数最小化的模型参数值。经过训练后，我们将需要使用这些参数来做出未来的预测。<br>此外，有时我们希望提取参数，以便在其他环境中复用它们，将模型保存下来，以便它可以在其他软件中执行，或者为了获得科学的理解而进行检查。<br>之前的介绍中，我们只依靠深度学习框架来完成训练的工作，而忽略了操作参数的具体细节。本节，我们将介绍以下内容：</p>
<ul>
<li>访问参数，用于调试、诊断和可视化；</li>
<li>参数初始化；</li>
<li>在不同模型组件间共享参数。</li>
</ul>
<p>我们首先看一下具有单隐藏层的多层感知机。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(), nn.Linear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line">X = torch.rand(size=(<span class="number">2</span>, <span class="number">4</span>))</span><br><span class="line">net(X)</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor([[-<span class="number">0.0970</span>],</span><br><span class="line">        [-<span class="number">0.0827</span>]], grad_fn=&lt;AddmmBackward0&gt;)</span><br></pre></td></tr></table></figure>

<h3 id="参数访问"><a href="#参数访问" class="headerlink" title="参数访问"></a>参数访问</h3><p>我们从已有模型中访问参数。 <strong>当通过 <code>Sequential</code> 类定义模型时，我们可以通过索引来访问模型的任意层</strong>。这就像模型是一个列表一样，每层的参数都在其属性中。</p>
<p>如下所示，我们可以<strong>检查第二个全连接层的参数。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].state_dict())</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">OrderedDict([(<span class="string">&#x27;weight&#x27;</span>, tensor([[-<span class="number">0.0427</span>, -<span class="number">0.2939</span>, -<span class="number">0.1894</span>,  <span class="number">0.0220</span>, -<span class="number">0.1709</span>, -<span class="number">0.1522</span>, -<span class="number">0.0334</span>, -<span class="number">0.2263</span>]])), (<span class="string">&#x27;bias&#x27;</span>, tensor([<span class="number">0.0887</span>]))])</span><br></pre></td></tr></table></figure>

<p>输出的结果告诉我们一些重要的事情： 首先，这个全连接层包含两个参数，分别是该层的权重和偏置。两者都存储为单精度浮点数（float32）。注意，参数名称允许唯一标识每个参数，即使在包含数百个层的网络中也是如此。</p>
<h4 id="目标参数"><a href="#目标参数" class="headerlink" title="目标参数"></a>目标参数</h4><p><strong>注意，每个参数都表示为参数类的一个实例。要对参数执行任何操作，首先我们需要访问底层的数值</strong>。有几种方法可以做到这一点。有些比较简单，而另一些则比较通用。<br>下面的代码从第二个全连接层（即第三个神经网络层）提取偏置，提取后返回的是一个参数类实例，并进一步<strong>访问该参数的值</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(net[<span class="number">2</span>].bias))</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias.data)</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;torch.nn.parameter.Parameter&#x27;</span>&gt;</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([<span class="number">0.0887</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">tensor([<span class="number">0.0887</span>])</span><br></pre></td></tr></table></figure>

<p>参数是复合的对象，包含值、梯度和额外信息。这就是我们需要显式参数值的原因。</p>
<p>除了值之外，我们还可以<strong>访问每个参数的梯度</strong>。在上面这个网络中，由于我们还没有调用反向传播，所以参数的梯度处于初始状态。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">net[<span class="number">2</span>].weight.grad == <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure>

<h4 id="一次性访问所有参数"><a href="#一次性访问所有参数" class="headerlink" title="一次性访问所有参数"></a>一次性访问所有参数</h4><p>当我们处理更复杂的块（例如，嵌套块）时，情况可能会变得特别复杂，因为我们需要递归整个树来提取每个子块的参数。</p>
<p>下面，我们将通过演示来<strong>比较访问第一个全连接层的参数和访问所有层。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 访问第一个全连接层</span></span><br><span class="line"><span class="built_in">print</span>(*[(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> net[<span class="number">0</span>].named_parameters()])</span><br><span class="line"><span class="comment">###</span></span><br><span class="line">(<span class="string">&#x27;weight&#x27;</span>, torch.Size([<span class="number">8</span>, <span class="number">4</span>])) (<span class="string">&#x27;bias&#x27;</span>, torch.Size([<span class="number">8</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment">#访问所有层</span></span><br><span class="line"><span class="built_in">print</span>(*[(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters()])</span><br><span class="line"><span class="comment">###</span></span><br><span class="line">(<span class="string">&#x27;0.weight&#x27;</span>, torch.Size([<span class="number">8</span>, <span class="number">4</span>])) (<span class="string">&#x27;0.bias&#x27;</span>, torch.Size([<span class="number">8</span>])) (<span class="string">&#x27;2.weight&#x27;</span>, torch.Size([<span class="number">1</span>, <span class="number">8</span>])) (<span class="string">&#x27;2.bias&#x27;</span>, torch.Size([<span class="number">1</span>]))</span><br></pre></td></tr></table></figure>

<blockquote>
<p>自动给权重和编制编号 <code>x.</code></p>
</blockquote>
<p>这为我们提供了另一种访问网络参数的方式，如下所示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net.state_dict()[<span class="string">&#x27;2.bias&#x27;</span>].data</span><br><span class="line"></span><br><span class="line">tensor([<span class="number">0.0887</span>])</span><br></pre></td></tr></table></figure>

<h4 id="从嵌套块收集参数"><a href="#从嵌套块收集参数" class="headerlink" title="从嵌套块收集参数"></a>从嵌套块收集参数</h4><p>如果我们将多个块相互嵌套，参数命名约定是如何工作的？</p>
<p>我们首先定义一个生成块的函数（可以说是“块工厂”），然后将这些块组合到更大的块中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">block1</span>():</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(),</span><br><span class="line">                         nn.Linear(<span class="number">8</span>, <span class="number">4</span>), nn.ReLU())</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">block2</span>():</span><br><span class="line">    net = nn.Sequential()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">        <span class="comment"># 在这里嵌套</span></span><br><span class="line">        net.add_module(<span class="string">f&#x27;block <span class="subst">&#123;i&#125;</span>&#x27;</span>, block1())</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line">rgnet = nn.Sequential(block2(), nn.Linear(<span class="number">4</span>, <span class="number">1</span>))</span><br><span class="line">rgnet(X)</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor([[<span class="number">0.2596</span>],</span><br><span class="line">        [<span class="number">0.2596</span>]], grad_fn=&lt;AddmmBackward0&gt;)</span><br></pre></td></tr></table></figure>

<p>设计了网络后，我们看看它是如何工作的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(rgnet)</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">Sequential(</span><br><span class="line">  (<span class="number">0</span>): Sequential(</span><br><span class="line">    (block <span class="number">0</span>): Sequential(</span><br><span class="line">      (<span class="number">0</span>): Linear(in_features=<span class="number">4</span>, out_features=<span class="number">8</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (<span class="number">1</span>): ReLU()</span><br><span class="line">      (<span class="number">2</span>): Linear(in_features=<span class="number">8</span>, out_features=<span class="number">4</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (<span class="number">3</span>): ReLU()</span><br><span class="line">    )</span><br><span class="line">    (block <span class="number">1</span>): Sequential(</span><br><span class="line">      (<span class="number">0</span>): Linear(in_features=<span class="number">4</span>, out_features=<span class="number">8</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (<span class="number">1</span>): ReLU()</span><br><span class="line">      (<span class="number">2</span>): Linear(in_features=<span class="number">8</span>, out_features=<span class="number">4</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (<span class="number">3</span>): ReLU()</span><br><span class="line">    )</span><br><span class="line">    (block <span class="number">2</span>): Sequential(</span><br><span class="line">      (<span class="number">0</span>): Linear(in_features=<span class="number">4</span>, out_features=<span class="number">8</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (<span class="number">1</span>): ReLU()</span><br><span class="line">      (<span class="number">2</span>): Linear(in_features=<span class="number">8</span>, out_features=<span class="number">4</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (<span class="number">3</span>): ReLU()</span><br><span class="line">    )</span><br><span class="line">    (block <span class="number">3</span>): Sequential(</span><br><span class="line">      (<span class="number">0</span>): Linear(in_features=<span class="number">4</span>, out_features=<span class="number">8</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (<span class="number">1</span>): ReLU()</span><br><span class="line">      (<span class="number">2</span>): Linear(in_features=<span class="number">8</span>, out_features=<span class="number">4</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (<span class="number">3</span>): ReLU()</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (<span class="number">1</span>): Linear(in_features=<span class="number">4</span>, out_features=<span class="number">1</span>, bias=<span class="literal">True</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>因为层是分层嵌套的，所以我们也可以像通过嵌套列表索引一样访问它们。下面，我们访问第一个主要的块中、第二个子块的第一层的偏置项。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rgnet[<span class="number">0</span>][<span class="number">1</span>][<span class="number">0</span>].bias.data</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor([ <span class="number">0.1999</span>, -<span class="number">0.4073</span>, -<span class="number">0.1200</span>, -<span class="number">0.2033</span>, -<span class="number">0.1573</span>,  <span class="number">0.3546</span>, -<span class="number">0.2141</span>, -<span class="number">0.2483</span>])</span><br></pre></td></tr></table></figure>

<h3 id="参数初始化-1"><a href="#参数初始化-1" class="headerlink" title="参数初始化"></a>参数初始化</h3><p>深度学习框架提供默认随机初始化，也允许我们创建自定义初始化方法，满足我们通过其他规则实现初始化权重。</p>
<p><strong>默认情况下，PyTorch 会根据一个范围均匀地初始化权重和偏置矩阵，这个范围是根据输入和输出维度计算出的。 PyTorch 的 <code>nn.init</code> 模块提供了多种预置初始化方法。</strong></p>
<h4 id="内置初始化"><a href="#内置初始化" class="headerlink" title="内置初始化"></a>内置初始化</h4><p>让我们首先调用内置的初始化器。下面的代码<strong>将所有权重参数初始化为标准差为0.01的高斯随机变量，且将偏置参数设置为0。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_normal</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">        nn.init.zeros_(m.bias)</span><br><span class="line">net.apply(init_normal)</span><br><span class="line">net[<span class="number">0</span>].weight.data[<span class="number">0</span>], net[<span class="number">0</span>].bias.data[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">(tensor([-<span class="number">0.0214</span>, -<span class="number">0.0015</span>, -<span class="number">0.0100</span>, -<span class="number">0.0058</span>]), tensor(<span class="number">0.</span>))</span><br></pre></td></tr></table></figure>

<p>我们还可以<strong>将所有参数初始化为给定的常数，比如初始化为1。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_constant</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.constant_(m.weight, <span class="number">1</span>)</span><br><span class="line">        nn.init.zeros_(m.bias)</span><br><span class="line">net.apply(init_constant)</span><br><span class="line">net[<span class="number">0</span>].weight.data[<span class="number">0</span>], net[<span class="number">0</span>].bias.data[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">(tensor([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]), tensor(<span class="number">0.</span>))</span><br></pre></td></tr></table></figure>


<p>我们还可以<strong>对某些块应用不同的初始化方法</strong>。例如，下面我们使用 <strong>Xavier 初始化</strong>方法初始化第一个神经网络层，然后将第三个神经网络层初始化为常量值42。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_xavier</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.xavier_uniform_(m.weight)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_42</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.constant_(m.weight, <span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">net[<span class="number">0</span>].apply(init_xavier)</span><br><span class="line">net[<span class="number">2</span>].apply(init_42)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">0</span>].weight.data[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data)</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor([ <span class="number">0.5236</span>,  <span class="number">0.0516</span>, -<span class="number">0.3236</span>,  <span class="number">0.3794</span>])</span><br><span class="line">tensor([[<span class="number">42.</span>, <span class="number">42.</span>, <span class="number">42.</span>, <span class="number">42.</span>, <span class="number">42.</span>, <span class="number">42.</span>, <span class="number">42.</span>, <span class="number">42.</span>]])</span><br></pre></td></tr></table></figure>

<h4 id="自定义初始化"><a href="#自定义初始化" class="headerlink" title="自定义初始化"></a>自定义初始化</h4><p>有时，深度学习框架没有提供我们需要的初始化方法。在下面的例子中，我们使用以下的分布为任意权重参数 $w$ 定义初始化方法：<br>$$<br>\begin{split}\begin{aligned}<br>    w \sim \begin{cases}<br>        U(5, 10) &amp; \text{ 可能性 } \frac{1}{4} \<br>            0    &amp; \text{ 可能性 } \frac{1}{2} \<br>        U(-10, -5) &amp; \text{ 可能性 } \frac{1}{4}<br>    \end{cases}<br>\end{aligned}\end{split}<br>$$<br>同样，我们实现了一个 <code>my_init</code> 函数来应用到 <code>net</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">my_init</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Init&quot;</span>, *[(name, param.shape)</span><br><span class="line">                        <span class="keyword">for</span> name, param <span class="keyword">in</span> m.named_parameters()][<span class="number">0</span>])</span><br><span class="line">        nn.init.uniform_(m.weight, -<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">        m.weight.data *= m.weight.data.<span class="built_in">abs</span>() &gt;= <span class="number">5</span></span><br><span class="line"></span><br><span class="line">net.apply(my_init)</span><br><span class="line">net[<span class="number">0</span>].weight[:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">Init weight torch.Size([<span class="number">8</span>, <span class="number">4</span>])</span><br><span class="line">Init weight torch.Size([<span class="number">1</span>, <span class="number">8</span>])</span><br><span class="line"></span><br><span class="line">tensor([[<span class="number">5.4079</span>, <span class="number">9.3334</span>, <span class="number">5.0616</span>, <span class="number">8.3095</span>],</span><br><span class="line">        [<span class="number">0.0000</span>, <span class="number">7.2788</span>, -<span class="number">0.0000</span>, -<span class="number">0.0000</span>]], grad_fn=&lt;SliceBackward0&gt;)</span><br></pre></td></tr></table></figure>

<p>注意，我们始终可以直接设置参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">net[<span class="number">0</span>].weight.data[:] += <span class="number">1</span></span><br><span class="line">net[<span class="number">0</span>].weight.data[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">42</span></span><br><span class="line">net[<span class="number">0</span>].weight.data[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor([<span class="number">42.0000</span>, <span class="number">10.3334</span>,  <span class="number">6.0616</span>,  <span class="number">9.3095</span>])</span><br></pre></td></tr></table></figure>

<h3 id="参数绑定"><a href="#参数绑定" class="headerlink" title="参数绑定"></a>参数绑定</h3><p>有时我们希望在多个层间共享参数： <strong>我们可以定义一个稠密层，然后使用它的参数来设置另一个层的参数。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我们需要给共享层一个名称，以便可以引用它的参数</span></span><br><span class="line">shared = nn.Linear(<span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(),</span><br><span class="line">                    shared, nn.ReLU(),</span><br><span class="line">                    shared, nn.ReLU(),</span><br><span class="line">                    nn.Linear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line">net(X)</span><br><span class="line"><span class="comment"># 检查参数是否相同</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data[<span class="number">0</span>] == net[<span class="number">4</span>].weight.data[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 确保它们实际上是同一个对象，而不只是有相同的值</span></span><br><span class="line">net[<span class="number">2</span>].weight.data[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">100</span> <span class="comment">#修改2的时候，也会修改4</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data[<span class="number">0</span>] == net[<span class="number">4</span>].weight.data[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>])</span><br><span class="line">tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>])</span><br></pre></td></tr></table></figure>

<p><strong>这个例子表明第三个和第五个神经网络层的参数是绑定的。它们不仅值相等，而且由相同的张量表示</strong>。因此，如果我们改变其中一个参数，另一个参数也会改变。 </p>
<p>这里有一个问题：<strong>当参数绑定时，梯度会发生什么情况？ 答案是由于模型参数包含梯度，因此在反向传播期间第二个隐藏层 （即第三个神经网络层）和第三个隐藏层（即第五个神经网络层）的梯度会加在一起。</strong></p>
<h2 id="3-延后初始化"><a href="#3-延后初始化" class="headerlink" title="3 延后初始化"></a>3 延后初始化</h2><p>到目前为止，我们忽略了建立网络时需要做的以下这些事情：</p>
<ul>
<li>我们定义了网络架构，但没有指定输入维度。</li>
<li>我们添加层时没有指定前一层的输出维度。</li>
<li>我们在初始化参数时，甚至没有足够的信息来确定模型应该包含多少参数。</li>
</ul>
<p>有些读者可能会对我们的代码能运行感到惊讶。 毕竟，深度学习框架无法判断网络的输入维度是什么。 <strong>这里的诀窍是框架的延后初始化（defers initialization）， 即直到数据第一次通过模型传递时，框架才会动态地推断出每个层的大小。</strong></p>
<p><strong>在以后，当使用卷积神经网络时，由于输入维度（即图像的分辨率）将影响每个后续层的维数，有了该技术将更加方便。现在我们在编写代码时无须知道维度是什么就可以设置参数，这种能力可以大大简化定义和修改模型的任务。</strong> </p>
<h2 id="4-自定义层"><a href="#4-自定义层" class="headerlink" title="4 自定义层"></a>4 自定义层</h2><p>深度学习成功背后的一个因素是神经网络的灵活性： 我们可以用创造性的方式组合不同的层，从而设计出适用于各种任务的架构。<br>有时我们会遇到或要自己发明一个现在在深度学习框架中还不存在的层。在这些情况下，必须构建自定义层。本节将展示如何构建自定义层。</p>
<h3 id="不带参数的层"><a href="#不带参数的层" class="headerlink" title="不带参数的层"></a>不带参数的层</h3><p>首先，我们构造一个没有任何参数的自定义层。回忆一下在 <a target="_blank" rel="noopener" href="http://zh.d2l.ai/chapter_deep-learning-computation/model-construction.html#sec-model-construction">5.1节</a>对块的介绍，这应该看起来很眼熟。</p>
<p>下面的 <code>CenteredLayer</code> 类要从其输入中减去均值。要构建它，我们只需继承 <code>nn.Module</code> 基础层类并实现前向传播功能。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CenteredLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> X - X.mean()</span><br></pre></td></tr></table></figure>

<p>让我们向该层提供一些数据，验证它是否能按预期工作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">layer = CenteredLayer()</span><br><span class="line">layer(torch.FloatTensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor([-<span class="number">2.</span>, -<span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>])</span><br></pre></td></tr></table></figure>

<p>现在，我们可以将层作为组件合并到更复杂的模型中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Linear(<span class="number">8</span>, <span class="number">128</span>), CenteredLayer())</span><br></pre></td></tr></table></figure>

<p>作为额外的健全性检查，我们可以在向该网络发送随机数据后，检查均值是否为0。由于我们处理的是浮点数，因为存储精度的原因，我们仍然可能会看到一个非常小的非零数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Y = net(torch.rand(<span class="number">4</span>, <span class="number">8</span>))</span><br><span class="line">Y.mean()</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor(<span class="number">7.4506e-09</span>, grad_fn=&lt;MeanBackward0&gt;)</span><br></pre></td></tr></table></figure>

<h3 id="带参数的层"><a href="#带参数的层" class="headerlink" title="带参数的层"></a>带参数的层</h3><p>下面我们继续定义具有参数的层，<strong>这些参数可以通过训练进行调整</strong>。<br><strong>我们可以使用内置函数来创建参数，这些函数提供一些基本的管理功能。比如管理访问、初始化、共享、保存和加载模型参数。这样做的好处之一是：我们不需要为每个自定义层编写自定义的序列化程序。</strong></p>
<p>现在，让我们实现自定义版本的全连接层。回想一下，该层需要两个参数，一个用于表示权重，另一个用于表示偏置项。在此实现中，我们使用 ReLU 激活函数。该层需要<strong>输入参数</strong>：<code>in_units</code> 和 <code>units</code>，分别<strong>表示输入数和输出数</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyLinear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_units, units</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.randn(in_units, units))</span><br><span class="line">        self.bias = nn.Parameter(torch.randn(units,))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        linear = torch.matmul(X, self.weight.data) + self.bias.data</span><br><span class="line">        <span class="keyword">return</span> F.relu(linear)</span><br><span class="line">    </span><br></pre></td></tr></table></figure>

<p>接下来，我们实例化 <code>MyLinear</code> 类并访问其模型参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">linear = MyLinear(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">linear.weight</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[ <span class="number">0.1775</span>, -<span class="number">1.4539</span>,  <span class="number">0.3972</span>],</span><br><span class="line">        [-<span class="number">0.1339</span>,  <span class="number">0.5273</span>,  <span class="number">1.3041</span>],</span><br><span class="line">        [-<span class="number">0.3327</span>, -<span class="number">0.2337</span>, -<span class="number">0.6334</span>],</span><br><span class="line">        [ <span class="number">1.2076</span>, -<span class="number">0.3937</span>,  <span class="number">0.6851</span>],</span><br><span class="line">        [-<span class="number">0.4716</span>,  <span class="number">0.0894</span>, -<span class="number">0.9195</span>]], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>我们可以使用自定义层直接执行前向传播计算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">linear(torch.rand(<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure>

<p>我们还可以使用自定义层构建模型，就像使用内置的全连接层一样使用自定义层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(MyLinear(<span class="number">64</span>, <span class="number">8</span>), MyLinear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line">net(torch.rand(<span class="number">2</span>, <span class="number">64</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor([[<span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>]])</span><br></pre></td></tr></table></figure>

<h2 id="5-读写文件"><a href="#5-读写文件" class="headerlink" title="5 读写文件"></a>5 读写文件</h2><p>有时我们希望保存训练的模型，以备将来在各种环境中使用（比如在部署中进行预测）。此外，<strong>当运行一个耗时较长的训练过程时，最佳的做法是定期保存中间结果，以确保在服务器电源被不小心断掉时，我们不会损失几天的计算结果</strong>。因此，现在是时候学习如何加载和存储权重向量和整个模型了。</p>
<h3 id="保存和加载向量"><a href="#保存和加载向量" class="headerlink" title="保存和加载向量"></a>保存和加载向量</h3><p>对于<strong>单个张量</strong>，我们可以直接调用 <code>load</code> 和 <code>save</code> 函数分别读写它们。这两个函数都要求我们提供一个名称，<code>save</code> 要求将要保存的变量作为输入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">x = torch.arange(<span class="number">4</span>)</span><br><span class="line">torch.save(x, <span class="string">&#x27;x-file&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>我们现在可以将存储在文件中的数据读回内存。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x2 = torch.load(<span class="string">&#x27;x-file&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure>

<p>我们可以存储一个<strong>张量列表</strong>，然后把它们读回内存。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">y = torch.zeros(<span class="number">4</span>)</span><br><span class="line">torch.save([x, y],<span class="string">&#x27;x-files&#x27;</span>)</span><br><span class="line"></span><br><span class="line">x2, y2 = torch.load(<span class="string">&#x27;x-files&#x27;</span>)</span><br><span class="line">(x2, y2)</span><br></pre></td></tr></table></figure>

<p>我们甚至可以写入或读取<strong>从字符串映射到张量的字典</strong>。当我们要读取或写入模型中的所有权重时，这很方便。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mydict = &#123;<span class="string">&#x27;x&#x27;</span>: x, <span class="string">&#x27;y&#x27;</span>: y&#125;</span><br><span class="line">torch.save(mydict, <span class="string">&#x27;mydict&#x27;</span>)</span><br><span class="line"></span><br><span class="line">mydict2 = torch.load(<span class="string">&#x27;mydict&#x27;</span>)</span><br><span class="line">mydict2</span><br></pre></td></tr></table></figure>

<h3 id="加载和保存模型参数"><a href="#加载和保存模型参数" class="headerlink" title="加载和保存模型参数"></a>加载和保存模型参数</h3><p>保存单个权重向量（或其他张量）确实有用，但是如果我们想保存整个模型，并在以后加载它们，单独保存每个向量则会变得很麻烦。毕竟，我们可能有数百个参数散布在各处。<br>因此，<strong>深度学习框架提供了内置函数来保存和加载整个网络</strong>。需要注意的一个重要细节是，这将<strong>保存模型的参数而不是保存整个模型</strong>。例如，如果我们有一个3层多层感知机，我们需要单独指定架构。因为模型本身可以包含任意代码，所以模型本身难以序列化。<strong>因此，为了恢复模型，我们需要用代码生成架构，然后从磁盘加载参数。</strong></p>
<p>让我们从熟悉的多层感知机开始尝试一下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.hidden = nn.Linear(<span class="number">20</span>, <span class="number">256</span>)</span><br><span class="line">        self.output = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.output(F.relu(self.hidden(x)))</span><br><span class="line"></span><br><span class="line">net = MLP()</span><br><span class="line">X = torch.randn(size=(<span class="number">2</span>, <span class="number">20</span>))</span><br><span class="line">Y = net(X)</span><br></pre></td></tr></table></figure>

<p>接下来，我们将模型的参数存储在一个叫做“mlp.params”的文件中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(net.state_dict(), <span class="string">&#x27;mlp.params&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><strong>为了恢复模型，我们实例化了原始多层感知机模型的一个备份。这里我们不需要随机初始化模型参数，而是直接读取文件中存储的参数。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">clone = MLP()</span><br><span class="line">clone.load_state_dict(torch.load(<span class="string">&#x27;mlp.params&#x27;</span>))</span><br><span class="line">clone.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">MLP(</span><br><span class="line">  (hidden): Linear(in_features=<span class="number">20</span>, out_features=<span class="number">256</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (output): Linear(in_features=<span class="number">256</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>由于两个实例具有相同的模型参数，在输入相同的 <code>X</code> 时，两个实例的计算结果应该相同。让我们来验证一下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Y_clone = clone(X)</span><br><span class="line">Y_clone == Y</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor([[<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">        [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>]])</span><br></pre></td></tr></table></figure>

<h2 id="6-GPU"><a href="#6-GPU" class="headerlink" title="6 GPU"></a>6 GPU</h2><p>我们先看看如何使用单个 NVIDIA GPU 进行计算。首先，确保至少安装了一个 NVIDIA GPU。然后，下载 <a target="_blank" rel="noopener" href="https://developer.nvidia.com/cuda-downloads">NVIDIA驱动和CUDA</a> 并按照提示设置适当的路径。当这些准备工作完成，就可以使用 <code>nvidia-smi</code> 命令来<strong>查看显卡信息</strong>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!nvidia-smi</span><br></pre></td></tr></table></figure>

<p><strong>在 PyTorch 中，每个数组都有一个设备（device），我们通常将其称为环境（context）</strong>。默认情况下，所有变量和相关的计算都分配给 CPU。有时环境可能是 GPU。当我们跨多个服务器部署作业时，事情会变得更加棘手。通过智能地将数组分配给环境，我们可以最大限度地减少在设备之间传输数据的时间。例如，当在带有 GPU 的服务器上训练神经网络时，我们通常希望模型的参数在 GPU 上。</p>
<p>要运行此部分中的程序，至少需要两个 GPU。注意，对大多数桌面计算机来说，这可能是奢侈的，但在云中很容易获得。例如可以使用 AWS EC2的多 GPU 实例。本书的其他章节大都不需要多个 GPU，而<strong>本节只是为了展示数据如何在不同的设备之间传递。</strong></p>
<h3 id="计算设备"><a href="#计算设备" class="headerlink" title="计算设备"></a>计算设备</h3><p>我们可以指定用于存储和计算的设备，如 CPU 和 GPU。<strong>默认情况下，张量是在内存中创建的，然后使用 CPU 计算它。</strong></p>
<p>在 PyTorch 中，CPU 和 GPU 可以用 <code>torch.device(&#39;cpu&#39;)</code> 和 <code>torch.device(&#39;cuda&#39;)</code> 表示。应该注意的是，<code>cpu</code> 设备意味着所有物理 CPU 和内存，这意味着 PyTorch 的计算将尝试使用所有 CPU 核心。然而，<code>gpu</code> 设备只代表一个卡和相应的显存 i。如果有多个 GPU，我们使用 <code>torch.device(f&#39;cuda:&#123;i&#125;&#39;)</code> 来表示第 $i$ 块 GPU（ $i$ 从0开始）。另外，<code>cuda:0</code> 和 <code>cuda</code> 是等价的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">torch.device(<span class="string">&#x27;cpu&#x27;</span>), torch.device(<span class="string">&#x27;cuda&#x27;</span>), torch.device(<span class="string">&#x27;cuda:1&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">(device(<span class="built_in">type</span>=<span class="string">&#x27;cpu&#x27;</span>), device(<span class="built_in">type</span>=<span class="string">&#x27;cuda&#x27;</span>), device(<span class="built_in">type</span>=<span class="string">&#x27;cuda&#x27;</span>, index=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<p>我们可以查询可用 gpu 的数量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.device_count()</span><br></pre></td></tr></table></figure>

<p>现在我们定义了两个方便的函数，这两个函数允许我们在不存在所需所有 GPU 的情况下运行代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">try_gpu</span>(<span class="params">i=<span class="number">0</span></span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;如果存在，则返回gpu(i)，否则返回cpu()&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> torch.cuda.device_count() &gt;= i + <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.device(<span class="string">f&#x27;cuda:<span class="subst">&#123;i&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.device(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">try_all_gpus</span>():  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回所有可用的GPU，如果没有GPU，则返回[cpu(),]&quot;&quot;&quot;</span></span><br><span class="line">    devices = [torch.device(<span class="string">f&#x27;cuda:<span class="subst">&#123;i&#125;</span>&#x27;</span>)</span><br><span class="line">             <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(torch.cuda.device_count())]</span><br><span class="line">    <span class="keyword">return</span> devices <span class="keyword">if</span> devices <span class="keyword">else</span> [torch.device(<span class="string">&#x27;cpu&#x27;</span>)]</span><br><span class="line"></span><br><span class="line">try_gpu(), try_gpu(<span class="number">10</span>), try_all_gpus()</span><br><span class="line"></span><br><span class="line">(device(<span class="built_in">type</span>=<span class="string">&#x27;cuda&#x27;</span>, index=<span class="number">0</span>),</span><br><span class="line"> device(<span class="built_in">type</span>=<span class="string">&#x27;cpu&#x27;</span>),</span><br><span class="line"> [device(<span class="built_in">type</span>=<span class="string">&#x27;cuda&#x27;</span>, index=<span class="number">0</span>), device(<span class="built_in">type</span>=<span class="string">&#x27;cuda&#x27;</span>, index=<span class="number">1</span>)])</span><br></pre></td></tr></table></figure>
<h3 id="张量与-GPU"><a href="#张量与-GPU" class="headerlink" title="张量与 GPU"></a>张量与 GPU</h3><p>我们可以查询张量所在的设备。默认情况下，张量是在 CPU 上创建的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">x.device</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">device(<span class="built_in">type</span>=<span class="string">&#x27;cpu&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><strong>需要注意的是，无论何时我们要对多个项进行操作，它们都必须在同一个设备上。</strong> 例如，如果我们对两个张量求和，我们需要确保两个张量都位于同一个设备上，否则框架将不知道在哪里存储结果，甚至不知道在哪里执行计算。</p>
<h3 id="存储在-GPU-上"><a href="#存储在-GPU-上" class="headerlink" title="存储在 GPU 上"></a>存储在 GPU 上</h3><p>有几种方法可以在GPU上存储张量。 例如，<strong>我们可以在创建张量时指定存储设备</strong>。接 下来，我们在第一个<code>gpu</code>上创建张量变量<code>X</code>。 在GPU上创建的张量只消耗这个GPU的<strong>显存</strong>。 我们可以使用<code>nvidia-smi</code>命令查看显存使用情况。 一般来说，我们需要确保不创建超过GPU显存限制的数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones(<span class="number">2</span>, <span class="number">3</span>, device=try_gpu())</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]], device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>假设我们至少有两个 GPU，下面的代码将在第二个 GPU 上创建一个随机张量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Y = torch.rand(<span class="number">2</span>, <span class="number">3</span>, device=try_gpu(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">tensor([[<span class="number">0.4860</span>, <span class="number">0.1285</span>, <span class="number">0.0440</span>],</span><br><span class="line">        [<span class="number">0.9743</span>, <span class="number">0.4159</span>, <span class="number">0.9979</span>]], device=<span class="string">&#x27;cuda:1&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="复制"><a href="#复制" class="headerlink" title="复制"></a>复制</h3><p>如果我们要计算 <code>X + Y</code>，我们需要决定在哪里执行这个操作。例如，如 图所示，我们可以将 <code>X</code> 传输到第二个 GPU 并在那里执行操作。 不要简单地 <code>X</code> 加上 <code>Y</code>，因为这会导致异常，运行时引擎不知道该怎么做：它在同一设备上找不到数据会导致失败。由于 <code>Y</code> 位于第二个 GPU 上，所以我们需要将 <code>X</code> 移到那里，然后才能执行相加运算。</p>
<p><img src="https://cdn.jsdmirror.com/gh/liuke101/PicGo-For-Hexo/image/202408010046353.svg" alt="copyto"></p>
<blockquote>
<p>复制数据以在同一设备上执行操作</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Z = X.cuda(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(X)</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]], device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]], device=<span class="string">&#x27;cuda:1&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>现在数据在同一个GPU上（<code>Z</code>和<code>Y</code>都在），我们可以将它们相加。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Y + Z</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor([[<span class="number">1.4860</span>, <span class="number">1.1285</span>, <span class="number">1.0440</span>],</span><br><span class="line">        [<span class="number">1.9743</span>, <span class="number">1.4159</span>, <span class="number">1.9979</span>]], device=<span class="string">&#x27;cuda:1&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>假设变量 <code>Z</code> 已经存在于第二个 GPU 上。如果我们还是调用 <code>Z.cuda(1)</code> 会发生什么？ 它将返回 <code>Z</code>，而不会复制并分配新内存。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Z.cuda(<span class="number">1</span>) <span class="keyword">is</span> Z</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure>

<h3 id=""><a href="#" class="headerlink" title=""></a></h3><p>人们使用 GPU 来进行机器学习，因为 GPU 相对运行速度快。<strong>但是在设备（CPU、GPU 和其他机器）之间传输数据比计算慢得多</strong>。这也使得并行化变得更加困难，<strong>因为我们必须等待数据被发送（或者接收），然后才能继续进行更多的操作</strong>。这就是为什么拷贝操作要格外小心。<br>根据经验，多个小操作比一个大操作糟糕得多。此外，一次执行几个操作比代码中散布的许多单个操作要好得多。如果一个设备必须等待另一个设备才能执行其他操作，那么这样的操作可能会阻塞。</p>
<p><strong>最后，当我们打印张量或将张量转换为NumPy格式时， 如果数据不在内存中，框架会首先将其复制到内存中， 这会导致额外的传输开销。 更糟糕的是，它现在受制于全局解释器锁，使得一切都得等待Python完成。</strong></p>
<p>不经意地移动数据可能会显著降低性能。一个典型的错误如下：计算 GPU 上每个小批量的损失，并在命令行中将其报告给用户（或将其记录在 NumPy <code>ndarray</code> 中）时，将触发全局解释器锁，从而使所有 GPU 阻塞。最好是为 GPU 内部的日志分配内存，并且只移动较大的日志。</p>
<h3 id="神经网络与-GPU"><a href="#神经网络与-GPU" class="headerlink" title="神经网络与 GPU"></a>神经网络与 GPU</h3><p>类似地，神经网络模型可以指定设备。下面的代码<strong>将模型参数放在 GPU 上。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Linear(<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">net = net.to(device=try_gpu())</span><br></pre></td></tr></table></figure>

<p>在接下来的几章中，我们将看到更多关于如何在 GPU 上运行模型的例子，因为它们将变得更加计算密集。</p>
<p><strong>当输入为GPU上的张量时，模型将在同一GPU上计算结果。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">net(X)</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">tensor([[-<span class="number">0.4275</span>],</span><br><span class="line">        [-<span class="number">0.4275</span>]], device=<span class="string">&#x27;cuda:0&#x27;</span>, grad_fn=&lt;AddmmBackward0&gt;)</span><br></pre></td></tr></table></figure>

<p>让我们确认模型参数存储在同一个GPU上。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">net[<span class="number">0</span>].weight.data.device</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">device(<span class="built_in">type</span>=<span class="string">&#x27;cuda&#x27;</span>, index=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p><strong>总之，只要所有的数据和参数都在同一个设备上， 我们就可以有效地学习模型</strong>。 在下面的章节中，我们将看到几个这样的例子。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://liuke101.github.io">灵玉</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://liuke101.github.io/post/54732.html">http://liuke101.github.io/post/54732.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://liuke101.github.io" target="_blank">游戏江湖</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Pytorch/">Pytorch</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdmirror.com/gh/liuke101/PicGo-For-Hexo/image/202408010047802.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdmirror.com/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdmirror.com/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赏碎银二两</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://cdn.jsdmirror.com/gh/liuke101/PicGo-For-Hexo/image/202404232247905.jpg" target="_blank"><img class="post-qr-code-img" src="https://cdn.jsdmirror.com/gh/liuke101/PicGo-For-Hexo/image/202404232247905.jpg" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://cdn.jsdmirror.com/gh/liuke101/PicGo-For-Hexo/image/202404232251074.jpg" target="_blank"><img class="post-qr-code-img" src="https://cdn.jsdmirror.com/gh/liuke101/PicGo-For-Hexo/image/202404232251074.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/post/53008.html" title="Python精粹"><img class="cover" src="https://cdn.jsdmirror.com/gh/liuke101/PicGo-For-Hexo/image/202408010045040.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Python精粹</div></div></a></div><div class="next-post pull-right"><a href="/post/14086.html" title="《Re.回响之信》"><img class="cover" src="https://cdn.jsdelivr.net/gh/liuke101/PicGo-For-Hexo/image/202408252142598.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">《Re.回响之信》</div></div></a></div></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div class="comment-switch"><span class="first-comment">Waline</span><span id="switch-btn"></span><span class="second-comment">Livere</span></div></div><div class="comment-wrap"><div><div id="waline-wrap"></div></div><div><div id="lv-container" data-id="city" data-uid="MTAyMC81OTc3Ny8zNjIzOQ=="></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://cdn.jsdmirror.com/gh/liuke101/PicGo-For-Hexo/image/I168.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">灵玉</div><div class="author-info__description">游戏开发</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">79</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">22</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/liuke101"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/liuke101" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="/gamerlk@163.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">1. 如图片加载错误请使用VPN或刷新页面。 2. 文章摘自个人obsidian笔记，存在部分md语法冲突，导致显示异常，暂未完全修复。 3. 文章部分内容参考/引用网络资源，用于个人学习，如有侵权，请联系我删除。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E5%9F%BA%E7%A1%80"><span class="toc-text">一、基础</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C"><span class="toc-text">1 数据操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#01-%E5%BC%A0%E9%87%8F"><span class="toc-text">01 张量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#02-%E8%BF%90%E7%AE%97%E7%AC%A6"><span class="toc-text">02 运算符</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#03-%E5%B9%BF%E6%92%AD"><span class="toc-text">03 广播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#04-%E7%B4%A2%E5%BC%95%E5%92%8C%E5%88%87%E7%89%87"><span class="toc-text">04 索引和切片</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#05-%E8%8A%82%E7%9C%81%E5%86%85%E5%AD%98"><span class="toc-text">05 节省内存</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#06-%E8%BD%AC%E6%8D%A2%E4%B8%BA%E5%85%B6%E4%BB%96-Python-%E5%AF%B9%E8%B1%A1"><span class="toc-text">06 转换为其他 Python 对象</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-text">2 数据预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#01-%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">01 读取数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96CSV"><span class="toc-text">读取CSV</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Dataset-%E5%92%8C-DataLoader"><span class="toc-text">Dataset 和 DataLoader</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#02-%E5%A4%84%E7%90%86%E7%BC%BA%E5%A4%B1%E5%80%BC"><span class="toc-text">02 处理缺失值</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#03-%E8%BD%AC%E6%8D%A2%E4%B8%BA%E5%BC%A0%E9%87%8F%E6%A0%BC%E5%BC%8F"><span class="toc-text">03 转换为张量格式</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0"><span class="toc-text">3 线性代数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#01-%E6%A0%87%E9%87%8F"><span class="toc-text">01 标量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#02-%E5%90%91%E9%87%8F"><span class="toc-text">02 向量</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%82%B9%E7%A7%AF"><span class="toc-text">点积</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#03-%E9%95%BF%E5%BA%A6%E3%80%81%E7%BB%B4%E5%BA%A6%E5%92%8C%E5%BD%A2%E7%8A%B6"><span class="toc-text">03 长度、维度和形状</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#04-%E7%9F%A9%E9%98%B5"><span class="toc-text">04 矩阵</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5-%E5%90%91%E9%87%8F%E7%A7%AF"><span class="toc-text">矩阵-向量积</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5-%E7%9F%A9%E9%98%B5%E7%A7%AF"><span class="toc-text">矩阵-矩阵积</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#torch-mul"><span class="toc-text">torch.mul ()</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#torch-mm"><span class="toc-text">torch.mm ()</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#torch-matmul"><span class="toc-text">torch.matmul ()</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#3-1-%E8%BE%93%E5%85%A5%E9%83%BD%E6%98%AF%E4%BA%8C%E7%BB%B4"><span class="toc-text">3.1 输入都是二维</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#3-2-%E8%BE%93%E5%85%A5%E9%83%BD%E6%98%AF%E4%B8%89%E7%BB%B4"><span class="toc-text">3.2 输入都是三维</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#3-3-%E8%BE%93%E5%85%A5%E7%9A%84%E7%BB%B4%E5%BA%A6%E4%B8%8D%E5%90%8C"><span class="toc-text">3.3 输入的维度不同</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#05-%E5%BC%A0%E9%87%8F"><span class="toc-text">05 张量</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E7%AE%97%E6%B3%95"><span class="toc-text">张量算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%99%8D%E7%BB%B4"><span class="toc-text">降维</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%B1%82%E5%92%8C"><span class="toc-text">求和</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%B1%82%E5%B9%B3%E5%9D%87"><span class="toc-text">求平均</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9D%9E%E9%99%8D%E7%BB%B4%E6%B1%82%E5%92%8C"><span class="toc-text">非降维求和</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#06-%E8%8C%83%E6%95%B0"><span class="toc-text">06 范数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%8C%83%E6%95%B0%E5%92%8C%E7%9B%AE%E6%A0%87"><span class="toc-text">范数和目标</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E5%BE%AE%E7%A7%AF%E5%88%86"><span class="toc-text">4 微积分</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6"><span class="toc-text">梯度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99"><span class="toc-text">链式法则</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86"><span class="toc-text">自动微分</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BE%8B%E5%AD%90"><span class="toc-text">例子</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9D%9E%E6%A0%87%E9%87%8F%E5%8F%98%E9%87%8F%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-text">非标量变量的反向传播</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%86%E7%A6%BB%E8%AE%A1%E7%AE%97"><span class="toc-text">分离计算</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Python-%E6%8E%A7%E5%88%B6%E6%B5%81%E7%9A%84%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="toc-text">Python 控制流的梯度计算</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-no-grad"><span class="toc-text">torch.no_grad()</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E6%A6%82%E7%8E%87"><span class="toc-text">5 概率</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E6%9F%A5%E9%98%85%E6%96%87%E6%A1%A3"><span class="toc-text">6 查阅文档</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">二、线性神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-text">1 线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#01-%E5%9F%BA%E6%9C%AC%E5%85%83%E7%B4%A0"><span class="toc-text">01 基本元素</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#02-%E7%9F%A2%E9%87%8F%E5%8C%96%E5%8A%A0%E9%80%9F"><span class="toc-text">02 矢量化加速</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-text">2 线性回归的实现</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E7%94%9F%E6%88%90%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">（1）生成数据集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E9%9B%86-data-iter"><span class="toc-text">（2）读取数据集 data_iter</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%883%EF%BC%89%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B-net"><span class="toc-text">（3）定义模型 net()</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%884%EF%BC%89%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="toc-text">（4）初始化模型参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%885%EF%BC%89%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-loss"><span class="toc-text">（5）定义损失函数 loss()</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%886%EF%BC%89%E5%AE%9A%E4%B9%89%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95-SGD"><span class="toc-text">（6）定义优化算法 SGD</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%887%EF%BC%89%E8%AE%AD%E7%BB%83"><span class="toc-text">（7）训练</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E9%9B%86-Fashion-MNIST"><span class="toc-text">3 图像分类数据集 Fashion-MNIST</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">读取数据集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96%E5%B0%8F%E6%89%B9%E9%87%8F"><span class="toc-text">读取小批量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B4%E5%90%88%E6%89%80%E6%9C%89%E7%BB%84%E4%BB%B6"><span class="toc-text">整合所有组件</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Softmax-%E5%9B%9E%E5%BD%92%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-text">4 Softmax 回归的实现</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%8E%E5%9B%9E%E5%BD%92%E5%88%B0%E5%88%86%E7%B1%BB"><span class="toc-text">从回归到分类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">（1）数据集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B-net-%EF%BC%8C%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="toc-text">（2）定义模型 net()，初始化模型参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%883%EF%BC%89-%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-loss"><span class="toc-text">（3） 定义损失函数 loss()</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%884%EF%BC%89%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95-SGD"><span class="toc-text">（4）优化算法 SGD</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%885%EF%BC%89-%E5%88%86%E7%B1%BB%E7%B2%BE%E5%BA%A6"><span class="toc-text">（5） 分类精度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%886%EF%BC%89-%E8%AE%AD%E7%BB%83"><span class="toc-text">（6） 训练</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-text">三、多层感知机</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-text">1 激活函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#RELU-%E5%87%BD%E6%95%B0"><span class="toc-text">RELU 函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Sigmoid-%E5%87%BD%E6%95%B0"><span class="toc-text">Sigmoid 函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tanh-%E5%87%BD%E6%95%B0"><span class="toc-text">tanh 函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-text">2 多层感知机的实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B-net"><span class="toc-text">定义模型 net()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83"><span class="toc-text">训练</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E3%80%81%E6%AC%A0%E6%8B%9F%E5%90%88%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-text">3 模型选择、欠拟合和过拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%A4%8D%E6%9D%82%E6%80%A7"><span class="toc-text">模型复杂性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9"><span class="toc-text">模型选择</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%AA%8C%E8%AF%81%E9%9B%86"><span class="toc-text">验证集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#K-%E6%8A%98%E6%8A%98%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="toc-text">K 折折交叉验证</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AC%A0%E6%8B%9F%E5%90%88-%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-text">欠拟合&#x2F;过拟合</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E6%AD%A3%E5%88%99%E5%8C%96%E6%A8%A1%E5%9E%8B-%E5%AF%B9%E6%8A%97%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-text">4 正则化模型-对抗过拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%8C%83%E6%95%B0%E4%B8%8E%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F"><span class="toc-text">范数与权重衰减</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9A%82%E9%80%80%E6%B3%95%EF%BC%88Dropout%EF%BC%89"><span class="toc-text">暂退法（Dropout）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%92%8C%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-text">5 数值稳定性和模型初始化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8-%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1"><span class="toc-text">梯度爆炸&#x2F;梯度消失</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%93%E7%A0%B4%E5%AF%B9%E7%A7%B0%E6%80%A7"><span class="toc-text">打破对称性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-text">参数初始化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%BB%98%E8%AE%A4%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-text">默认初始化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Xavier-%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-text">Xavier 初始化</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97"><span class="toc-text">四、深度学习计算</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%B1%82%E5%92%8C%E5%9D%97"><span class="toc-text">1 层和块</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%9D%97"><span class="toc-text">自定义块</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A1%BA%E5%BA%8F%E5%BF%AB"><span class="toc-text">顺序快</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9C%A8%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%87%BD%E6%95%B0%E4%B8%AD%E6%89%A7%E8%A1%8C%E4%BB%A3%E7%A0%81"><span class="toc-text">在前向传播函数中执行代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B5%8C%E5%A5%97%E5%9D%97"><span class="toc-text">嵌套块</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%8F%82%E6%95%B0%E7%AE%A1%E7%90%86"><span class="toc-text">2 参数管理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E8%AE%BF%E9%97%AE"><span class="toc-text">参数访问</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87%E5%8F%82%E6%95%B0"><span class="toc-text">目标参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E6%AC%A1%E6%80%A7%E8%AE%BF%E9%97%AE%E6%89%80%E6%9C%89%E5%8F%82%E6%95%B0"><span class="toc-text">一次性访问所有参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%8E%E5%B5%8C%E5%A5%97%E5%9D%97%E6%94%B6%E9%9B%86%E5%8F%82%E6%95%B0"><span class="toc-text">从嵌套块收集参数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96-1"><span class="toc-text">参数初始化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%86%85%E7%BD%AE%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-text">内置初始化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-text">自定义初始化</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E7%BB%91%E5%AE%9A"><span class="toc-text">参数绑定</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%BB%B6%E5%90%8E%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-text">3 延后初始化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B1%82"><span class="toc-text">4 自定义层</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8D%E5%B8%A6%E5%8F%82%E6%95%B0%E7%9A%84%E5%B1%82"><span class="toc-text">不带参数的层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%A6%E5%8F%82%E6%95%B0%E7%9A%84%E5%B1%82"><span class="toc-text">带参数的层</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E8%AF%BB%E5%86%99%E6%96%87%E4%BB%B6"><span class="toc-text">5 读写文件</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD%E5%90%91%E9%87%8F"><span class="toc-text">保存和加载向量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E5%92%8C%E4%BF%9D%E5%AD%98%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="toc-text">加载和保存模型参数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-GPU"><span class="toc-text">6 GPU</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E8%AE%BE%E5%A4%87"><span class="toc-text">计算设备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E4%B8%8E-GPU"><span class="toc-text">张量与 GPU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%98%E5%82%A8%E5%9C%A8-GPU-%E4%B8%8A"><span class="toc-text">存储在 GPU 上</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%8D%E5%88%B6"><span class="toc-text">复制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text"></span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E-GPU"><span class="toc-text">神经网络与 GPU</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/post/33227.html" title="《仓鼠球！GO!》"><img src="https://cdn.jsdelivr.net/gh/liuke101/PicGo-For-Hexo/image/202408242348371.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="《仓鼠球！GO!》"/></a><div class="content"><a class="title" href="/post/33227.html" title="《仓鼠球！GO!》">《仓鼠球！GO!》</a><time datetime="2024-08-17T16:00:00.000Z" title="发表于 2024-08-18 00:00:00">2024-08-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/3834.html" title="Effective Modern C++"><img src="https://cdn.jsdelivr.net/gh/liuke101/PicGo-For-Hexo/image/202408250023909.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Effective Modern C++"/></a><div class="content"><a class="title" href="/post/3834.html" title="Effective Modern C++">Effective Modern C++</a><time datetime="2024-01-18T04:20:00.000Z" title="发表于 2024-01-18 12:20:00">2024-01-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/57053.html" title="Lua精粹"><img src="https://cdn.jsdelivr.net/gh/liuke101/PicGo-For-Hexo/image/202408250021603.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Lua精粹"/></a><div class="content"><a class="title" href="/post/57053.html" title="Lua精粹">Lua精粹</a><time datetime="2023-10-28T16:00:00.000Z" title="发表于 2023-10-29 00:00:00">2023-10-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/46644.html" title="UE AI系统">UE AI系统</a><time datetime="2023-10-15T16:00:00.000Z" title="发表于 2023-10-16 00:00:00">2023-10-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/46645.html" title="GAS精粹"><img src="https://cdn.jsdelivr.net/gh/liuke101/PicGo-For-Hexo/image/202408250017189.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="GAS精粹"/></a><div class="content"><a class="title" href="/post/46645.html" title="GAS精粹">GAS精粹</a><time datetime="2023-10-15T16:00:00.000Z" title="发表于 2023-10-16 00:00:00">2023-10-16</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://cdn.jsdmirror.com/gh/liuke101/PicGo-For-Hexo/image/202408010047802.png')"><div id="footer-wrap"><div class="copyright">&copy;2024 By 灵玉</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdmirror.com/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdmirror.com/npm/instant.page@5.2.0/instantpage.min.js" type="module"></script><script src="https://cdn.jsdmirror.com/npm/node-snackbar@0.1.16/dist/snackbar.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdmirror.com/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>(() => {
  let initFn = window.walineFn || null

  const initWaline = (Fn) => {
    const waline = Fn(Object.assign({
      el: '#waline-wrap',
      serverURL: 'https://waline-server-indol.vercel.app/',
      pageview: false,
      dark: 'html[data-theme="dark"]',
      path: window.location.pathname,
      comment: true,
    }, null))

    const destroyWaline = () => {
      waline.destroy()
    }

    btf.addGlobalFn('pjax', destroyWaline, 'destroyWaline')
  }

  const loadWaline = async () => {
    if (initFn) initWaline(initFn)
    else {
      await getCSS('https://cdn.jsdmirror.com/npm/@waline/client@3.1.2/dist/waline.min.css')
      const { init } = await import('https://cdn.jsdmirror.com/npm/@waline/client@3.1.2/dist/waline.min.js')
      initFn = init || Waline.init
      initWaline(initFn)
      window.walineFn = initFn
    }
  }

  if ('Waline' === 'Waline' || !false) {
    if (false) btf.loadComment(document.getElementById('waline-wrap'),loadWaline)
    else setTimeout(loadWaline, 0)
  } else {
    window.loadOtherComment = loadWaline
  }
})()</script><script>(()=>{
  const loadLivere = () => {
    if (typeof LivereTower === 'object') window.LivereTower.init()
    else {
      (function(d, s) {
          var j, e = d.getElementsByTagName(s)[0];
          if (typeof LivereTower === 'function') { return; }
          j = d.createElement(s);
          j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
          j.async = true;
          e.parentNode.insertBefore(j, e);
      })(document, 'script');
    }
  }

  if ('Waline' === 'Livere' || !false) {
    if (false) btf.loadComment(document.getElementById('lv-container'), loadLivere)
    else loadLivere()
  } else {
    window.loadOtherComment = loadLivere
  }
})()</script></div><script>window.addEventListener('load', () => {
  const changeContent = content => {
    if (content === '') return content

    content = content.replace(/<img.*?src="(.*?)"?[^\>]+>/ig, '[图片]') // replace image link
    content = content.replace(/<a[^>]+?href=["']?([^"']+)["']?[^>]*>([^<]+)<\/a>/gi, '[链接]') // replace url
    content = content.replace(/<pre><code>.*?<\/pre>/gi, '[代码]') // replace code
    content = content.replace(/<[^>]+>/g,"") // remove html tag

    if (content.length > 150) {
      content = content.substring(0,150) + '...'
    }
    return content
  }

  const generateHtml = array => {
    let result = ''

    if (array.length) {
      for (let i = 0; i < array.length; i++) {
        result += '<div class=\'aside-list-item\'>'

        if (true) {
          const name = 'src'
          result += `<a href='${array[i].url}' class='thumbnail'><img ${name}='${array[i].avatar}' alt='${array[i].nick}'></a>`
        }

        result += `<div class='content'>
        <a class='comment' href='${array[i].url}' title='${array[i].content}'>${array[i].content}</a>
        <div class='name'><span>${array[i].nick} / </span><time datetime="${array[i].date}">${btf.diffDate(array[i].date, true)}</time></div>
        </div></div>`
      }
    } else {
      result += '没有评论'
    }

    let $dom = document.querySelector('#card-newest-comments .aside-list')
    $dom && ($dom.innerHTML= result)
    window.lazyLoadInstance && window.lazyLoadInstance.update()
    window.pjax && window.pjax.refresh($dom)
  }

  const getComment = async () => {
    try {
      const res = await fetch('https://waline-server-indol.vercel.app/api/comment?type=recent&count=6', { method: 'GET' })
      const result = await res.json()
      const walineArray = result.data.map(e => {
        return {
          'content': changeContent(e.comment),
          'avatar': e.avatar,
          'nick': e.nick,
          'url': e.url + '#' + e.objectId,
          'date': e.time || e.insertedAt
        }
      })
      saveToLocal.set('waline-newest-comments', JSON.stringify(walineArray), 10/(60*24))
      generateHtml(walineArray)
    } catch (err) {
      console.error(err)
      const $dom = document.querySelector('#card-newest-comments .aside-list')
      $dom.textContent= "无法获取评论，请确认相关配置是否正确"
    }
  }

  const newestCommentInit = () => {
    if (document.querySelector('#card-newest-comments .aside-list')) {
      const data = saveToLocal.get('waline-newest-comments')
      if (data) {
        generateHtml(JSON.parse(data))
      } else {
        getComment()
      }
    }
  }

  newestCommentInit()
  document.addEventListener('pjax:complete', newestCommentInit)
})</script><div class="aplayer no-destroy" data-id="9896988395" data-server="netease" data-type="playlist" data-fixed="true" data-autoplay="false"> </div><link rel="stylesheet" href="https://cdn.jsdmirror.com/npm/aplayer@1.10.1/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdmirror.com/npm/aplayer@1.10.1/dist/APlayer.min.js"></script><script src="https://cdn.jsdmirror.com/npm/butterfly-extsrc@1.1.3/metingjs/dist/Meting.min.js"></script><script src="https://cdn.jsdmirror.com/npm/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["meta[property=\"og:image\"]","meta[property=\"og:title\"]","meta[property=\"og:url\"]","head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener
  btf.removeGlobalFnEvent('pjax')
  btf.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div><script src="https://cdn.jsdmirror.com/npm/algoliasearch@4.22.1/dist/algoliasearch-lite.umd.min.js"></script><script src="https://cdn.jsdmirror.com/npm/instantsearch.js@4.65.0/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js?v=4.13.0"></script></div></div></body></html>